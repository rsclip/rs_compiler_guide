<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Writing a Compiler in Rust from scratch</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="introduction.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="getting_started.html"><strong aria-hidden="true">2.</strong> Getting Started</a></li><li class="chapter-item expanded "><a href="error_handling.html"><strong aria-hidden="true">3.</strong> Error Handling</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="error_handling_with_ariadne.html"><strong aria-hidden="true">3.1.</strong> Error Handling with ariadne</a></li><li class="chapter-item expanded "><a href="error_handling_with_codespan_reporting.html"><strong aria-hidden="true">3.2.</strong> Error Handling with codespan-reporting</a></li></ol></li><li class="chapter-item expanded "><a href="lexical_analysis.html"><strong aria-hidden="true">4.</strong> Lexical Analysis</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="finite_automata.html"><strong aria-hidden="true">4.1.</strong> Finite Automata</a></li><li class="chapter-item expanded "><a href="tokens.html"><strong aria-hidden="true">4.2.</strong> Tokens</a></li><li class="chapter-item expanded "><a href="lexer.html"><strong aria-hidden="true">4.3.</strong> Lexer</a></li></ol></li><li class="chapter-item expanded "><a href="parsing_and_syntax_analysis.html"><strong aria-hidden="true">5.</strong> Parsing and Syntax Analysis</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="abstract_syntax_trees.html"><strong aria-hidden="true">5.1.</strong> Abstract Syntax Trees</a></li><li class="chapter-item expanded "><a href="context_free_grammars.html"><strong aria-hidden="true">5.2.</strong> Context-free Grammars</a></li><li class="chapter-item expanded "><a href="parsing_techniques.html"><strong aria-hidden="true">5.3.</strong> Parsing Techniques</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="recursive_descent_parsing.html"><strong aria-hidden="true">5.3.1.</strong> Recursive Descent Parsing</a></li><li class="chapter-item expanded "><a href="ll_k_parsing.html"><strong aria-hidden="true">5.3.2.</strong> LL(k) Parsing</a></li><li class="chapter-item expanded "><a href="lr_parsing.html"><strong aria-hidden="true">5.3.3.</strong> LR Parsing</a></li></ol></li><li class="chapter-item expanded "><a href="implementing_an_ast.html"><strong aria-hidden="true">5.4.</strong> Implementing an AST</a></li><li class="chapter-item expanded "><a href="implementing_a_parser.html"><strong aria-hidden="true">5.5.</strong> Implementing a Parser</a></li></ol></li><li class="chapter-item expanded "><a href="semantic_analysis.html"><strong aria-hidden="true">6.</strong> Semantic Analysis</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="symbol_tables.html"><strong aria-hidden="true">6.1.</strong> Symbol Tables</a></li><li class="chapter-item expanded "><a href="type_checking.html"><strong aria-hidden="true">6.2.</strong> Type Checking</a></li><li class="chapter-item expanded "><a href="implementing_a_symbol_table.html"><strong aria-hidden="true">6.3.</strong> Implementing a Symbol Table</a></li><li class="chapter-item expanded "><a href="implementing_a_semantic_analyzer.html"><strong aria-hidden="true">6.4.</strong> Implementing a Semantic Analyzer</a></li></ol></li><li class="chapter-item expanded "><a href="intermediate_representation.html"><strong aria-hidden="true">7.</strong> Intermediate Representation</a></li><li class="chapter-item expanded "><a href="code_generation.html"><strong aria-hidden="true">8.</strong> Code Generation</a></li><li class="chapter-item expanded "><a href="postlude.html"><strong aria-hidden="true">9.</strong> Postlude</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Writing a Compiler in Rust from scratch</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p>This guide serves as a brief process of developing a compiler for our own programming language from scratch into executable programs. We'll cover each stage of compilation, from tokenization to code generation, all with explanations.</p>
<p>I am quite a beginner in both Rust and Compiler Development, so the code may be messy and very badly/inefficiently implemented. Either way, it can be a great starting point!</p>
<p>I'll link resources frequently throughout the guide, and I'll also provide a list of resources at the end. They will be at the bottom of each (sub)section.</p>
<p>Not all code covered will be included in the guide, but the final code will be available in the final GitHub repository at the bottom.</p>
<h2 id="goals-of-the-guide"><a class="header" href="#goals-of-the-guide">Goals of the Guide</a></h2>
<p>Learning how to develop a compiler is ridiculously confusing with a high barrier-to-entry and no solid guide on how to make one. Thus, this guide will:</p>
<ul>
<li>Provide a somewhat thorough understanding of the compilation process, including its stages and components</li>
<li>Explains the role and significance of each stage of said process</li>
<li>Offer insights into common techniques, optimizations, etc. used in modern compilers</li>
<li>Provide a good understanding in how languages actually work</li>
<li>Allow you to make your own compiler! Kind of.</li>
</ul>
<h2 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h2>
<p>While this guide aims to be accessible to developers at various levels, you should have an intermediate understanding of programming languages and Computer Science fundamentals.</p>
<p>The guide uses Rust exclusively, but it avoids difficult or non-conventional concepts. It can be followed in another similar language to an extent.</p>
<p><strong>Recommended:</strong></p>
<ul>
<li>Good understanding of programming concepts (variables, data-types, control structures, functions, recursive &amp; iterative methods, etc.)</li>
<li>Familiarity with Rust (if you know C++, this should be relatively easy to pick up)</li>
<li>Understanding of fundamental data structures (stacks, trees, etc.)</li>
<li>Knowledge of algorithm design &amp; analysis principles</li>
</ul>
<p><strong>Beneficial</strong>:
Albeit not required, it would greatly help if you have knowledge of some more topics.</p>
<ul>
<li>Understanding of memory management concepts</li>
<li>Knowledge of Abstract Syntax Trees (ASTs)</li>
<li>Understanding of grammars (EBNF, BNF, etc.)</li>
<li>Knowledge of basic compiler theory concepts (lexical analysis, parsing, optimization, etc.)</li>
<li>Familiarity with any Assembly languages or low-level programming concepts</li>
<li>Understanding of basic Mathematics (set theory, logic, etc.)</li>
<li>Understanding of Mathematic notations</li>
</ul>
<p>Although this may be a daunting list, it's not necessary to know everything. The guide will cover most of these topics, and it's not at all difficult to learn them as you go along!</p>
<h2 id="overview-of-compilers"><a class="header" href="#overview-of-compilers">Overview of Compilers</a></h2>
<h3 id="what-is-a-compiler"><a class="header" href="#what-is-a-compiler">What is a Compiler?</a></h3>
<p>A compiler is a program that translates source code (represented as text) written in a higher-level programming language into machine code which a computer can execute directly. It may also be translated into bytecode, in which a virtual machine (like Java's Virtual Machine or Python's interpreter) can interpret and run, although this requires the VM to be installed. It plays a crucial role by facilitating in the conversion of human-readable code into a format understandable by the computer's hardware.</p>
<h3 id="key-concepts"><a class="header" href="#key-concepts">Key concepts</a></h3>
<p>There are some critical concepts of Compiler Theory you need to understand before going through the stages:</p>
<p><strong>Abstract Syntax Trees</strong>: ASTs are trees which determine the structure of the program in a machine-readable way. For example, <code>a = 1 + 2</code> becomes:</p>
<pre><code>	VarDeclaration
Ident(a)       Add
			1		2
</code></pre>
<p><strong>Tokens</strong>: Represent the small "building blocks" of text, as opposed to a list of characters. For example, <code>fn main() { return 0; }</code> would be <code>["fn", "main", "(", ")", "{", "return", "0", ";", "}"</code>. There are no whitespaces.</p>
<p><strong>Instruction Set Architecture</strong>: ISAs are a set of instructions a processor can run on. These vary from architecture to architecture.</p>
<p><strong>Intermediate Representation</strong>: Different devices have different ISAs, and so the last code we generate will be an intermediate representation between ASTs and actual Assembly code. This way, we can convert it to multiple ISAs much more easily.</p>
<h3 id="components-of-a-compiler"><a class="header" href="#components-of-a-compiler">Components of a Compiler</a></h3>
<p>There's various components of a Compiler which the source code has to go through:</p>
<p><strong>Lexer (Tokenizer)</strong>:
Breaks down the source code into smaller units called <strong>tokens</strong>. These represent the smallest meaningful elements of the programming language such as keywords, identifiers, and literals. We use this to ignore redundant characters like whitespaces, making it easier for us to parse.</p>
<p>This would also allow us to identify when there are unexpected characters or unterminated strings, for example.</p>
<p><strong>Parser</strong>:
The parser analyzes the structure of the source code by parsing the stream of tokens generated by the lexer. It verifies whether the code conforms to the grammar rules of the language, generating an Abstract Syntax Tree (AST).</p>
<p><strong>Semantic Analyzer</strong>:
Performs analysis to check for type compatibility, variable declarations, scoping rules, and more to ensure the code behaves as intended. This is especially important if the user does not return the correct type, which -- if gone unchecked -- can be especially catastrophic and unsafe; imagine a function intended to return a pointer to dynamically allocated memory (like a Vector), but due to a type-checking error, it instead returns a pointer to a different object. The caller may not properly deallocate memory (leading to memory leaks), and subsequent operations on the data could lead to memory corruption.</p>
<p><strong>IR generation</strong>:
From the resulting AST, the compiler generates an Intermediate Representation (IR) code prior to translating to Assembly. This is especially important since different computers, devices, and operating systems run on different architectures and ISAs (Instruction Set Architecture). This would be the lowest level of code yet produced, and we can easily convert this to assembly depending on the ISA.</p>
<p><strong>Optimization</strong>:
Applies various transformations to the IR -- or directly to the AST -- to improve the efficiency and performance of the generated code. They may include constant folding (converting <code>3 + 2</code> into just <code>5</code>), loop unrolling (getting rid of the condition checks), dead code elimination (removing parts that are never used), and register allocation (assigning variables/values to processor registers instead of memory locations, as they are significantly faster).</p>
<p><strong>Code generation</strong>:
Finally, the optimized IR is translated into machine code suitable for execution on the target ISA. This maps the constructs in IR to low-level instructions, whilst ensuring the target architecture's resources are being used efficiently.</p>
<h1 id="introductory-resources"><a class="header" href="#introductory-resources">Introductory Resources</a></h1>
<p>Some resources you could read into for this page in particular.</p>
<ul>
<li><a href="https://craftinginterpreters.com/">Crafting Interpreters</a>: A book by Bob Nystrom which covers both interpreters and compilers. It's a great resource for learning how to make a language.</li>
<li><a href="https://doc.rust-lang.org/book/">Rust Programming Language</a>: The official Rust book is a great resource for learning Rust.</li>
<li><a href="https://www.amazon.com/Programming-Language-Pragmatics-Michael-Scott/dp/0124104096">Programming Language Pragmatics</a>: A book by Michael Scott which covers the theory of programming languages, including compilers.</li>
<li><a href="https://www.cs.cornell.edu/courses/cs412/2008sp/lectures/lec20.pdf">Intro to Compilers</a>, a lecture by Andrew Myers at Cornell University.</li>
</ul>
<h1 id="all-resources"><a class="header" href="#all-resources">All resources</a></h1>
<p>All resources used in this guide will be listed here!</p>
<ul>
<li>
<p><a href="./introduction.html">Introduction</a></p>
<ul>
<li><a href="https://craftinginterpreters.com/">Crafting Interpreters</a>: A book by Bob Nystrom which covers both interpreters and compilers. It's a great resource for learning how to make a language.</li>
<li><a href="https://doc.rust-lang.org/book/">Rust Programming Language</a>: The official Rust book is a great resource for learning Rust.</li>
<li><a href="https://www.amazon.com/Programming-Language-Pragmatics-Michael-Scott/dp/0124104096">Programming Language Pragmatics</a>: A book by Michael Scott which covers the theory of programming languages, including compilers.</li>
<li><a href="https://www.cs.cornell.edu/courses/cs412/2008sp/lectures/lec20.pdf">Intro to Compilers</a>, a lecture by Andrew Myers at Cornell University.</li>
</ul>
</li>
<li>
<p><a href="./getting_started.html">Getting Started</a></p>
<ul>
<li><a href="https://doc.rust-lang.org/book/ch01-01-installation.html">Rust's book</a> (the installation section): The official Rust book is a great resource for installing and learning Rust.</li>
<li><a href="https://doc.rust-lang.org/rust-by-example/">Rust by Example</a>: A great resource for learning Rust by example.</li>
</ul>
</li>
<li>
<p><a href="./error_handling.html">Error Handling</a></p>
<ul>
<li><a href="https://docs.rs/codespan-reporting/0.11.1/codespan_reporting/">codespan-reporting</a></li>
<li><a href="https://docs.rs/thiserror/1.0.24/thiserror/">thiserror</a></li>
<li><a href="https://crates.io/crates/codespan-reporting">codespan-reporting example</a></li>
<li><a href="https://docs.rs/codespan-reporting/0.11.1/codespan_reporting/files/struct.SimpleFiles.html">SimpleFiles</a></li>
<li><a href="https://docs.rs/codespan-reporting/0.11.1/codespan_reporting/diagnostic/struct.Diagnostic.html">Diagnostic</a></li>
<li><a href="https://docs.rs/codespan-reporting/0.11.1/codespan_reporting/diagnostic/struct.Label.html">Label</a></li>
<li><a href="https://en.wikipedia.org/wiki/Error_recovery">Error Recovery</a></li>
<li><a href="https://go.dev/blog/defer-panic-and-recover">Panic vs Recover in Go</a></li>
</ul>
</li>
<li>
<p><a href="./lexical_analysis.html">Lexical Analysis</a></p>
<ul>
<li><a href="https://github.com/leonardomso/awesome-fsm">Awesome FSM</a>: A curated list of awesome finite state machine libraries, software and resources.</li>
<li><a href="https://doc.rust-lang.org/std/iter/trait.Iterator.html">Iterator trait</a>: The official documentation for the <code>Iterator</code> trait.</li>
<li><a href="https://doc.rust-lang.org/rust-by-example/scope/lifetime.html">Lifetimes in Rust</a></li>
<li><a href="./finite_automata.html">Finite Automata</a> in this guide!</li>
<li><a href="https://medium.com/@tarungudipalli/exploring-rusts-string-a-comprehensive-guide-with-examples-25f398ade356">Rust's Strings</a>: A comprehensive guide to Rust's strings.</li>
</ul>
</li>
<li>
<p><a href="./parsing_and_syntax_analysis.html">Parsing and Syntax Analysis</a></p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">Abstract Syntax Trees - Wikipedia</a></li>
<li><a href="https://eli.thegreenplace.net/2009/02/16/abstract-vs-concrete-syntax-trees">ASTs vs CSTs</a></li>
<li><a href="https://docs.rs/syn/1.0.72/syn/">Rust Syn crate</a></li>
<li><a href="https://en.wikipedia.org/wiki/Tree_(data_structure)">Tree Data Structures</a></li>
<li><a href="https://en.wikipedia.org/wiki/Tree_traversal">Tree Traversal</a></li>
<li><a href="https://www.cs.cornell.edu/courses/cs2110/2017sp/online/dfs/dfs01.html">DFS, BFS</a></li>
<li><a href="https://www.ketteringscienceacademy.org/attachments/download.asp?file=1057&amp;type=pdf">BNF</a>: The dates example is from this resource.</li>
<li><a href="https://en.wikipedia.org/wiki/Extended_Backus%E2%80%93Naur_form">EBNF</a>: Wikipedia's page on EBNF.</li>
<li><a href="https://www.cl.cam.ac.uk/~mgk25/iso-14977.pdf">EBNF</a>: The official ISO standard for EBNF.</li>
<li><a href="https://www.cs.uic.edu/~liub/teach/cs494/ebnf.pdf">EBNF</a>: A more concise and readable resource on EBNF.</li>
<li><a href="https://en.wikibooks.org/wiki/Introduction_to_Programming_Languages/Grammars">Intro to Programming Languages/Grammars</a>: A Wikibooks page on grammars.</li>
<li><a href="https://en.wikipedia.org/wiki/Recursive_descent_parser">Recursive Descent Parsing</a> on Wikipedia</li>
<li><a href="https://maeyler.github.io/Automata-2018/cfg/Bilal_RecursiveDescentParser.html">RDP Visualizer</a> - A visualizer for recursive descent parsers</li>
<li><a href="https://en.wikipedia.org/wiki/LL_parser">LL(k) Parsing</a> on Wikipedia</li>
<li><a href="https://www.cs.uaf.edu/~cs331/notes/FirstFollow.pdf">LL(k) Parsing</a> on University of Alaska Fairbanks</li>
<li><a href="https://github.com/GabrielMajeri/LL-K-Parser">LL(k) Parser</a>: A Python implementation of an LL(k) parser</li>
<li><a href="https://en.wikipedia.org/wiki/LR_parser">LR Parsing</a> on Wikipedia</li>
<li><a href="https://www.geeksforgeeks.org/lr-parser/">LR Parsing GfG</a>: GeeksforGeeks article on LR Parsing</li>
<li><a href="https://en.wikipedia.org/wiki/Operator-precedence_parser">Operator Precedence</a></li>
<li><a href="https://doc.rust-lang.org/std/boxed/struct.Box.html">Box<T></a></li>
<li><a href="https://doc.rust-lang.org/std/fmt/trait.Display.html"><code>Display</code> trait</a></li>
<li><a href="https://doc.rust-lang.org/std/cell/struct.RefCell.html">RefCell documentation</a></li>
<li><a href="https://doc.rust-lang.org/book/ch15-05-interior-mutability.html">RefCell in book</a></li>
</ul>
</li>
</ul>
<p><strong>Todo</strong>:</p>
<ul>
<li><input disabled="" type="checkbox"/>
<a href="./semantic_analysis.html">Semantic Analysis</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="getting-started"><a class="header" href="#getting-started">Getting Started</a></h1>
<h2 id="setting-up-your-environment"><a class="header" href="#setting-up-your-environment">Setting up your environment</a></h2>
<p>Setting up your development environment varies on your Operating System and IDE. Here, we'll use Windows and VSCode. Assuming those are installed:</p>
<ol>
<li>Install <a href="https://rustup.rs/">rustup</a> and follow the instructions</li>
<li>Verify your installation: <code>rustc --version</code> and <code>cargo --version</code></li>
<li>Install any IDE plugins you'd find useful
<ol>
<li><a href="https://marketplace.visualstudio.com/items?itemName=formulahendry.code-runner">Code Runner</a>, <a href="https://marketplace.visualstudio.com/items?itemName=rust-lang.rust-analyzer">rust-analyzer</a> are currently greatly useful. Debugging tools may help too.</li>
</ol>
</li>
<li>Create a new project: <code>cargo new compiler</code></li>
</ol>
<p>I would recommend using git for version control.</p>
<h3 id="crates"><a class="header" href="#crates">Crates</a></h3>
<p>There are some crates (libraries) we can install to make the development process easier.</p>
<p><strong>Errors</strong><br />
<a href="https://crates.io/crates/anyhow">anyhow</a> is a great crate for propagating errors idiomatically.
<a href="https://crates.io/crates/thiserror">thiserror</a> will help us to implement our own error enums.
For our error messages, this guide will use <a href="https://crates.io/crates/ariadne">ariadne</a> for error diagnostics. Alternatively, you can use <a href="https://crates.io/crates/codespan_reporting">codespan-reporting</a> (implemented in <a href="./error_handling_with_codespan_reporting.html">Error Handling with codespan-reporting</a>).</p>
<p><strong>CLI Parsing</strong><br />
This guide will use <a href="https://crates.io/crates/gumdrop">gumdrop</a> since our compiler will be incredibly simplistic, and its much faster than <a href="https://crates.io/crates/clap">clap</a> (although that's a great alternative, albeit relatively slow and large).</p>
<p><strong>Code Generation</strong><br />
This guide (un)fortunately won't cover the specific code-generation as it's very out-of-scope and complex, not suitable for a beginner guide.</p>
<p><strong>Debugging</strong>
<a href="https://crates.io/crates/log">log</a> + <a href="https://crates.io/crates/env_logger">env_logger</a> will help us debug our compiler by introducing macros like <code>debug!</code>, etc. They will not be displayed in code examples, but I recommend using them.</p>
<p>Additionally, learn how to set up a debugger in your IDE. This will be incredibly useful for debugging your compiler. Here's a guide for <a href="https://code.visualstudio.com/docs/cpp/cpp-debug-linux">Linux</a> and <a href="https://code.visualstudio.com/docs/cpp/cpp-debug-windows">Windows</a>.</p>
<p><a href="https://cranelift.dev/">Cranelift</a> is a fast compiler backend made in Rust we'll use.</p>
<h4 id="want-a-library-for-lexingparsing"><a class="header" href="#want-a-library-for-lexingparsing">Want a library for lexing/parsing?</a></h4>
<p>Although this guide is intended for writing these by hand, you can always rely on more industrial alternatives. I'd recommend using these only if you have written a lexer and parser by hand. Also, the errors may or may not be as pretty and insightful.</p>
<ul>
<li><a href="https://pest.rs/">pest.rs</a> is a powerful and elegant parser, very helpful for modifying your grammar.</li>
<li><a href="https://crates.io/crates/chumsky">chumsky</a> is a parser with a focus on error recovery, which uses its sister crate <a href="https://crates.io/crates/ariadne">ariadne</a>.</li>
<li><a href="https://crates.io/crates/logos">logos</a> is an incredibly fast lexer you can use if you don't want to tokenize your inputs yourself.</li>
<li><a href="https://crates.io/crates/combine">combine</a> provides parser combinations with zero-copy support, making it very flexible.</li>
</ul>
<h4 id="not-fond-of-cranelift"><a class="header" href="#not-fond-of-cranelift">Not fond of Cranelift?</a></h4>
<p>If you want to look into more conventional Compiler infrastructures, <a href="https://llvm.org/">LLVM</a> is the perfect candidate. There are multiple Rust bindings, so here's a brief overview:</p>
<ul>
<li><a href="https://crates.io/crates/inkwell">Inkwell</a> is a safe(?) idiomatic wrapper on LLVM</li>
<li><a href="https://crates.io/crates/llvm-sys">llvm-sys</a> is a direct binding for LLVM</li>
<li><a href="https://github.com/maekawatoshiki/vicis">Vicis</a> if you want to use LLVM IR, <em>without LLVM</em>. I found LLVM a pain to setup, so this may be useful.</li>
</ul>
<h3 id="file-structure"><a class="header" href="#file-structure">File structure</a></h3>
<p>It's important to modularize your codebase so it's easier to use. We'll use a simple structure:</p>
<pre><code>compiler/
|-- Cargo.toml
|-- grammar.bnf
+-- src
  +-- ast          // We will store our AST items here (folder)
  |-- errors.rs    // Error enum here
  |-- lexer.rs     // Lexical analysis
  |-- main.rs      // Main entry point
  |-- parser.rs    // Our parser
  |-- token.rs     // Our tokens
+-- tests
</code></pre>
<div class="warning">
<p>This is <strong>not</strong> the final structure, but a starting point. We will add more files as we go along.</p>
</div>
<h1 id="resources"><a class="header" href="#resources">Resources</a></h1>
<ul>
<li><a href="https://doc.rust-lang.org/book/ch01-01-installation.html">Rust's book</a> (the installation section): The official Rust book is a great resource for installing and learning Rust.</li>
<li><a href="https://doc.rust-lang.org/rust-by-example/">Rust by Example</a>: A great resource for learning Rust by example.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="error-handling"><a class="header" href="#error-handling">Error Handling</a></h1>
<p>Before we dive into developing the compiler, it's important to have some foundations for how we will handle errors.</p>
<h2 id="introduction-1"><a class="header" href="#introduction-1">Introduction</a></h2>
<h3 id="error-types"><a class="header" href="#error-types">Error Types</a></h3>
<p>There are three types of errors that we will handle in our compiler:</p>
<ul>
<li><strong>Lexical Errors</strong>: These errors occur when the lexer encounters an invalid token.</li>
<li><strong>Syntax Errors</strong>: These errors occur when the parser encounters an invalid syntax.</li>
<li><strong>Semantic Errors</strong>: These errors occur when the compiler encounters an invalid semantic.</li>
</ul>
<p>We'll cover Semantic Errors later on when we discuss Semantic Analysis.</p>
<h3 id="error-recovery"><a class="header" href="#error-recovery">Error Recovery</a></h3>
<p>When an error is encountered, the compiler should attempt to recover from the error and continue parsing the input. This is important because it allows the compiler to report multiple errors in a single run, rather than stopping at the first error.</p>
<p>We can either <strong>panic</strong> or <strong>recover</strong> from an error. When we panic, we stop parsing and report the error. When we recover, we attempt to continue parsing the input. It's better to recover from an error, as it allows us to report multiple errors in a single run; imagine fixing an error one, recompiling, and then finding another error repeatedly. Not fun.</p>
<p>The errors should be <strong>propagated</strong> (i.e., passed up the call stack) to the top-level function, where they can be reported to the user.</p>
<h2 id="error-handling-in-rust"><a class="header" href="#error-handling-in-rust">Error Handling in Rust</a></h2>
<p>Let's setup the main <code>enum LangError</code> representing the different errors we might encounter at any stage in the process.</p>
<pre><code class="language-rust ignore">use crate::token::{Span, TokenKind};

#[derive(Debug)]
pub enum LangError {
    // An unexpected character was found
    UnexpectedCharacter(String, Span),

    // A string was not terminated
    UnterminatedString(Span),

    // Was expecting a particular token, found another
    ExpectedToken {
        expected: TokenKind,
        found: TokenKind,
        span: Span,
    },

    // Was expecting any of the tokens, found another
    ExpectedAnyToken {
        expected: Vec&lt;TokenKind&gt;,
        found: TokenKind,
        span: Span,
    },

    // Unexpected EOF
    UnexpectedEOF(Span),

    // Invalid literal
    InvalidLiteral(String, Span),
}</code></pre>
<p>Straightforward, and it's clear to see it separated in two categories for now: Lexical and Syntax errors. We can always add more as we go along.</p>
<p>However, we could also like to use <code>thiserror</code> to derive the <code>Error</code> trait for our <code>LangError</code> enum. This will allow us to use the <code>?</code> operator to propagate errors up the call stack.</p>
<p>Let's implement:</p>
<pre><code class="language-rust ignore">use crate::token::{Span, TokenKind};
use thiserror::Error; // &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; Make sure you use this!

#[derive(Debug, Error)]
pub enum LangError {
    // An unexpected character was found
    #[error("Unexpected character: `{0}`")]
    UnexpectedCharacter(String, Span),

    // A string was not terminated
    #[error("Unterminated string")]
    UnterminatedString(Span),

    // Was expecting a particular token, found another
    #[error("Expected token: `{expected}`, found: `{found}`")]
    ExpectedToken {
        expected: TokenKind,
        found: TokenKind,
        span: Span,
    },

    // Was expecting any of the tokens, found another
    #[error(
        "Expected any of the tokens: `{}`, found: `{found}`",
        LangError::any_tokens_display(expected)
    )]
    ExpectedAnyToken {
        expected: Vec&lt;TokenKind&gt;,
        found: TokenKind,
        span: Span,
    },

    // Unexpected EOF
    #[error("Unexpected EOF")]
    UnexpectedEOF(Span),

    // Invalid literal
    #[error("Invalid literal: `{0}`")]
    InvalidLiteral(String, Span),
}</code></pre>
<p>Simply put, we add <code>#[error("...")]</code> to each variant, and we can use <code>{}</code> to interpolate values. We also add a method <code>any_tokens_display</code> to the <code>LangError</code> enum to display the expected tokens in a more readable format.</p>
<pre><code class="language-rust ignore">impl LangError {
    fn any_tokens_display(expected: &amp;Vec&lt;TokenKind&gt;) -&gt; String {
        expected
            .iter()
            .map(|t| format!("{:?}", t))
            .collect::&lt;Vec&lt;String&gt;&gt;()
            .join(", ")
    }
}</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="error-handling-with-ariadne"><a class="header" href="#error-handling-with-ariadne">Error Handling with <code>ariadne</code></a></h1>
<blockquote>
<p>✅ This is the error crate we will be using.</p>
<p>Too confusing? Try <a href="./error_handling_with_codespan_reporting.html">codespan-reporting</a> instead.</p>
</blockquote>
<p>We'll use <code>ariadne</code> to design prettier error messages. This library allows us to highlight the source code where the error occurred, and provide a helpful message to the user. It's extremely strong and versatile, as well as able to generate beautiful error messages.</p>
<p>Take a moment to review <a href="https://docs.rs/ariadne/*/ariadne/">ariadne's documentation</a>, and the <a href="https://crates.io/crates/ariadne">example</a> can help tremendously. As of writing, the documentation can be initially difficult to understand, but the example is very helpful.</p>
<h2 id="file-system"><a class="header" href="#file-system">File System</a></h2>
<p>We'll opt for making our file system for <code>ariadne</code> to use. This will allow us to load a file into memory, and then use the file ID to generate diagnostics.</p>
<p>Let's look into the structure we need to create for the file system.</p>
<h3 id="sourcefile"><a class="header" href="#sourcefile"><code>SourceFile</code></a></h3>
<p>We need to store our source files somewhere. Let's create a <code>SourceFile</code> struct to hold the file's name and the source code. We'll define some simple functions for creating them too.</p>
<p>We will use <code>ariadne::Source</code> to make sure it's compatible with the library; we can easily create it from source code by <code>Source::from(source_code: String)</code>.</p>
<pre><code class="language-rust ignore">use ariadne::Source;

/// Represents a source file
pub struct SourceFile {
    pub id: String,
    pub source: Source,
}

impl SourceFile {
    /// Create a new `SourceFile`
    pub fn new(id: String, contents: String) -&gt; Self {
        Self {
            id,
            source: Source::from(contents),
        }
    }

    /// Create a new `SourceFile` from a string
    /// Name is set to "src"
    pub fn from_code(contents: String) -&gt; Self {
        Self {
            id: String::new(),
            source: Source::from(contents),
        }
    }
}</code></pre>
<p>Great, we can create a source file from a string, and we can also create a source file from a file name and its contents.</p>
<h3 id="files"><a class="header" href="#files"><code>Files</code></a></h3>
<p>Here, we'll create a <code>struct</code> to manage all these <code>SourceFile</code>s. We'll be able to add files, get files, and get the source code of a file.</p>
<p>There are some other features we might find useful:</p>
<ul>
<li><code>get_file</code> to get a file by its name</li>
<li><code>get_source</code> to get the source code of a file by its name</li>
<li><code>add_file</code> to add a file to the collection</li>
</ul>
<p>In <code>files.rs</code>:</p>
<pre><code class="language-rust ignore">use std::collections::HashMap;

/// Represents a collection of source files
pub struct Files {
    files: HashMap&lt;String, SourceFile&gt;,
}

impl Files {
    /// Create a new `Files`
    pub fn new() -&gt; Self {
        Self {
            files: HashMap::new(),
        }
    }

    /// Add a file to the collection
    pub fn add_file(&amp;mut self, id: String, contents: String) {
        let file = SourceFile::new(id, contents);
        self.files.insert(file.id.clone(), file);
    }

    /// Get a file from the collection
    fn get_file(&amp;self, path: &amp;str) -&gt; Option&lt;&amp;SourceFile&gt; {
        self.files.get(path)
    }
}</code></pre>
<p>This "filesystem" is essentially what <code>ariadne</code> considers a <strong>Cache</strong>. This cache essentially is a store of source files (dependant on the file ID), and we can use it to generate diagnostics.</p>
<p>Because of the way we designed our struct, implementing the <a href="https://docs.rs/ariadne/*/ariadne/trait.Cache.html"><code>Cache</code> trait</a> is simple. We can use the <code>get</code> method to get a file by its ID, and the <code>get_source</code> method to get the source code of a file by its ID.</p>
<pre><code class="language-rust ignore">impl ariadne::Cache&lt;String&gt; for Files {
    type Storage = String;

    fn fetch(
        &amp;mut self,
        id: &amp;String,
    ) -&gt; Result&lt;&amp;Source&lt;Self::Storage&gt;, Box&lt;dyn std::fmt::Debug + '_&gt;&gt; {
        let file = self.get_file(id).unwrap();
        Ok(&amp;file.source)
    }

    fn display&lt;'a&gt;(&amp;self, id: &amp;'a String) -&gt; Option&lt;Box&lt;dyn fmt::Display + 'a&gt;&gt; {
        let file = self.get_file(id)?;
        Some(Box::new(file.id.clone()))
    }
}</code></pre>
<p>The <code>Storage</code> type declares exactly what the ID is (we refer to files by <code>String</code>s here). The <code>fetch</code> method is used to get the source code of a file by its ID, and the <code>display</code> method is used to display the file's name.</p>
<p>In the <code>fetch</code> method, we simply call the function we defined earlier to get the file by its ID, and then return the source code. In the <code>display</code> method, we get the file by its ID, and then return the file's name.</p>
<h2 id="working-with-spans"><a class="header" href="#working-with-spans">Working with spans</a></h2>
<p><code>ariadne</code> really wants us to use spans which it can use, through the <a href="https://docs.rs/ariadne/latest/ariadne/trait.Span.html"><code>ariadne::Span</code> trait</a>. This requires us to actually have the file ID of the span, which (un)fortunately our current <code>Span</code> lacks. We would prefer not to store the file ID in the <code>Span</code> itself -- storing it for each token is memory-inefficient.</p>
<p>Instead, we can determine that the only time we would need to access the <code>Span</code> in a reporting manner is, of course, when we need to report an error. Thus, we can create a new <code>struct</code> which is generated on the fly, which includes the needed information.</p>
<details>
<summary>Why do we need to store the file ID?</summary>
We'd need to implement the trait later on, which means we need to store the file ID in the struct itself. If you can think of a way to retrieve the file ID without storing it, that would be interesting.
</details>
<pre><code class="language-rust ignore">/// Ariadne-compatible span
/// To be generated on-the-fly.
#[derive(Debug, Clone, PartialEq)]
pub struct ReportableSpan {
    pub file: String,
    pub start: usize,
    pub end: usize,
}

impl ReportableSpan {
    pub fn new(file: String, span: &amp;Span) -&gt; Self {
        Self {
            file,
            start: span.start,
            end: span.end,
        }
    }
}</code></pre>
<p>This <code>struct</code> is essentially a wrapper around the <code>Span</code> struct, but it includes the file ID. We can create it from a <code>Span</code> and a file ID, and then use it to generate diagnostics.</p>
<p>Before it's ready for use, we need to implement <code>ariadne::Span</code> to it:</p>
<pre><code class="language-rust ignore">impl ariadne::Span for ReportableSpan {
    type SourceId = String;

    fn source(&amp;self) -&gt; &amp;Self::SourceId {
        &amp;self.file
    }

    fn start(&amp;self) -&gt; usize {
        self.start
    }

    fn end(&amp;self) -&gt; usize {
        self.end
    }
}</code></pre>
<h2 id="generating-diagnostics"><a class="header" href="#generating-diagnostics">Generating Diagnostics</a></h2>
<p>Here's an example error message we'll be able to generate later:</p>
<pre><code class="language-plaintext">Error:
   ╭─[tests/test:1:12]                                      // File:Line:Column
   │
 5 │     if name == 335.333.2 { return true; }              // Source from span
   │                ───────┬─
   │                       ╰──── Unexpected character: `.`  // A label with a message
───╯
</code></pre>
<p>Pretty, right? We can build this using <code>Report::build</code>!</p>
<p>You can build an error with a <code>ReportKind</code> (i.e. <code>ReportKind::Error</code>), the file ID, and the span. After that, you can add whatever you like, such as labels with their own spans and messages and colors.</p>
<p>Let's implement a function to generate a diagnostic across any <code>LangError</code> variant.</p>
<p>In <code>errors.rs</code>:</p>
<pre><code class="language-rust ignore">use ariadne::{Color, Label, Report, ReportKind};
use crate::token::{ReportableSpan, Span};

impl LangError {
    /// Produces a diagnostic for the error
    pub fn diagnostic(&amp;self, file: String) -&gt; Report&lt;ReportableSpan&gt; {
        // Get the span of the error
        let span = self.span(file);

        // This is pretty much the error message
        let label = Label::new(span.clone())
            .with_message(self.to_string())
            .with_color(Color::Red);

        // Create an `ReportKind::Error` report with the message
        Report::build(ReportKind::Error, self.to_string(), span.start)
            .with_label(label)
            .finish()
    }

    /// Get the span of the error
    pub fn span(&amp;self, file: String) -&gt; ReportableSpan {
        ReportableSpan::new(
            file,
            match self {
                LangError::UnexpectedCharacter(_, span) =&gt; span,
                LangError::UnterminatedString(span) =&gt; span,
                LangError::ExpectedToken { span, .. } =&gt; span,
                LangError::ExpectedAnyToken { span, .. } =&gt; span,
                LangError::UnexpectedEOF(span) =&gt; span,
                LangError::InvalidLiteral(_, span) =&gt; span,
            },
        )
    }
}</code></pre>
<p>We also included a little helper function to extract the span out of errors, since because of a silly design decision, the position of the span in the error is not consistent. This might not be needed if you have a better design!</p>
<p>Note that <code>self.to_string()</code> is thanks to <code>thiserror</code>'s implementation using the strings we made above each variant.</p>
<h2 id="error-reporting"><a class="header" href="#error-reporting">Error Reporting</a></h2>
<p>To streamline the error process, we can create our own <code>ErrorReporter</code> struct that will handle the file system, and report all the diagnostics at the end.</p>
<p>It'll have a <strong>mutable reference</strong> to the file system (necessary for diagnostic printing).</p>
<pre><code class="language-rust ignore">pub struct ErrorReporter&lt;'a&gt; {
    files: &amp;'a mut Files,
}</code></pre>
<p>We can create a <code>report</code> method for a single error. We'll simply want to generate a <code>Diagnostic</code> and then render it to the terminal (or another output).</p>
<pre><code class="language-rust ignore">impl&lt;'a&gt; ErrorReporter&lt;'a&gt; {
    pub fn report(&amp;mut self, file: String, error: &amp;anyhow::Error) -&gt; Result&lt;()&gt; {
        let diagnostic = match error.downcast_ref::&lt;LangError&gt;() {
            Some(e) =&gt; e.diagnostic(file),
            None =&gt; {
                return Err(anyhow!("Unknown error: {}", error));
            }
        };

        match diagnostic.eprint(&amp;mut self.files) {
            Ok(_) =&gt; Ok(()),
            Err(e) =&gt; Err(anyhow!("Failed to print diagnostic: {}", e)),
        }
    }
}</code></pre>
<p>This is moderately complex. We first need to downcast the error to a <code>LangError</code>, and then we can generate a diagnostic from it. After that, we can print the diagnostic to the terminal. We make sure to catch any possible errors which may arise.</p>
<h1 id="resources-1"><a class="header" href="#resources-1">Resources</a></h1>
<ul>
<li><a href="https://docs.rs/ariadne/*/ariadne/">ariadne documentation</a></li>
<li><a href="https://docs.rs/ariadne/latest/ariadne/trait.Span.html">ariadne Span trait</a></li>
<li><a href="https://docs.rs/ariadne/*/ariadne/trait.Cache.html">ariadne Cache trait</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="error-handling-with-codespan-reporting"><a class="header" href="#error-handling-with-codespan-reporting">Error Handling with <code>codespan-reporting</code></a></h1>
<blockquote>
<p>⚠️ We will not be using this crate, but it's a simpler alternative to <code>ariadne</code>.</p>
</blockquote>
<p>We'll use <code>codespan-reporting</code> to design prettier error messages. This library allows us to highlight the source code where the error occurred, and provide a helpful message to the user. Plus, it's incredibly useful!</p>
<h2 id="error-diagnostics"><a class="header" href="#error-diagnostics">Error Diagnostics</a></h2>
<p>It's important to display the errors properly, so that the user can see exactly where the error occurred, why it occurred, and other useful information to help them debug the issue.</p>
<p>Using <code>codespan-reporting</code> we can generate a <code>Diagnostic</code> for each error, and then render the diagnostics to the terminal. The <code>Diagnostic</code> will contain the error message, the source code, and the span of the error.</p>
<p>Let's implement a function to generate a <code>Diagnostic</code> for each <code>LangError</code> variant.</p>
<h3 id="diagnostics"><a class="header" href="#diagnostics">Diagnostics</a></h3>
<p>Before we jump into making the function, it's important to review the <a href="https://docs.rs/codespan-reporting/0.11.1/codespan_reporting/">documentation</a> (as always), and the <a href="https://crates.io/crates/codespan-reporting">example</a> can help too.</p>
<p>Here's a simple uncolored error message we'll be able to generate later:</p>
<pre><code>error: Unexpected character: `.`                    // Message
  ┌─ test:1:12                                      // File:Line:Column
  │
1 │ if name == 335.333.2 { return true; }           // Source from span
  │            ^^^^^^^^^ Unexpected character: `.`  // A label with a message
</code></pre>
<p>Here the literal uses two decimal points (which is invalid), and the error message is displayed with the source code and the span of the error. I've labelled what each part of the error message is.</p>
<p>We can determine that a <code>Diagnostic</code> error can be created by <code>Diagnostic::error()</code>. From here, we can add a message, and add as many labels as we want. We can add a span too, which is the range of the error in the source code.</p>
<h3 id="file-systems"><a class="header" href="#file-systems">File systems</a></h3>
<p>In <code>codespan-reporting</code>, we have a simple file system  <code>SimpleFiles&lt;Name, Source&gt;</code> we'll use. This essentially allows you to load a file into memory (which you get an associated file ID), and then you can use the file ID to generate diagnostics.</p>
<h3 id="generating-diagnostics-1"><a class="header" href="#generating-diagnostics-1">Generating Diagnostics</a></h3>
<p>Let's implement a function to generate a <code>Diagnostic</code> for each <code>LangError</code> variant.</p>
<pre><code class="language-rust ignore">impl LangError {
    /// Produces a diagnostic for the error
    pub fn diagnostic(&amp;self, file_id: usize) -&gt; Diagnostic&lt;usize&gt; {
        // Generate the base diagnostic with a main message
        let mut diagnostic = Diagnostic::error().with_message(self.to_string());

        // Add a label
        match self {
            LangError::UnexpectedCharacter(ch, span) =&gt; {
                diagnostic =
                    diagnostic.with_labels(vec![Label::primary(file_id, span.start..span.end)
                        .with_message(format!("Unexpected character: `{}`", ch))]);
            },
            // ...
        }

        diagnostic
    }
}</code></pre>
<p>You can implement the rest of the variants in the <code>match</code> statement. It would be ideal for the message to briefly describe the overall error, and labels specifying exactly what went wrong.</p>
<h2 id="error-reporting-1"><a class="header" href="#error-reporting-1">Error Reporting</a></h2>
<p>To streamline the error process, we can create our own <code>ErrorReporter</code> struct that will handle the file system, and report all the diagnostics at the end.</p>
<pre><code class="language-rust ignore">pub struct ErrorReporter&lt;'a&gt; {
    /// Store the files in memory
    files: &amp;'a SimpleFiles&lt;String, String&gt;,
    /// Store where to write the diagnostics
    writer: StandardStream,
    /// Store the configuration for the terminal
    config: codespan_reporting::term::Config,
}</code></pre>
<p>You can determine how it can be constructed, but the key concept for us is that the immutable reference <code>&amp;'a SimpleFiles&lt;String, String&gt;</code> implies we do not add any files.</p>
<p>Next, we can create a <code>report</code> method for a single error. We'll simply want to generate a <code>Diagnostic</code> and then render it to the terminal (or another output).</p>
<pre><code class="language-rust ignore">impl&lt;'a&gt; ErrorReporter&lt;'a&gt; {
    pub fn report(&amp;self, file_id: usize, error: &amp;anyhow::Error) -&gt; anyhow::Result&lt;()&gt; {
        // Try to downcast the error to a LangError
        let diagnostic = if let Some(lang_error) = error.downcast_ref::&lt;LangError&gt;() {
            lang_error.diagnostic(file_id)
        } else {
            // If it fails somehow, just create a generic error
            Diagnostic::error().with_message(error.to_string())
        };

        // Render the diagnostic to the terminal
        codespan_reporting::term::emit(
            &amp;mut self.writer.lock(),
            &amp;self.config,
            self.files,
            &amp;diagnostic,
        )?;

        Ok(())
    }
}</code></pre>
<p>To make life easier, we can introduce a <code>report_all</code> method to report all the errors at once. This will be useful when we want to report all the errors at the end of the compilation process.</p>
<pre><code class="language-rust ignore">impl&lt;'a&gt; ErrorReporter&lt;'a&gt; {
    // ...

    pub fn report_all(&amp;self, errors: Vec&lt;(usize, &amp;anyhow::Error)&gt;) -&gt; anyhow::Result&lt;()&gt; {
        for (file_id, error) in errors {
            self.report(file_id, error)?;
        }

        Ok(())
    }
}</code></pre>
<h1 id="resources-2"><a class="header" href="#resources-2">Resources</a></h1>
<ul>
<li><a href="https://docs.rs/codespan-reporting/0.11.1/codespan_reporting/">codespan-reporting</a></li>
<li><a href="https://docs.rs/thiserror/1.0.24/thiserror/">thiserror</a></li>
<li><a href="https://crates.io/crates/codespan-reporting">codespan-reporting example</a></li>
<li><a href="https://docs.rs/codespan-reporting/0.11.1/codespan_reporting/files/struct.SimpleFiles.html">SimpleFiles</a></li>
<li><a href="https://docs.rs/codespan-reporting/0.11.1/codespan_reporting/diagnostic/struct.Diagnostic.html">Diagnostic</a></li>
<li><a href="https://docs.rs/codespan-reporting/0.11.1/codespan_reporting/diagnostic/struct.Label.html">Label</a></li>
<li><a href="https://en.wikipedia.org/wiki/Error_recovery">Error Recovery</a></li>
<li><a href="https://go.dev/blog/defer-panic-and-recover">Panic vs Recover in Go</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lexical-analysis"><a class="header" href="#lexical-analysis">Lexical Analysis</a></h1>
<p>In this chapter, we'll cover the theory and implement a working lexer.</p>
<h2 id="introduction-2"><a class="header" href="#introduction-2">Introduction</a></h2>
<p>The primary purpose of lexical analysis is to simplify the subsequent phases of the compiler (the parser!) by providing an unstructured categorized representation of the source code as a stream of <strong>tokens</strong>. This can be done in a few ways.</p>
<p>Regular expressions are patterns used to define the syntax of tokens in a programming language, used by tokenizers/lexers to recognize tokens. This guide will not use regular expressions. This is mainly because it's inefficient (lots of backtracking), is very deterministic, and is difficult to debug and control. I don't like regex either.</p>
<p>Finite automata, including deterministic finite automata (DFAs) and non-deterministic finite automata (NFAs), serve as theoretical models for defining and implementing lexical analyzers.</p>
<p>We will go into more detail in later sub-sections. For now, let's get started!</p>
<h2 id="our-language"><a class="header" href="#our-language">Our Language</a></h2>
<p>Before we start, we should get a clear idea of what our language should look like. We'll keep it simple, with the following features:</p>
<ul>
<li>Types (integer and boolean), i.e. <code>int</code>, <code>bool</code></li>
<li>Variables, i.e. <code>let x: int = 5;</code></li>
<li>Arithmetic operations (<code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>), i.e. <code>let x: int = 5 + 3;</code></li>
<li>Parentheses, i.e. <code>let x: int = (5 + 3) * 2;</code></li>
<li>If statements, i.e. <code>if (x &gt; 5) { ... } else { ... }</code></li>
</ul>
<p>We would like it to be <strong>strongly typed</strong>, so we can't do things like <code>let x: int = true;</code>. We'll also have a few reserved keywords, such as <code>let</code>, <code>if</code>, <code>else</code>, <code>int</code>, <code>bool</code>. Each function should have a return type, and we'll have a few built-in functions.</p>
<p>Let's create a simple example code snippet for our language:</p>
<pre><pre class="playground"><code class="language-rust">// Declare a function `main()`, which returns an integer (exit code)
fn main() -&gt; int {
    let x: int = 5;
    let y: int = 3;
    let z: int = x + y;
    if (z &gt; 5) {            // Branching!
        return z;           // Return z
    } else {
        return 0;           // Return 0
    }
}</code></pre></pre>
<p>This is a simple program which declares two variables, adds them together, and returns the result if it's greater than 5. Otherwise, it returns 0. It follows a rust-like syntax, but it's not exactly Rust.</p>
<p>Anyways, let's break it down into tokens:</p>
<ol>
<li><code>fn</code>: Keyword declaring a function</li>
<li><code>main</code>: Identifier for the function</li>
<li><code>(</code>: Parenthesis</li>
<li><code>)</code>: Parenthesis</li>
<li><code>-&gt;</code>: Arrow, indicating the return type</li>
<li><code>int</code>: Keyword for the return type</li>
<li><code>{</code>: Curly brace</li>
<li><code>let</code>: Keyword for declaring a variable</li>
<li><code>x</code>: Identifier for the variable</li>
<li><code>:</code>: Colon, indicating the type</li>
<li><code>int</code>: Keyword for the type</li>
<li><code>=</code>: Assignment operator</li>
<li><code>5</code>: Integer literal</li>
<li><code>;</code>: Semicolon, indicating the end of the statement</li>
</ol>
<p>And so forth. This will help us understand how we can structure our tokens.</p>
<h1 id="resources-3"><a class="header" href="#resources-3">Resources</a></h1>
<ul>
<li><a href="https://github.com/leonardomso/awesome-fsm">Awesome FSM</a>: A curated list of awesome finite state machine libraries, software and resources.</li>
<li><a href="https://doc.rust-lang.org/std/iter/trait.Iterator.html">Iterator trait</a>: The official documentation for the <code>Iterator</code> trait.</li>
<li><a href="https://doc.rust-lang.org/rust-by-example/scope/lifetime.html">Lifetimes in Rust</a></li>
<li><a href="./finite_automata.html">Finite Automata</a> in this guide!</li>
<li><a href="https://medium.com/@tarungudipalli/exploring-rusts-string-a-comprehensive-guide-with-examples-25f398ade356">Rust's Strings</a>: A comprehensive guide to Rust's strings.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="finite-automata"><a class="header" href="#finite-automata">Finite Automata</a></h1>
<p>Finite Automata (FAs) are theoretical models we use to implement lexical analyzers (aka lexers, tokenizers). These models define how we transition between states (how we move between the current symbol and the next), allowing the analyzer to recognize tokens by traversing the automaton.</p>
<p>There are two groups we will describe below:</p>
<h2 id="deterministic-finite-automata"><a class="header" href="#deterministic-finite-automata">Deterministic Finite Automata</a></h2>
<p>DFAs are state machines where, given an input symbol, there is exactly <strong>one next state</strong>. Each token type is represented as a state, and transitions between states are determined by the source code. While the notation may seem scary, re-reading it will help you understand it's just fancy-speak!</p>
<p>Before we recognize some identifiers, there are of-course rules:</p>
<ul>
<li>The identifier must start with a letter
<ul>
<li>Typically underscores <code>_</code> are allowed but for simplicity, letters</li>
</ul>
</li>
<li>The identifier does not contain whitespace</li>
<li>The identifier may contain numbers <em>after</em> the first symbol</li>
</ul>
<p>With these rules, we can construct the transition function, as below.</p>
<p>Consider a simple DFA for recognizing identifiers in a programming language:</p>
<ul>
<li>States: \(Q = {q_{start}, q_{identifier}}\)</li>
<li>Alphabet: \(\sum = \{\text{Letters}, \text{Digits}\}\)</li>
<li>Transition function:
$$\delta(q, a) = \begin{cases}
q_{identifier} &amp;\text{if} \ q = q_{start} \ \text{and} \ a \in \text{Letters} \\
q_{identifier} &amp;\text{if} \ q = q_{identifier} \ \text{and} \ a \in \text{Letters} \ \bigcup \ \text{Digits} \\
\text{undefined} &amp;\text{otherwise}
\end{cases}$$<sup class="footnote-reference"><a href="#note">1</a></sup></li>
<li>Input: <code>"variable123 ..."</code></li>
</ul>
<p>By DFA, we will follow this process:</p>
<ol>
<li>Start in \(q_{start}\)</li>
<li>Transitions to \(q_{identifier}\) on encountering a letter <code>v</code>
<ol>
<li>Stays in \(q_{identifier}\) for subsequent letters/digits <code>ariable123</code></li>
</ol>
</li>
<li>Reaches end in \(q_{identifier}\) at whitespace <code> </code></li>
<li>Recognises <code>variable123</code> as an identifier</li>
</ol>
<p>The transition function \(\delta\) determines how the state should transition, depending on the given conditions.</p>
<h2 id="non-deterministic-finite-automata"><a class="header" href="#non-deterministic-finite-automata">Non-Deterministic Finite Automata</a></h2>
<p>NFAs are state machines where, given an input symbol, there can be <strong>multiple possible next states</strong> (as opposed to exactly one in DFAs). We would use these in lexical analysis as a <em>theoretical basis</em>, before using DFAs.</p>
<p>Before we recognize some floating-point literals, we must identify the rules:</p>
<ul>
<li>Literals must contain digits, and no letters</li>
<li>Literals may contain a single decimal point <code>.</code></li>
<li>Literals are defined in two parts: the integer part and the fractional part
<ul>
<li>The integer part represents the whole number section of the literal (i.e. <code>3</code>)</li>
<li>The fractional part represents the decimal section of the literal (ie. <code>.14</code>)</li>
</ul>
</li>
</ul>
<p>And thus, we can construct a transition function as below!</p>
<p>Consider an NFA for recognizing floating-point literals:</p>
<ul>
<li>States: \(Q = {q_{start}, q{int}, q_{decimal}, q_{frac}}\)
<ul>
<li>\(q_{int}\), \(q_{frac}\) are the integer and fractional parts respectively</li>
<li>\(q_{decimal}\) is the presence of a decimal point</li>
</ul>
</li>
<li>Alphabet: \(\sum = \{\text{Digits}, \text{Decimal Point}\}\)
<ul>
<li>Keep in mind "Alphabet" is simply the set of parse-able symbols</li>
</ul>
</li>
<li>Transition function:
$$\delta(q, a) = \begin{cases}
q_{int} \ \text{if} &amp;q = q_{start} \ \text{and} \ a \in \text{Digits} \\
q_{decimal} \ \text{if} &amp;q = q_{intPart} \ \text{and} \ a \in \text{Decimal Point} \\
q_{frac} \ \text{if} &amp;q = q_{decimalPoint} \ \text{and} \ a \in \text{DIGITS} \\
\text{undefined} &amp;\text{otherwise}
\end{cases}$$<sup class="footnote-reference"><a href="#note">1</a></sup></li>
<li>Input: <code>"3.14"</code></li>
</ul>
<p>By NFA, we will follow this process:</p>
<ol>
<li>Start in \(q_{start}\)</li>
<li>Transition to \(q_{int}\) for <code>3</code></li>
<li>Transition to \(q_{decimal}\) for <code>.</code></li>
<li>Transition to \(q_{frac}\) for <code>1</code>
<ol>
<li>Remain in \(q_{frac}\) for <code>4</code></li>
</ol>
</li>
<li>End or whitespace is reached</li>
<li>Recognises <code>3.14</code> as a floating-point literal: <code>3</code> for integer part, <code>14</code> for fractional part.</li>
</ol>
<p>Again, the transition function \(\delta\) determines how the state should transition, <strong>and</strong> to which state it should transition to, depending on the given conditions.</p>
<h2 id="which-finite-automata-do-we-use"><a class="header" href="#which-finite-automata-do-we-use">Which Finite Automata do we use?</a></h2>
<p>The clear difference between the two is the DFAs are much more simplistic and efficient, with clear and optimal state transitions. NFAs, however, allow for multiple possible states and are much more flexible.</p>
<p>We will choose to use Deterministic Finite Automata for its simplicity! We will not have an explicit handling on non-determinism, meaning we will not branch or have multiple possible transitions based on an input character. In more simple terms: at each character symbol, we will not consider multiple state possibilities to transition into, but instead exactly one next state.</p>
<p>In a nutshell, we chose to not complicate life more than it already is.</p>
<h1 id="resources-4"><a class="header" href="#resources-4">Resources</a></h1>
<ul>
<li><a href="https://github.com/leonardomso/awesome-fsm">Awesome FSM</a>: A curated list of awesome finite state machine libraries, software and resources.</li>
</ul>
<div class="footnote-definition" id="note"><sup class="footnote-definition-label">1</sup>
<p>The second parameter \(a\) is the input symbol. The last case is implies end of local input.</p>
</div>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tokens"><a class="header" href="#tokens">Tokens</a></h1>
<p>To implement a lexer, we'll have to go through multiple steps. First, let's declare the different kinds of tokens we might encounter and want to parse. They would serve as the building blocks of the language before it can be understood.</p>
<p>In <code>token.rs</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Debug, Clone, PartialEq)]
pub enum TokenKind {
    // keywords
    Fn,
    If,
    Else,
    While,
    For,
    Return,
    Let,
    True,
    False,

    // single-character tokens
    Plus,      // +
    Minus,     // -
    Star,      // *
    Slash,     // /
    Percent,   // %
    Caret,     // ^
    Bang,      // !
    Colon,     // :
    Semicolon, // ;
    Comma,     // ,
    Dot,       // .
    Equals,    // =
    Less,      // &lt;
    Greater,   // &gt;
    LParen,    // (
    RParen,    // )
    LBrace,    // {
    RBrace,    // }
    Quote,     // "

    // double-character tokens
    PlusEquals,    // +=
    MinusEquals,   // -=
    StarEquals,    // *=
    SlashEquals,   // /=
    PercentEquals, // %=
    CaretEquals,   // ^=
    BangEquals,    // !=
    LessEquals,    // &lt;=
    GreaterEquals, // &gt;=
    EqualsEquals,  // ==
    Arrow,         // -&gt;

    // literals
    Ident(String),
    IntLiteral(i32),
    FloatLiteral(f32),
    BoolLiteral(bool),

    // Data types
    Int,
    Bool,
    Float,

    // End of file
    Eof,
}
<span class="boring">}</span></code></pre></pre>
<p>We split these into a few groups:</p>
<ol>
<li><strong>Keywords</strong>: These are the reserved keywords, e.g. for declaring functions</li>
<li><strong>Single-character tokens</strong>: Self-explanatory.</li>
<li><strong>Multi-character tokens</strong>: Tokens which are multiple (typically two) characters</li>
<li><strong>Datatypes</strong>: The primitive datatypes like <code>int</code></li>
<li><strong>End of File</strong>: This token signifies the end of the token stream.</li>
</ol>
<p>This is great, but it is important to think ahead. If we encounter an error during the lexing process, unless we keep track of the actual start &amp; end positions (introducing complexity), we won't be able to show the user where the error is.</p>
<p>We will declare a <code>Token</code> and <code>Span</code> struct which will store both the kind of token it is, and where it spans in the source code:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Debug, Clone, PartialEq, Hash)]
pub struct Token {
    pub kind: TokenKind,
    pub span: Span,
}

#[derive(Debug, Clone, PartialEq, Hash)]
pub struct Span {
    pub start: usize,
    pub end: usize,
}
<span class="boring">}</span></code></pre></pre>
<p>It is important to note they are declared with <code>pub</code> to ensure they're accessible outside the module.</p>
<p>Let's also add a helper function which will convert a string to a keyword or identifier; this is because when lexing, we may be unsure if a string is a keyword or not. This will prioritise the keyword so they do not be overwritten by an identifier.</p>
<p>Additionally, we will create a constructor to convert a string to an actual <code>TokenKind</code> operator. It is cleaner to keep certain functionalities within this place instead of the lexer since it's more tidy.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl TokenKind {
    /// Get operator from a string
    pub fn operator_from(op: &amp;str) -&gt; TokenKind {
        match op {
            "+" =&gt; TokenKind::Plus,
            "-" =&gt; TokenKind::Minus,
            "*" =&gt; TokenKind::Star,
            "/" =&gt; TokenKind::Slash,
            "%" =&gt; TokenKind::Percent,
            "^" =&gt; TokenKind::Caret,
            "!" =&gt; TokenKind::Bang,
            ":" =&gt; TokenKind::Colon,
            ";" =&gt; TokenKind::Semicolon,
            "," =&gt; TokenKind::Comma,
            "." =&gt; TokenKind::Dot,
            "=" =&gt; TokenKind::Equals,
            "&lt;" =&gt; TokenKind::Less,
            "&gt;" =&gt; TokenKind::Greater,
            "+=" =&gt; TokenKind::PlusEquals,
            "-=" =&gt; TokenKind::MinusEquals,
            "*=" =&gt; TokenKind::StarEquals,
            "/=" =&gt; TokenKind::SlashEquals,
            "%=" =&gt; TokenKind::PercentEquals,
            "^=" =&gt; TokenKind::CaretEquals,
            "!=" =&gt; TokenKind::BangEquals,
            "&lt;=" =&gt; TokenKind::LessEquals,
            "&gt;=" =&gt; TokenKind::GreaterEquals,
            "==" =&gt; TokenKind::EqualsEquals,
            "-&gt;" =&gt; TokenKind::Arrow,
            _ =&gt; panic!("Unknown operator: {}", op),
        }
    }

	/// Get keyword from a string
    pub fn keyword_from(keyword: &amp;str) -&gt; TokenKind {
        match keyword {
            "fn" =&gt; TokenKind::Fn,
            "if" =&gt; TokenKind::If,
            "else" =&gt; TokenKind::Else,
            "while" =&gt; TokenKind::While,
            "for" =&gt; TokenKind::For,
            "return" =&gt; TokenKind::Return,
            "let" =&gt; TokenKind::Let,
            "true" =&gt; TokenKind::True,
            "false" =&gt; TokenKind::False,
            "int" =&gt; TokenKind::Int,
            "bool" =&gt; TokenKind::Bool,
            "float" =&gt; TokenKind::Float,
            _ =&gt; TokenKind::Ident(keyword.to_string()),
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<p>This is tons of boilerplate code, but it's important we get this right; we'll be using them throughout the first two stages of the compilation process, until we construct an AST.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="implementing-a-lexer"><a class="header" href="#implementing-a-lexer">Implementing a Lexer</a></h1>
<p>Now that we have our tokens declared, we can start implementing our lexer. The lexer will take in a string of source code and output a list of tokens. We'll use a simple state machine to do this, as mentioned in <a href="./finite_automata.html">Finite Automata</a>.</p>
<p>Keep in mind we will not be explicitly storing States and Transitions, but instead it'll be implemented implicitly in the code.</p>
<h2 id="what-are-iterators"><a class="header" href="#what-are-iterators">What are Iterators?</a></h2>
<p>Rust includes the concept of <strong>Iterators</strong>, which essentially allow you to iterate over some sort of collection (e.g. the source code characters) and perform some action on each item. This is a very powerful concept, and we'll use it to our advantage.</p>
<p>We will use the <a href="https://doc.rust-lang.org/std/iter/trait.Iterator.html"><code>Iterator</code> trait</a> to apply the iterator concept to our lexer. As stated, our lexer will be a Deterministic Finite Automaton (DFA). We will use the <code>Iterator</code> trait to iterate over the characters of the source code, and the state machine will consume these characters and transition to the next state.</p>
<h2 id="the-lexer"><a class="header" href="#the-lexer">The Lexer</a></h2>
<p>Let's start by creating a new file called <code>lexer.rs</code> in the <code>src</code> directory. This file will contain our lexer implementation. We'll start by defining the <code>Lexer</code> struct and implementing the <code>Iterator</code> trait for it.</p>
<pre><code class="language-rust ignore">use anyhow::{anyhow, Error, Result};

use crate::errors::LangError;
use crate::token::{Span, Token, TokenKind};

/// A lexer iterator.
pub struct Lexer&lt;'a&gt; {
    src: &amp;'a str,
    pos: usize,
}</code></pre>
<p>The lexer will contain a reference to the source code <code>src</code>, and a <code>pos</code> field to keep track of the current position in the source code. We would rather use <code>&amp;'a str</code> instead of <code>String</code> to avoid unnecessary allocations; we won't be modifying the source code, so we don't need to own it.</p>
<blockquote>
<p><strong>Note</strong></p>
<p>Some helper functions (e.g. <code>advance</code>, <code>peek</code>) are not shown here, but they will be used in the implementation. They are simple and self-explanatory, and you can find them in the full source code.</p>
</blockquote>
<h2 id="dfa-model"><a class="header" href="#dfa-model">DFA Model</a></h2>
<p>Before we implement the <code>Iterator</code> trait, we need to understand exactly how iteration will work here.</p>
<p>We will use a simple DFA model to implement our lexer. The DFA will have a set of states, and each state will have a set of transitions to other states. The transitions will be based on the current character being processed.</p>
<p>Consider:</p>
<ul>
<li>States: \( Q = \{q_{start}, q_{id}, q_{kw}, q_{num}, q_{op}, q_{whitespace}, q_{eof}\} \)</li>
<li>Alphabet: \( \sum = \{\text{Letters}, \text{Digits}, \text{Operators}, \text{Symbols}\} \)
<ul>
<li>\(\text{Operators}\) represent both single-char and multi-char operators.</li>
</ul>
</li>
</ul>
<p>Let's derive a transition function! We should define these for each state. We will exclude numbers and identifiers since they were explored in <a href="./finite_automata.html">Finite Automata</a>.</p>
<h3 id="tokenizing-operators"><a class="header" href="#tokenizing-operators">Tokenizing operators</a></h3>
<p>The major case we will need to handle is tokenizing operators which may or may not be multi-character.</p>
<p>Take <code>+</code> and <code>+=</code> as an example: the lexer should recognize <code>+</code> as an operator token, and <code>+=</code> as an operator token as well. The lexer should not tokenize <code>+</code> as a single <code>+</code> token and <code>=</code> as a separate token.</p>
<p>We can define the transition functions for the \(q_{op}\) state as follows:</p>
<p>$$\begin{align*} \delta(q_{op}, a) &amp;= \begin{cases} q_{op}^{(1)} &amp; \exists s_i \in \text{ 2-char Operators}: \text{a startswith } s_i \\ q_{op} &amp; \exists s_i\in \text{ 1-char Operators}: \text{a startswith } s_i \\ \text{null} &amp; \text{otherwise} \end{cases} \\ \delta(q_{op}^{(1)}, b) &amp;= \begin{cases} q_{op} &amp; ab \in \text{ 2-char Operators} \\ \text{null} &amp; \text{otherwise} \end{cases} \\
\end{align*}$$</p>
<p>Essentially, this just means depending on the start of the operator, we will transition to a new state \(q_{op}^{(1)}\) (the state for multi-char operators) and then transition back to the \(q_{op}\) state if the next character is not part of a multi-char operator.</p>
<h3 id="tokenizing-comments-and-whitespace"><a class="header" href="#tokenizing-comments-and-whitespace">Tokenizing comments and whitespace</a></h3>
<p>We will also need to handle comments and whitespace. Typically, we would ignore them unless they hold some significance (e.g. a comment token).</p>
<p>When we transition into the \(q_{comment}\) state, we will simply consume until a newline is encountered. We will then transition back to the \(q_{start}\) state:</p>
<p>$$\delta(q_{comment}, a) = \begin{cases} q_{comment} &amp; a \neq \text{newline} \\ q_{start} &amp; a = \text{newline} \end{cases}$$</p>
<p>The \(q_{whitespace}\) state will be similar, but we will transition back to the \(q_{start}\) state on encountering a non-whitespace character:</p>
<p>$$\delta(q_{whitespace}, a) = \begin{cases} q_{whitespace} &amp; a = \text{whitespace} \\ q_{start} &amp; a \neq \text{whitespace} \end{cases}$$</p>
<h3 id="tokening-symbols"><a class="header" href="#tokening-symbols">Tokening symbols</a></h3>
<p>This is the easiest case to handle. We will simply transition to a final state when we encounter a symbol:</p>
<p>$$\delta(q_{symbol}, a) = \begin{cases} q_{symbol} &amp; \text{if a is a symbol} \end{cases}$$</p>
<h3 id="helping-dfa-realise-its-over"><a class="header" href="#helping-dfa-realise-its-over">Helping DFA realise its over</a></h3>
<p>We will also need to handle the end of the file. We will transition to the \(q_{eof}\) state when we reach the end of the file:</p>
<p>$$\delta(q_{eof}, a) = \begin{cases} q_{eof} &amp; \text{EOF} \end{cases}$$</p>
<p>It should simply never transition out of the state, as there are no more characters to process.</p>
<h2 id="implementing-the-iterator-trait"><a class="header" href="#implementing-the-iterator-trait">Implementing the Iterator trait</a></h2>
<p>To implement the lexer there is a single method we need to implement: <code>next</code>. This method will return the next token in the source code.</p>
<pre><code class="language-rust ignore">impl&lt;'a&gt; Iterator for Lexer&lt;'a&gt; {
    type Item = Token;

    fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; {
        // ...
    }
}</code></pre>
<p>At each iteration, we will consume some characters depending on the state, and upon reaching a final state, we will return the token. We will also need to handle the case where the lexer reaches the end of the source code.</p>
<p>First, we should skip any whitespace and comments since they cannot be tokenized into anything meaningful. It'll also be cleaner to move this to a separate <code>impl</code> block.</p>
<pre><code class="language-rust ignore">impl&lt;'a&gt; Lexer&lt;'a&gt; {
    fn skip_whitespace(&amp;mut self) {
        while let Some(ch) = self.peek() {
            if ch.is_whitespace() {
                self.advance();
            } else {
                break;
            }
        }
    }

    fn skip_comments(&amp;mut self) {
        if self.peek() == Some('/') &amp;&amp; self.src[self.pos + 1..].chars().next() == Some('/') { // ridiculously slow :(
            while let Some(ch) = self.peek() {
                if ch == '\n' {
                    break;
                } else {
                    self.advance();
                }
            }
        }
    }
}

impl&lt;'a&gt; Iterator for Lexer&lt;'a&gt; {
    type Item = Token;

    fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; {
        self.skip_whitespace();
        self.skip_comments();

        // ...
    }
}</code></pre>
<p>Now after transitioning from \(q_{start}\) to potentially \(q_{whitespace}\) or \(q_{comment}\) and back to \(q_{start}\), we can start lexing the next token. Rust's <code>match</code> keyword is extremely powerful in this instance!</p>
<pre><code class="language-rust ignore">        if let Some(ch) = self.peek() {
            // we'll define our transition functions here
            let token = match ch {
                // Transition to q_{op} state
                '+' | '-' | '*' | '/' | '%' | '^' | '=' | '&lt;' | '&gt;' | '!' | '&amp;' | '|' =&gt; {
                    self.lex_op()
                }
                // Transition to q_{num} state
                '0'..='9' =&gt; self.lex_number(),
                // Transition to q_{id} state. This also includes keywords.
                'a'..='z' | 'A'..='Z' | '_' =&gt; self.lex_word(),
                // Transition into symbols state
                '{' =&gt; Ok(self.lex_single_char(TokenKind::LBrace)),
                '}' =&gt; Ok(self.lex_single_char(TokenKind::RBrace)),
                '(' =&gt; Ok(self.lex_single_char(TokenKind::LParen)),
                ')' =&gt; Ok(self.lex_single_char(TokenKind::RParen)),
                ';' =&gt; Ok(self.lex_single_char(TokenKind::Semicolon)),
                ',' =&gt; Ok(self.lex_single_char(TokenKind::Comma)),
                '.' =&gt; Ok(self.lex_single_char(TokenKind::Dot)),
                ':' =&gt; Ok(self.lex_single_char(TokenKind::Colon)),
                _ =&gt; {
                    // return an error including the span
                    Err(anyhow!(LangError::UnexpectedCharacter(
                        ch.to_string(),
                        Span {
                            start: self.pos,
                            end: self.pos + ch.len_utf8()
                        }
                    )))
                }
            };

            Some(token)
        } else {
            None
        }</code></pre>
<p>One important note is that whilst it returns <code>Option&lt;Self::Item&gt;</code> -- or <code>Option&lt;Result&lt;Token&gt;&gt;</code> more specifically -- upon encountering an error we will attempt to recover instead of declaring there is no token there. This is a common practice in lexing and parsing so we can catch multiple errors instead of stopping early.</p>
<p>Next, there's a few transition functions we need to define.</p>
<h3 id="transitioning-to-the-q_op-state-parsing-operators"><a class="header" href="#transitioning-to-the-q_op-state-parsing-operators">Transitioning to the \(q_{op}\) state (parsing operators)</a></h3>
<p>Recall the transition functions for the \(q_{op}\) state:</p>
<p>$$\begin{align*} \delta(q_{op}, a) &amp;= \begin{cases} q_{op}^{(1)} &amp; \exists s_i \in \text{ 2-char Operators}: \text{a startswith } s_i \\ q_{op} &amp; \exists s_i\in \text{ 1-char Operators}: \text{a startswith } s_i \\ \text{null} &amp; \text{otherwise} \end{cases} \\ \delta(q_{op}^{(1)}, b) &amp;= \begin{cases} q_{op} &amp; ab \in \text{ 2-char Operators} \\ \text{null} &amp; \text{otherwise} \end{cases} \\
\end{align*}$$</p>
<p>Whilst it seems complex, we can implement it in a single function. The steps are simply:
0. Assume that the current character is an operator</p>
<ol>
<li>Keep track of the start index of the operator</li>
<li>Check if the next character forms a multi-char operator
<ul>
<li><strong>If so</strong>, consume the next character and return the multi-char operator token</li>
<li><strong>If not</strong>, return the single-char operator token</li>
</ul>
</li>
</ol>
<p>Let's implement this in the main <code>impl</code> block:</p>
<pre><code class="language-rust ignore">    fn lex_op(&amp;mut self) -&gt; Result&lt;Token&gt; {
        // 1. Keep track of the start index of the operator
        let start = self.pos;

        // 2. Check if the next character forms a multi-char operator
        self.advance();
        let mut end = self.pos;

        if let Some(ch) = self.peek() {
            match ch {
                // if so, consume the next character and update end
                '+' | '-' | '*' | '/' | '%' | '^' | '=' | '&lt;' | '&gt;' | '!' | '&amp;' | '|' =&gt; {
                    self.advance();
                    end = self.pos;
                }
                // if not, the end is the same as the start
                _ =&gt; {}
            }
        } else {
            // Make sure we dont go out of bounds
            return Err(anyhow!(LangError::UnexpectedEOF(Span { start, end })));
        }

        Ok(Token {
            kind: TokenKind::operator_from(&amp;self.src[start..end]),
            span: Span { start, end },
        })
    }</code></pre>
<p>Keep in mind we are using the <code>operator_from</code> method on <code>TokenKind</code> to convert the operator string to a <code>TokenKind</code>. We implemented this in the <a href="./tokens.html">Tokens</a> section.</p>
<h3 id="transitioning-to-the-q_num-state-parsing-numbers"><a class="header" href="#transitioning-to-the-q_num-state-parsing-numbers">Transitioning to the \(q_{num}\) state (parsing numbers)</a></h3>
<p>Recall the transition functions for the \(q_{num}\) state in the <a href="./finite_automata.html">Finite Automata</a> section:
$$\delta(q_{num}, a) = \begin{cases}
q_{int} &amp;q = q_{start} \ \land \ a \in \text{Digits} \\
q_{decimal} &amp;q = q_{intPart} \ \land \ a \in \text{Decimal Point} \\
q_{frac} &amp;q = q_{decimalPoint} \ \land \ a \in \text{DIGITS} \\
\text{null} &amp;\text{otherwise}
\end{cases}$$</p>
<p>We'll use this for parsing both integers and floats (in this case, they will be parsed as integers unless a <code>.</code> is present).</p>
<p>First, let's get the entire span of the literal, including whether or not it contains a decimal point:</p>
<pre><code class="language-rust ignore">    fn lex_number(&amp;mut self) -&gt; Result&lt;Token&gt; {
        let start = self.pos;
        let mut has_decimal = false;

        while let Some(ch) = self.peek() {
            // if it's a number, we keep going
            if ch.is_digit(10) {
                self.advance();
            } else if ch == '.' {
                // we keep going still, but we need to keep track of whether we've seen a decimal point
                if has_decimal {
                    self.advance();
                    let val = &amp;self.src[self.pos - 1..self.pos];

                    // advance until whitespace
                    while let Some(ch) = self.peek() {
                        if ch.is_whitespace() {
                            break;
                        } else {
                            self.advance();
                        }
                    }

                    return Err(anyhow!(LangError::UnexpectedCharacter(
                        val.to_string(),
                        Span {
                            start,
                            end: self.pos
                        }
                    )));
                } else {
                    has_decimal = true;
                    self.advance();
                }
            } else {
                break;
            }
        }
        let end = self.pos;

        // ...
    }</code></pre>
<p>Here, we continually advance across the number until we reach a non-digit character. If we encounter a decimal point <code>.</code>, we handle it depending on whether we've seen one before:</p>
<ul>
<li>If we haven't, we set <code>has_decimal</code> to <code>true</code> and continue</li>
<li>If we have, we advance until the end of the entire "number" and return an error</li>
</ul>
<p>Next, we should check whether the number is a float or not (determined by <code>has_decimal</code>), and convert it into the respective <code>Token</code>:</p>
<pre><code class="language-rust ignore">        if has_decimal {
            // attempt to parse as a float, otherwise return an error
            match value.parse() {
                Ok(f) =&gt; Ok(Token {
                    kind: TokenKind::FloatLiteral(f),
                    span: Span { start, end },
                }),
                Err(_) =&gt; Err(anyhow!(LangError::InvalidLiteral(
                    value.to_string(),
                    Span { start, end }
                ))),
            }
        } else {
            // attempt to parse as an integer, otherwise return an error
            match value.parse() {
                Ok(i) =&gt; Ok(Token {
                    kind: TokenKind::IntLiteral(i),
                    span: Span { start, end },
                }),
                Err(_) =&gt; Err(anyhow!(LangError::InvalidLiteral(
                    value.to_string(),
                    Span { start, end }
                ))),
            }
        }</code></pre>
<h3 id="transitioning-to-the-q_kw-bigcup-q_id-state-parsing-identifiers"><a class="header" href="#transitioning-to-the-q_kw-bigcup-q_id-state-parsing-identifiers">Transitioning to the \(q_{kw} \bigcup q_{id}\) state (parsing identifiers)</a></h3>
<p>We're going to group these two states together, since we may not be certain if the identifier is a keyword or not. We'll check this after we've determined the entire span of the word.</p>
<p>Again, let's recall the transition functions for the \(q_{id}\) state in the <a href="./finite_automata.html">Finite Automata</a> section (which we will rename to \(q_{word}\) for clarity):</p>
<p>We'll remain in the state \(q_{word}\) for any subsequent letters or digits, and transition to the state \(q_{word}^{(1)}\) if we encounter a whitespace character. From there, we'll transition into a final state \(q_{kw}\) or \(q_{id}\) depending on whether the word is a keyword or not.</p>
<p>$$\begin{align*} \delta(q_{word}, a) &amp;= \begin{cases} q_{word} &amp; a \in \text{Letters} \ \bigcup \ \text{Digits} \\ q_{word}^{(1)} &amp; a \in \text{Whitespace} \end{cases} \\ \delta(q_{word}^{(1)}, b) &amp;= \begin{cases} q_{kw} &amp; ab \in \text{Keywords} \\ q_{id} &amp; \text{otherwise} \end{cases} \\
\end{align*}$$</p>
<p>Let's implement this in our Lexer! Here's an overview of the process:</p>
<ol start="0">
<li>Assumption: the current character is a letter or underscore (start of word)</li>
<li>Keep track of the start index of the word</li>
<li>Keep advancing until we reach a non-letter or non-digit character</li>
<li>Check if the word is a keyword
<ul>
<li><strong>If so</strong>, return the keyword token</li>
<li><strong>If not</strong>, return the identifier token</li>
</ul>
</li>
</ol>
<pre><code class="language-rust ignore">    fn lex_word(&amp;mut self) -&gt; Result&lt;Token&gt; {
        // 1. Keep track of the start index of the word
        let start = self.pos;

        // 2. Keep advancing until we reach a non-letter or non-digit character
        while let Some(ch) = self.peek() {
            if ch.is_alphanumeric() || ch == '_' {
                self.advance();
            } else {
                break;
            }
        }
        let end = self.pos;

        let value = &amp;self.src[start..end];

        // Generate the respective token!
        Ok(Token {
            kind: TokenKind::keyword_from(value),
            span: Span { start, end },
        })
    }</code></pre>
<p>Again, we're using the <code>keyword_from</code> method on <code>TokenKind</code> to convert the word to a <code>TokenKind</code>, covered in the <a href="./tokens.html">Tokens</a> section.</p>
<h3 id="transitioning-to-the-q_symbol-state-parsing-symbols"><a class="header" href="#transitioning-to-the-q_symbol-state-parsing-symbols">Transitioning to the \(q_{symbol}\) state (parsing symbols)</a></h3>
<p>We leave the easiest for last. We'll simply transition to a final state when we encounter a symbol, since they're guaranteed to be single-characters and final states.</p>
<p>Recall the transition function:</p>
<p>$$\delta(q_{symbol}, a) = \begin{cases} q_{symbol} &amp; \text{if a is a symbol} \end{cases}$$</p>
<p>We can quickly implement this in our Lexer; it will advance past the single-character and return the respective <code>Token</code>:</p>
<pre><code class="language-rust ignore">    fn lex_single_char(&amp;mut self, kind: TokenKind) -&gt; Token {
        let start = self.pos;
        self.advance();
        let end = self.pos;

        Token {
            kind,
            span: Span { start, end },
        }
    }</code></pre>
<h2 id="helper-methods"><a class="header" href="#helper-methods">Helper methods</a></h2>
<p>If you noticed, the iterator returns <code>Option&lt;Result&lt;Token&gt;&gt;</code> which is pretty ugly albeit necessary if we want errors. This also means that we can't use the <code>collect</code> method on the iterator, which is a bit of a pain.</p>
<p>It would help a lot to have a method that consumes an iterator, returning a stream of tokens and the errors encountered. Let's create a function <code>consume_lexer(lexer: Lever) -&gt; (Vec&lt;Token&gt;, Vec&lt;Error&gt;)</code> to do this.</p>
<pre><code class="language-rust ignore">/// Consumes and partitions the Lexer iterator into 2 `vec`s for tokens and errors.
pub fn consume_lexer(lexer: Lexer) -&gt; (Vec&lt;Token&gt;, Vec&lt;Error&gt;) {
    let mut tokens = Vec::new();
    let mut errors = Vec::new();
    for token in lexer {
        match token {
            Ok(token) =&gt; tokens.push(token),
            Err(err) =&gt; errors.push(err),
        }
    }
    (tokens, errors)
}</code></pre>
<p>Very simple and straightforward and makes life a lot easier.</p>
<h1 id="resources-5"><a class="header" href="#resources-5">Resources</a></h1>
<ul>
<li><a href="https://doc.rust-lang.org/std/iter/trait.Iterator.html">Iterator trait</a>: The official documentation for the <code>Iterator</code> trait.</li>
<li><a href="https://doc.rust-lang.org/rust-by-example/scope/lifetime.html">Lifetimes in Rust</a></li>
<li><a href="./finite_automata.html">Finite Automata</a> in this guide!</li>
<li><a href="https://medium.com/@tarungudipalli/exploring-rusts-string-a-comprehensive-guide-with-examples-25f398ade356">Rust's Strings</a>: A comprehensive guide to Rust's strings.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="parsing-and-syntax-analysis"><a class="header" href="#parsing-and-syntax-analysis">Parsing and Syntax Analysis</a></h1>
<p>Now that we have a stream of tokens, we can start to analyze the syntax of the program. This is the process of parsing the tokens to determine the structure of the program. The result of this process is an abstract syntax tree (AST), which is a tree representation of the program's structure.</p>
<p>This chapter will cover the following topics:</p>
<ul>
<li>ASTs and their uses</li>
<li>How grammars work, and how to use them</li>
<li>Parsing techniques</li>
<li>How to implement a parser</li>
</ul>
<h1 id="resources-6"><a class="header" href="#resources-6">Resources</a></h1>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">Abstract Syntax Trees - Wikipedia</a></li>
<li><a href="https://eli.thegreenplace.net/2009/02/16/abstract-vs-concrete-syntax-trees">ASTs vs CSTs</a></li>
<li><a href="https://docs.rs/syn/1.0.72/syn/">Rust Syn crate</a></li>
<li><a href="https://en.wikipedia.org/wiki/Tree_(data_structure)">Tree Data Structures</a></li>
<li><a href="https://en.wikipedia.org/wiki/Tree_traversal">Tree Traversal</a></li>
<li><a href="https://www.cs.cornell.edu/courses/cs2110/2017sp/online/dfs/dfs01.html">DFS, BFS</a></li>
<li><a href="https://www.ketteringscienceacademy.org/attachments/download.asp?file=1057&amp;type=pdf">BNF</a>: The dates example is from this resource.</li>
<li><a href="https://en.wikipedia.org/wiki/Extended_Backus%E2%80%93Naur_form">EBNF</a>: Wikipedia's page on EBNF.</li>
<li><a href="https://www.cl.cam.ac.uk/~mgk25/iso-14977.pdf">EBNF</a>: The official ISO standard for EBNF.</li>
<li><a href="https://www.cs.uic.edu/~liub/teach/cs494/ebnf.pdf">EBNF</a>: A more concise and readable resource on EBNF.</li>
<li><a href="https://en.wikibooks.org/wiki/Introduction_to_Programming_Languages/Grammars">Intro to Programming Languages/Grammars</a>: A Wikibooks page on grammars.</li>
<li><a href="https://en.wikipedia.org/wiki/Recursive_descent_parser">Recursive Descent Parsing</a> on Wikipedia</li>
<li><a href="https://maeyler.github.io/Automata-2018/cfg/Bilal_RecursiveDescentParser.html">RDP Visualizer</a> - A visualizer for recursive descent parsers</li>
<li><a href="https://en.wikipedia.org/wiki/LL_parser">LL(k) Parsing</a> on Wikipedia</li>
<li><a href="https://www.cs.uaf.edu/~cs331/notes/FirstFollow.pdf">LL(k) Parsing</a> on University of Alaska Fairbanks</li>
<li><a href="https://github.com/GabrielMajeri/LL-K-Parser">LL(k) Parser</a>: A Python implementation of an LL(k) parser</li>
<li><a href="https://en.wikipedia.org/wiki/LR_parser">LR Parsing</a> on Wikipedia</li>
<li><a href="https://www.geeksforgeeks.org/lr-parser/">LR Parsing GfG</a>: GeeksforGeeks article on LR Parsing</li>
<li><a href="https://en.wikipedia.org/wiki/Operator-precedence_parser">Operator Precedence</a></li>
<li><a href="https://doc.rust-lang.org/std/boxed/struct.Box.html">Box<T></a></li>
<li><a href="https://doc.rust-lang.org/std/fmt/trait.Display.html"><code>Display</code> trait</a></li>
<li><a href="https://doc.rust-lang.org/std/cell/struct.RefCell.html">RefCell documentation</a></li>
<li><a href="https://doc.rust-lang.org/book/ch15-05-interior-mutability.html">RefCell in book</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="abstract-syntax-trees"><a class="header" href="#abstract-syntax-trees">Abstract Syntax Trees</a></h1>
<h2 id="introduction-to-asts"><a class="header" href="#introduction-to-asts">Introduction to ASTs</a></h2>
<p>In order to understand the structure of a program, we need to interpet and parse it in a way that is easy to understand and manipulate.</p>
<p>An abstract syntax tree (AST) is a tree representation of the abstract syntactic structure of source code written in a programming language. Each node of the tree denotes a construct occurring in the source code. The syntax is "abstract" in the sense that it does not represent every detail appearing in the real syntax of the programming language.</p>
<h3 id="components-of-asts"><a class="header" href="#components-of-asts">Components of ASTs</a></h3>
<p>ASTs are structured the same way general Tree Data Structures are, except that:</p>
<ol>
<li>Each node in the tree represents a construct in the source code.</li>
<li>The children of a node are the subconstructs of the construct represented by the node.</li>
</ol>
<p>Let's take a look at an example of an AST for the following C code:</p>
<pre><code class="language-c">int main() {
    return 0;
}
</code></pre>
<p>The AST for the above code would look like this:</p>
<pre><code>Program
    |
    └── FunctionDeclaration
        |
        ├── TypeSpecifier
        |   |
        |   └── int
        |
        ├── Identifier
        |   |
        |   └── main
        |
        └── CompoundStatement
            |
            └── ReturnStatement
                |
                └── Constant
                    |
                    └── 0
</code></pre>
<p>Each node can represent different constructs in the source code, such as:</p>
<ul>
<li>Function, variable, and type declarations</li>
<li>Control structures (if, while, for, etc.)</li>
<li>Expressions (arithmetic, logical, etc.)</li>
<li>Statements (assignments, function calls, etc.)</li>
<li>Literals (integers, strings, etc.)</li>
</ul>
<h3 id="uses-of-asts"><a class="header" href="#uses-of-asts">Uses of ASTs</a></h3>
<p>ASTs are used in many areas of computer science, such as:</p>
<ul>
<li><strong>Compilers</strong>: Used to represent the structure of the source code</li>
<li><strong>Code analysis and transformation</strong>: Used to analyze and transform source code</li>
<li><strong>Language processing</strong>: Used in natural language processing to represent the structure of sentences and phrases</li>
</ul>
<p>Of course, we will focus on the use of ASTs in compilers.</p>
<h3 id="constructing-an-ast"><a class="header" href="#constructing-an-ast">Constructing an AST</a></h3>
<p>The process of constructing an AST from source code is called <strong>parsing</strong>. This process involves analyzing the tokens generated by the lexer and constructing a tree that represents the structure of the source code.</p>
<p>Parsing is a complex process, and is dependent on the grammar it follows -- as well as the parsing technique you use. We will cover these topics in more detail in the next sections. Regardless, the common goal is to construct a tree that represents the structure of the source code, given a stream of tokens.</p>
<h2 id="traversal"><a class="header" href="#traversal">Traversal</a></h2>
<p>Once we have constructed an AST, there are two main ways to traverse it:</p>
<ol>
<li>
<p><strong>Depth-First Traversal</strong>: In this traversal, we start at the root node and explore as far as possible along each branch before backtracking. This is the most common way to traverse an AST, and is used in many algorithms that operate on trees.</p>
</li>
<li>
<p><strong>Breadth-First Traversal</strong>: In this traversal, we visit all the nodes on a level before moving to the next level. This is less common, but can be useful in certain scenarios.</p>
</li>
</ol>
<h2 id="alternative-representations"><a class="header" href="#alternative-representations">Alternative representations</a></h2>
<p>There are other ways to represent the syntactic structure of a program. One such example are <strong>Concrete Syntax Trees (CSTs)</strong>, which represent the concrete syntax of the program. The main difference between CSTs and ASTs is that CSTs represent every detail of the syntax, including whitespace and comments.</p>
<p>Rust's <code>syn</code> crate is a good example of a library that uses CSTs to represent the syntax of Rust code. It provides a way to parse Rust code into a CST, and then manipulate it using Rust's syntax.</p>
<p>Some useful features of CSTs include:</p>
<ul>
<li><strong>Preservation of whitespace and comments</strong>: This can be useful when you want to preserve the original formatting of the code. Rust uses this to generate documentation from the comments in the code.</li>
<li><strong>More detailed error messages</strong>: Since CSTs represent the concrete syntax of the program, they can provide more detailed error messages when parsing fails.</li>
</ul>
<p>However, ASTs are more abstract and thus are more commonly used in compilers and language processing. We will be using this representation in our compiler.</p>
<h1 id="resources-7"><a class="header" href="#resources-7">Resources</a></h1>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">Abstract Syntax Trees - Wikipedia</a></li>
<li><a href="https://eli.thegreenplace.net/2009/02/16/abstract-vs-concrete-syntax-trees">ASTs vs CSTs</a></li>
<li><a href="https://docs.rs/syn/1.0.72/syn/">Rust Syn crate</a></li>
<li><a href="https://en.wikipedia.org/wiki/Tree_(data_structure)">Tree Data Structures</a></li>
<li><a href="https://en.wikipedia.org/wiki/Tree_traversal">Tree Traversal</a></li>
<li><a href="https://www.cs.cornell.edu/courses/cs2110/2017sp/online/dfs/dfs01.html">DFS, BFS</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="context-free-grammars"><a class="header" href="#context-free-grammars">Context-free Grammars</a></h1>
<p>Here, we'll discuss the concept of Context-free Grammars (CFGs) and their significance in compiler design. We'll also cover the formal definition of CFGs, metasyntax notations, and designing grammars.</p>
<p>This section is important, as it provides the grammar our parser will be using.</p>
<h2 id="introduction-to-context-free-grammars"><a class="header" href="#introduction-to-context-free-grammars">Introduction to Context-free Grammars</a></h2>
<p>Context-free grammars (CFGs) are a formal way to describe the syntax of programming languages. They are used to define the structure of a language by specifying the rules for constructing valid programs. CFGs are an essential part of the parsing process in compilers, as they provide a systematic way to recognize and analyze the syntactic structure of source code.</p>
<p>It's similar to how a natural language has a grammar, which defines the rules for constructing valid sentences. In the same way, a programming language has a grammar that defines the rules for constructing valid programs.</p>
<p>You would identify that <code>He. had a hat</code> is not a valid sentence in English, and in the same way, <code>int x = 5 +;</code> is not a valid statement in C. CFGs help us define these rules and constraints.</p>
<h3 id="what-do-you-mean-by-context-free"><a class="header" href="#what-do-you-mean-by-context-free">What do you mean by "context-free"?</a></h3>
<p>In the context of grammars, "context-free" means that the production of a non-terminal symbol can occur without regard to the surrounding context. This property simplifies parsing, as it allows the recognition of language constructs solely based on the local structure, without considering the broader context in which they appear.</p>
<p>This is a mouthful, but in simple terms, it means that the structure of a construct (i.e. a variable declaration) can be recognized based on its own local structure (i.e. only the variable declaration statement and its components), without needing to consider the broader context (i.e. the outer scope or function).</p>
<h3 id="example"><a class="header" href="#example">Example</a></h3>
<p>Consider a context-free grammar for simple arithemetic expressions (supporting addition).</p>
<p>The product rule for addition may be defined as:</p>
<pre><code>// `expr` is defined as `term` followed by `+` followed by `expr`
expr -&gt; expr + term
        | term          // or just `term`
</code></pre>
<p>The expression can be composed of another expression <em>inside</em> the <code>expr</code> rule, and the <code>expr</code> rule can be used to define the <code>term</code> rule. This is the "context-free" property of the grammar. We call this <strong>replacement</strong>.</p>
<h2 id="formal-definition-of-context-free-grammars"><a class="header" href="#formal-definition-of-context-free-grammars">Formal Definition of Context-free Grammars</a></h2>
<p>Indeed we need a formal definition of CFGs. It's a bit complex, but it's important to understand the formal definition of CFGs to understand how they are used in compiler design.</p>
<p>CFGs are defined as a 4-tuple \(G = (N, \sum, P, S)\), where:</p>
<ul>
<li>\(N\) is a finite set of non-terminal symbols.
<ul>
<li>"non-terminal" symbols are symbols that can be replaced by a sequence of other symbols. Essentially, the parser can expand these symbols into other symbols.</li>
<li>For example, the non-terminal symbol <code>expr</code> in the previous example can be expanded into <code>expr + term</code> or just <code>term</code>.</li>
</ul>
</li>
<li>\(\sum\) is a finite set of terminal symbols, <strong>disjoint</strong> from \(N\).
<ul>
<li>"terminal" symbols are symbols that cannot be replaced by other symbols. They are the smallest units of the language.</li>
<li>For example, the <code>+</code> symbol in the previous example is a terminal symbol.</li>
</ul>
</li>
<li>\(P\) is a finite set of production rules, where each rule is of the form \(A \rightarrow \alpha\), where \(A \in N\) and \( \alpha \in (N \cup \sum)^* \) (a string of symbols from \((N \cup \sum)^*\)).
<ul>
<li>Each production rule defines how a non-terminal symbol can be replaced by a sequence of other symbols.</li>
<li>For example, the production rule <code>expr -&gt; expr + term</code> in the previous example.</li>
<li>\((N \cup \sum)^*\) denotes the set of all strings of symbols that can be formed from the union of \(N\) and \(\sum\).</li>
</ul>
</li>
<li>\(S\) is the start symbol, which is a special non-terminal symbol that represents the entire language.
<ul>
<li>It is the symbol from which the derivation of the entire language begins.</li>
</ul>
</li>
</ul>
<p>Some important points to note:</p>
<ul>
<li>Non-terminal symbols represents syntactic categories in the language</li>
<li>Terminal symbols represent the primary elements of the language
<ul>
<li>Identifiers, keywords, operators, etc.</li>
</ul>
</li>
<li>Production rules define how non-terminal symbols can be replaced by sequences of other symbols</li>
<li>The start symbol represents the entire language and is used as the initial symbol for the derivation process</li>
</ul>
<h3 id="example-1"><a class="header" href="#example-1">Example</a></h3>
<p>Let's consider a simple CFG for a very basic arithmetic expression language. The language supports the basic operations of numbers.</p>
<p>Given the CFG \(G = (N, \sum, P, S)\), where:</p>
<ul>
<li>\(N = {\text{Expr}, \text{Term}, \text{Factor}}\) are non-terminal symbols</li>
<li>\(\sum = {+, *, (, ), \text{num}}\) are terminal symbols</li>
<li>\(P\) is a set of production rules</li>
<li>\(S = \text{Expr}\) is the start symbol</li>
</ul>
<p>The production rules for the CFG are:
$$\begin{align*}
\text{Expr} &amp;\rightarrow \text{Expr} + \text{Term} \\
\text{Expr} &amp;\rightarrow \text{Expr} - \text{Term} \\
\text{Expr} &amp;\rightarrow \text{Term} \\
\text{Term} &amp;\rightarrow \text{Term} * \text{Factor} \\
\text{Term} &amp;\rightarrow \text{Term} \div \text{Factor} \\
\text{Term} &amp;\rightarrow \text{Factor} \\
\text{Factor} &amp;\rightarrow (\text{Expr}) \\
\text{Factor} &amp;\rightarrow \text{num}
\end{align*}$$</p>
<p>Let's parse the arithmetic expression \(3 \times (4 + 5)\) using \(G\). We'll do it step by step, with indentations to demonstrate the process.</p>
<p>Let the token stream be <code>"3", "*", "(", "4", "+", "5", ")"</code>.</p>
<ul>
<li>\(\text{Expr} \Rightarrow \text{Term}\)
<ul>
<li>\(\text{Term} \Rightarrow \text{Term} * \text{Factor}\)
<ul>
<li>\(\text{Term} \Rightarrow \text{Factor}\)
<ul>
<li>\(\text{Factor} \Rightarrow \text{num}\) applies at \(3\)</li>
</ul>
</li>
<li>\(\text{Factor} \Rightarrow (\text{Expr})\) applies at \((4 + 5)\)
\(\text{Expr} \Rightarrow \text{Expr} + \text{Term}\)
<ul>
<li>\(\text{Expr} \Rightarrow \text{Term}\)
<ul>
<li>\(\text{Term} \Rightarrow \text{Factor}\)
<ul>
<li>\(\text{Factor} \Rightarrow \text{num}\) applies at \(4\)</li>
</ul>
</li>
</ul>
</li>
<li>\(\text{Term} \Rightarrow \text{Factor}\)
<ul>
<li>\(\text{Factor} \Rightarrow \text{num}\) applies at \(5\)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>\(\text{Factor} \Rightarrow \text{num}\) applies at \(3\)</li>
</ul>
</li>
</ul>
<p>The derivation process starts with the start symbol \(\text{Expr}\) and proceeds by applying production rules to non-terminal symbols until only terminal symbols remain. This process is called <strong>derivation</strong>.</p>
<p>This would yield the parse tree for the expression \(3 \times (4 + 5)\):</p>
<pre><code>   *
  / \
 3   +
    / \
   4   5
</code></pre>
<p>Notice how operator precedence is implicitly defined by the grammar. The <code>*</code> operator has a higher precedence than the <code>+</code> operator, and the parentheses are used to override the precedence.</p>
<p>In compilers, you may either implement operator precedence within the grammar, or during parsing. We'll discuss this in the next section.</p>
<h2 id="metasyntax-notations"><a class="header" href="#metasyntax-notations">Metasyntax Notations</a></h2>
<p>Metasyntax notations are used to define the syntax of context-free grammars.</p>
<h3 id="bnf"><a class="header" href="#bnf">BNF</a></h3>
<p><a href="https://en.wikipedia.org/wiki/Backus%E2%80%93Naur_form">Backus-Naur Form (BNF)</a> is a notation used to describe the syntax of programming languages (and other formal languages). It's commonly used to define CFGs, as it provides a concise and readable way to express the production rules of a grammar.</p>
<p>In BNF notation, <strong>productions</strong> are used to define rules for constructing valid strings in a language. Each production consists of a <strong>non-terminal symbol</strong> on the left-hand side, followed by the <code>::=</code> symbol, and a sequence of <strong>terminal</strong> and/or <strong>non-terminal symbols</strong> on the right-hand side.</p>
<p>For example, the BNF production rule for dates may look like this:</p>
<pre><code class="language-bnf">&lt;date&gt;   ::= &lt;month&gt; "/"" &lt;day&gt;
&lt;year&gt;   ::= &lt;digit&gt; &lt;digit&gt; &lt;digit&gt; &lt;digit&gt;
&lt;month&gt;  ::= 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12
</code></pre>
<ul>
<li><code>&lt;non-terminal&gt;</code> represents a syntactic category or placeholder symbol</li>
<li><code>::=</code> is the "is defined as" symbol, otherwise known as the "production" symbol</li>
<li><code>&lt;terminal&gt;</code> represents a literal symbol or token, otherwise represented as <code>&lt;expression&gt;</code></li>
</ul>
<p>You may notice some semantic features, such as the pipe <code>|</code> denoting "or" and the double quotes <code>""</code> denoting a literal symbol. There are other notations, listed below:</p>
<ul>
<li><code>[]</code> denotes optionality
<ul>
<li>e.g. <code>&lt;date&gt; ::= &lt;month&gt; "/" &lt;day&gt; ["/" &lt;year&gt;]</code></li>
</ul>
</li>
<li><code>()</code> denotes grouping
<ul>
<li>e.g. <code>&lt;expression&gt; ::= "(" &lt;expression&gt; ")" | &lt;term&gt;</code></li>
</ul>
</li>
<li><code>*</code> denotes repetition (0 or more times)
<ul>
<li>e.g. <code>&lt;identifier&gt; ::= &lt;letter&gt; &lt;letter&gt;*</code></li>
</ul>
</li>
</ul>
<h3 id="ebnf"><a class="header" href="#ebnf">EBNF</a></h3>
<p><a href="https://en.wikipedia.org/wiki/Extended_Backus%E2%80%93Naur_form">Extended Backus-Naurs Form (EBNF) grammar</a> is an extension of BNF that adds additional syntactic features to make the notation more expressive and concise. It's commonly used to define the syntax of programming languages and other formal languages.</p>
<p>Here are some of the differences:</p>
<ul>
<li>EBNF denotes optional elements by <code>{}</code> instead of <code>[]</code></li>
<li>EBNF extends repetition by introducing the use of <code>[]</code> to represent optional repetition, and <code>+</code> to represent one or more repetitions
<ul>
<li>e.g. <code>&lt;block&gt; ::= "{" &lt;statement&gt;* "}"</code></li>
</ul>
</li>
</ul>
<p>Typically, you'd use EBNF over BNF as it's more concise and expressive.</p>
<p>As a side-note, you don't need to declare symbols with <code>&lt;</code> and <code>&gt;</code> in EBNF, but it's a common practice to do so.</p>
<h2 id="designing-grammars"><a class="header" href="#designing-grammars">Designing Grammars</a></h2>
   <!-- - Discuss strategies for designing grammars:
     - Top-down vs. bottom-up approach
     - Language features and constraints
     - Hierarchical structure
   - Provide guidelines for creating clear, unambiguous, and concise grammars. -->
<p>There's different strategies for designing grammars for your language.</p>
<h3 id="top-down-vs-bottom-up-approach"><a class="header" href="#top-down-vs-bottom-up-approach">Top-down vs. Bottom-up approach</a></h3>
<p>The top-down approach starts with the start symbol \(S\) and tries to derive the input string by applying production rules in a leftmost derivation. In simple terms, it starts from the top of the parse tree (i.e. programs, functions) and works its way down.</p>
<p>The bottom-up approach starts with the input string and tries to apply production rules in a rightmost derivation to reach the start symbol \(S\). In simple terms, it starts from the bottom of the parse tree (i.e. from tokens, expressions) and works its way up.</p>
<h3 id="quick-guidelines"><a class="header" href="#quick-guidelines">Quick guidelines</a></h3>
<p>When designing grammars, it's important to maximise the usage of the following:</p>
<ul>
<li><strong>Clarity</strong>: The grammar should be clear and easy to understand</li>
<li><strong>Consistent naming</strong>: Use consistent naming conventions for non-terminal symbols</li>
<li><strong>Unambiguous</strong>: The grammar should be unambiguous, meaning that each string in the language has exactly one valid parse tree</li>
<li><strong>Conciseness</strong>: Do not make it overly complex. Keep it simple and concise</li>
<li><strong>Modularize</strong>: Similar to programming, modularize your grammar. Break it down into smaller, more manageable parts</li>
<li><strong>Document</strong>: Annotate with comments and documentation to explain the purpose of each production rule</li>
</ul>
<h2 id="lets-design-a-grammar"><a class="header" href="#lets-design-a-grammar">Let's design a grammar!</a></h2>
<p>First let's refresh our memory of the language we're designing a grammar for. Read through <a href="./lexical_analysis.html#our-language">Lexical Analysis - Our Language</a> to understand the language we're designing a grammar for. Let's see the example code we wrote:</p>
<pre><pre class="playground"><code class="language-rust">fn main() -&gt; int {
    let x: int = 5;
    let y: int = 3;
    let z: int = x + y;
    if (z &gt; 5) {            // Branching!
        return z;           // Return z
    } else {
        return 0;           // Return 0
    }
}</code></pre></pre>
<p>We can either work top-down, defining the program structure first, or bottom-up, defining the smallest elements first. We'll start with the top-down approach.</p>
<p>We will also be using <strong>EBNF</strong> to define our grammar. Feel free to use a different notation and/or a bottom-up approach.</p>
<h3 id="high-level-constructs"><a class="header" href="#high-level-constructs">High-level constructs</a></h3>
<p>We can start by defining the high-level constructs of the language, such as the program, and the main components of the program. We would like it to be function-based (potentially including structs, enums, etc.).</p>
<p>Let's consider a program to have multiple items, which may be a function, a struct, or an enum. For simplicity, we'll only consider functions for now.</p>
<pre><code class="language-ebnf"># A program is a sequence of 0 or more items
program           ::= item*

# An item is a function declaration
item              ::= function_decl
</code></pre>
<p>Now we can define the function declaration. Here's what the declaration looks like:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn function_name(param1: type1, param2: type2) -&gt; return_type { ... }
<span class="boring">}</span></code></pre></pre>
<p>It consists of the <code>fn</code> keyword, the function name, a list of parameters, and the return type. We'll also include the function body, which we call a "block".</p>
<pre><code class="language-ebnf"># Create the function declaration. There are optional parameter lists too.
function_decl     ::= "fn" IDENTIFIER "(" {parameter_list} ")" "-&gt;" type block

# A parameter list is a sequence of 0 or more parameters
parameter_list    ::= parameter ("," parameter)*

# A parameter is an identifier followed by a type
parameter         ::= IDENTIFIER ":" type
</code></pre>
<p>It's important to keep track of what you have not defined so far. Here's a list of things we haven't defined:</p>
<ul>
<li><code>type</code></li>
<li><code>block</code></li>
<li><code>IDENTIFIER</code></li>
</ul>
<h3 id="types"><a class="header" href="#types">Types</a></h3>
<p>To get parameters and names out of the way, let's create production rules for types.</p>
<p>We don't need to worry about defining the specifics of how an identifier is structured for example, since the lexer has already handled it for us. We can simply refer to it as <code>IDENTIFIER</code>.</p>
<p>Types can be simple, like <code>int</code> and <code>bool</code>, which we call <strong>primitive types</strong>. They can also be more complex, like <code>struct</code> and <code>enum</code>, which we call <strong>compound types</strong>. For now, we'll only consider primitive types.</p>
<pre><code class="language-ebnf">type              ::= primitive_type

primitive_type    ::= "int" | "float" | "bool"
</code></pre>
<p><strong>Undefined</strong>:</p>
<ul>
<li><code>block</code></li>
</ul>
<h3 id="blocks-and-statements"><a class="header" href="#blocks-and-statements">Blocks and Statements</a></h3>
<p>A block is a sequence of statements enclosed in curly braces. A statement can be an expression, a variable declaration, or a control flow statement.</p>
<p>This is straightforward to define:</p>
<pre><code class="language-ebnf">block             ::= "{" statement* "}"

statement         ::= expression
                    | variable_decl
                    | flow_statement
                    | return_statement
</code></pre>
<p>Expressions are a bit more complex, as they can be composed of other expressions. We'll define them later. Let's focus on variable declarations and control flow statements.</p>
<p>A variable declaration must have a type and an expression. For safety (and to avoid null), we'll require an expression to be assigned to the variable.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let x: int = 5;
<span class="boring">}</span></code></pre></pre>
<pre><code class="language-ebnf">variable_decl     ::= "let" IDENTIFIER ":" type "=" expression ";"
</code></pre>
<p>Control flow statements would consist of a few tokens: <code>if</code>, <code>else</code>, <code>return</code>. This is a bit tricky to design, as it can be recursive. Let's design it!</p>
<p>Here's what a control flow statement looks like:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>if (condition) {
    // if block
} else if (condition) {
    // else if block
} else {
    // else block
}
<span class="boring">}</span></code></pre></pre>
<p>As you see, the <code>else</code> token is condition, as is the <code>else if</code> token. We can define the <code>else if</code> block as a control flow statement, and the <code>else</code> block as a block.</p>
<pre><code class="language-ebnf">flow_statement    ::= "if" expression block {"else" block}
</code></pre>
<p>We don't know if <code>expression</code> would yield a boolean value, but we'll leave that to the semantic analysis phase of the compilation process. We only care about understanding the structure of the language.</p>
<p>One big flaw with this production rule is that it doesn't support an arbritary number of <code>else if</code> statements. Let's modify it to support that.</p>
<pre><code class="language-ebnf">flow_statement    ::= "if" expression block {"else" block | "else" flow_statement}
</code></pre>
<p>Now, the <code>else</code> token can be a block or another <code>if</code> statement. We can repeat it as many times as we want.</p>
<p>In this book, we will opt for a simple <code>if</code>-<code>else</code> statement, but you can expand it to support more complex constructs.</p>
<p>For return statements, it's simple:</p>
<pre><code class="language-ebnf">return_statement  ::= "return" {expression}
</code></pre>
<p><strong>Undefined</strong>:</p>
<ul>
<li><code>expression</code></li>
</ul>
<h3 id="expressions"><a class="header" href="#expressions">Expressions</a></h3>
<p>Here's the tough part. Expressions can be composed of other expressions, and can be quite complex. There's various things to support such as binary, unary, and ternary operations, function calls, and more. Similar to the previous expression example, we'll modularize this into smaller parts.</p>
<p>An expression can be either:</p>
<ul>
<li>A primary expression (i.e. a literal, a variable, or a function call)</li>
<li>A unary expression (i.e. a negation, a dereference)</li>
<li>A binary expression (i.e. an addition, a multiplication)</li>
</ul>
<p>We call then "unary" and "binary" expressions because they operate on one or two operands, respectively. While we're at it, we'll also define primary expression and literals.</p>
<pre><code class="language-ebnf">expression        ::= primary_expression
                    | unary_expression
                    | binary_expression

primary_expression ::= literal
                    | IDENTIFIER
                    | "(" expression ")"
                    | function_call

literal           ::= INT | FLOAT | BOOLEAN
</code></pre>
<p>That leaves us with unary and binary expressions.</p>
<p>Unary expressions consist of a unary operator and an expression. For example, <code>-4</code> or <code>!true</code>.</p>
<pre><code class="language-ebnf">unary_expression  ::= "-" expression | "!" expression
</code></pre>
<p>Binary expressions consist of two expressions and a binary operator. For example, <code>4 + 5</code> or <code>true &amp;&amp; false</code>.</p>
<pre><code class="language-ebnf">binary_expression ::= expression OPERATOR expression
</code></pre>
<p>And finally, our last undefined symbol is <code>function_call</code>. A function call consists of a function name and a list of arguments.</p>
<pre><code class="language-ebnf">function_call     ::= IDENTIFIER "(" {expression} ")"
</code></pre>
<h3 id="end-result"><a class="header" href="#end-result">End result</a></h3>
<p>And that's it! We've designed a grammar for our language. Here's the complete grammar so far:</p>
<pre><code class="language-ebnf">program           ::= item*

item              ::= function_decl
                    # | struct_decl
                    # | enum_decl
                    # | impl_block

function_decl     ::= "fn" IDENTIFIER "(" {parameter_list} ")" "-&gt;" type block

parameter_list    ::= parameter ("," parameter)*

parameter         ::= IDENTIFIER ":" type

type              ::= primitive_type
                    # | user_defined_type
                    # | tuple_type
                    # | reference_type
                    # | array_type
                    # | function_type

primitive_type    ::= "int" | "float" | "bool"

block             ::= "{" statement* "}"

statement         ::= expression
                    | variable_decl
                    | flow_statement
                    | return_statement


expression        ::= primary_expression
                    | unary_expression
                    | binary_expression

primary_expression ::= literal
                    | IDENTIFIER
                    | "(" expression ")"
                    | function_call

literal           ::= INT | FLOAT | BOOLEAN

unary_expression  ::= "-" expression | "!" expression #| "&amp;" expression | "*" expression

binary_expression ::= expression OPERATOR expression

function_call     ::= IDENTIFIER "(" arguments ")"
arguments         ::= expression ("," expression)* | ε

variable_decl     ::= "let" IDENTIFIER ":" type "=" expression ";"

flow_statement    ::= "if" expression block {"else" block}

return_statement  ::= "return" {expression} ";"
</code></pre>
<p>This is a simple grammar that supports the basic constructs of our language. It's not complete, but it's a good starting point. We can always expand it to support more complex constructs and language features.</p>
<h1 id="resources-8"><a class="header" href="#resources-8">Resources</a></h1>
<ul>
<li><a href="https://www.ketteringscienceacademy.org/attachments/download.asp?file=1057&amp;type=pdf">BNF</a>: The dates example is from this resource.</li>
<li><a href="https://en.wikipedia.org/wiki/Extended_Backus%E2%80%93Naur_form">EBNF</a>: Wikipedia's page on EBNF.</li>
<li><a href="https://www.cl.cam.ac.uk/~mgk25/iso-14977.pdf">EBNF</a>: The official ISO standard for EBNF.</li>
<li><a href="https://www.cs.uic.edu/~liub/teach/cs494/ebnf.pdf">EBNF</a>: A more concise and readable resource on EBNF.</li>
<li><a href="https://en.wikibooks.org/wiki/Introduction_to_Programming_Languages/Grammars">Intro to Programming Languages/Grammars</a>: A Wikibooks page on grammars.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="parsing-techniques"><a class="header" href="#parsing-techniques">Parsing Techniques</a></h1>
<p>We've created a lexer to recognize tokens in the input into a stream of tokens, and we've designed a formal grammar for our language.</p>
<p>Next, we will need to write a parser to recognize the structure of the input and build an abstract syntax tree (AST) from it.</p>
<p>This section will cover how parsing works, the different techniques for parsing, and how to implement your own parser.</p>
<h1 id="resources-9"><a class="header" href="#resources-9">Resources</a></h1>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Recursive_descent_parser">Recursive Descent Parsing</a> on Wikipedia</li>
<li><a href="https://maeyler.github.io/Automata-2018/cfg/Bilal_RecursiveDescentParser.html">RDP Visualizer</a> - A visualizer for recursive descent parsers</li>
<li><a href="https://en.wikipedia.org/wiki/LL_parser">LL(k) Parsing</a> on Wikipedia</li>
<li><a href="https://www.cs.uaf.edu/~cs331/notes/FirstFollow.pdf">LL(k) Parsing</a> on University of Alaska Fairbanks</li>
<li><a href="https://github.com/GabrielMajeri/LL-K-Parser">LL(k) Parser</a>: A Python implementation of an LL(k) parser</li>
<li><a href="https://en.wikipedia.org/wiki/LR_parser">LR Parsing</a> on Wikipedia</li>
<li><a href="https://www.geeksforgeeks.org/lr-parser/">LR Parsing GfG</a>: GeeksforGeeks article on LR Parsing</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="recursive-descent-parsing"><a class="header" href="#recursive-descent-parsing">Recursive Descent Parsing</a></h1>
<p>They make these names sound scary for no reason.</p>
<h2 id="notable-definitions"><a class="header" href="#notable-definitions">Notable Definitions</a></h2>
<p>Refer to these when needed, but don't memorize them. They are here for reference.</p>
<ul>
<li><strong>Top-down parsing</strong>: A parsing technique that constructs a parse tree from the top and the input is read from left to right.</li>
<li><strong>LL(k) parser</strong>: A parser that reads input from left to right, constructs a leftmost derivation of the input, and uses k tokens of lookahead to make parsing decisions.</li>
<li><strong>Parse tree</strong>: A tree that represents the syntactic structure of a language construct according to a formal grammar.</li>
<li><strong>Abstract Syntax Tree (AST)</strong>: A tree that represents the abstract syntactic structure of a language construct according to a formal grammar.</li>
<li><strong>Lookahead</strong>: The number of tokens the parser looks ahead to make parsing decisions.</li>
<li><strong>Predictive parsing</strong>: A parsing technique that uses a lookahead of 1 to make parsing decisions.</li>
<li><strong>Backtracking</strong>: A technique used in recursive descent parsing to handle ambiguous grammars.</li>
<li><strong>Left recursion</strong>: A grammar is left-recursive if it has a non-terminal A that can directly or indirectly derive a string that starts with A itself.</li>
<li><strong>Left factoring</strong>: A technique used to eliminate common prefixes in a grammar.</li>
<li><strong>Mutually recursive</strong>: A set of procedures that call each other in a circular manner.</li>
</ul>
<h3 id="ll-parsing"><a class="header" href="#ll-parsing">LL(...) Parsing</a></h3>
<p>LL(k) and LL(1) parsing may be confusing at first. Here's a simple explanation:</p>
<p>"LL" stands for "Left-to-right, Leftmost derivation". This means that the parser reads the input from left to right and constructs a leftmost derivation of the input. The "k" represents the number of tokens of lookahead the parser uses to make parsing decisions.</p>
<p>LL(1) parsers are a subset of LL(k) parsers, where k is 1. This means that an LL(1) parser uses a lookahead of 1 to make parsing decisions.</p>
<p>LL(1) is more memory-efficient (particularly a point of concern with RDP) and simpler, but it can only parse a subset of grammars. LL(k) parsers can parse a wider range of grammars, but they are more complex and require more memory.</p>
<h2 id="introduction-to-recursive-descent-parsing-rdp"><a class="header" href="#introduction-to-recursive-descent-parsing-rdp">Introduction to Recursive Descent Parsing (RDP)</a></h2>
<p>Recursive Descent Parsing is a <strong>top-down parsing technique</strong> that constructs a parse tree (AST) from the top and the input is read from left to right.</p>
<p>In simple terms, a recursive descent parser is a set of mutually recursive procedures that correspond to the grammar rules of the language. Each procedure usually corresponds to a non-terminal symbol (on the left) in the grammar. The parser starts with the start symbol \(S\) of the grammar \(G\) and uses these procedures to recursively parse the input.</p>
<h3 id="evaluation"><a class="header" href="#evaluation">Evaluation</a></h3>
<p>RDP is a simple and very efficient parsing technique; beyond being easy to implement, it's easy to debug and understand. It's also very flexible and can be used to parse a wide range of grammars, including ambiguous ones (although you should avoid them if possible).</p>
<p>Given its recursive nature, it can be pretty memory-intensive (since it uses the call stack). This worsens if adapted to LL(k) parsing.</p>
<h2 id="how-it-works"><a class="header" href="#how-it-works">How it works</a></h2>
<p>The parser is a set of <strong>mutually recursive</strong> procedures, in which their names correspond to the non-terminal symbols of the grammar. Each procedure corresponds to a grammar rule and is responsible for parsing the input according to that rule.</p>
<p>The parser starts with the start symbol \(S\) of the grammar and uses these procedures to recursively parse the input.</p>
<h3 id="example-2"><a class="header" href="#example-2">Example</a></h3>
<p>For example, given the following simple grammar:
$$
\begin{align*}
S &amp;\rightarrow A \\
A &amp;\rightarrow b
\end{align*}
$$
The parser would have the following procedures:</p>
<pre><code class="language-rust ignore">/// Name corresponds with the left-hand side for S -&gt; aAb
fn parse_S() {
    parse_A();
}

/// Name corresponds with the left-hand side for A -&gt; b
fn parse_A() {
    match next_token() {
        Token::B =&gt; consume(),
        _ =&gt; panic!("Unexpected token"),
    }
}</code></pre>
<h3 id="handling-ambiguity"><a class="header" href="#handling-ambiguity">Handling Ambiguity</a></h3>
<p>RDP can handle ambiguous grammars by using <strong>backtracking</strong>. When the parser encounters a choice between two or more alternatives, it tries each alternative in turn. If one of the alternatives fails, the parser backtracks to the point where the choice was made and tries the next alternative.</p>
<p>This is particularly useful in scenarios such as parsing expressions, where the parser needs to decide between different operators.</p>
<div class="warning">
    <p><strong>Warning</strong>: Backtracking can be very inefficient and can lead to exponential time complexity in the worst case. It's best to avoid ambiguous grammars if possible.</p>
</div>
<h3 id="left-recursion-in-rdp"><a class="header" href="#left-recursion-in-rdp">Left Recursion in RDP</a></h3>
<p>RDP cannot handle left-recursive grammars directly. Left recursion is when a non-terminal can directly or indirectly derive a string that starts with itself. For example, the grammar \(A \rightarrow Aa | b\) is left-recursive because it can derive strings that start with \(A\).</p>
<p>To understand why, consider the following example:
$$
\begin{align*}
A &amp;\rightarrow Aa \ | \ b \\
A &amp;\rightarrow b
\end{align*}
$$</p>
<p>If we try to write a recursive descent parser for this grammar, we might end up with an infinite loop. This is because the parser will keep calling the \(A\) procedure, which will keep calling itself.</p>
<h3 id="left-factoring"><a class="header" href="#left-factoring">Left Factoring</a></h3>
<p>To handle left recursion, we can use a technique called <strong>left factoring</strong>. Left factoring is a technique used to eliminate common prefixes in a grammar. It's a simple process that involves creating a new non-terminal for the common prefixes.</p>
<p>Consider the previous example:
$$
\begin{align*}
A &amp;\rightarrow Aa \ | \ b \\
A &amp;\rightarrow b
\end{align*}
$$</p>
<p>We can factor out the common prefix \(A\) to get:
$$
\begin{align*}
A &amp;\rightarrow bA' \\
A' &amp;\rightarrow aA' \ | \ \epsilon
\end{align*}
$$</p>
<!-- Explanation -->
<p>The new non-terminal \(A'\) represents the remaining part of the original \(A\) rule. The \(\epsilon\) represents the empty string, which is used to terminate the recursion.</p>
<h4 id="example-3"><a class="header" href="#example-3">Example</a></h4>
<p>Since this concept was hard for me to understand, I'll provide an example to help you understand it better.</p>
<p>Consider the previous example again:
$$
\begin{align*}
A &amp;\rightarrow bA' \\
A' &amp;\rightarrow aA' \ | \ \epsilon
\end{align*}
$$</p>
<p>The parser would have the following procedures:</p>
<pre><code class="language-rust ignore">/// Name corresponds with the left-hand side for A -&gt; bA'
fn parse_A() {
    match next_token() {
        Token::B =&gt; {
            consume();
            // Call the A' procedure instead of itself
            parse_A_prime();
        }
        _ =&gt; panic!("Unexpected token"),
    }
}

/// Name corresponds with the left-hand side for A' -&gt; aA' | ε
fn parse_A_prime() {
    match next_token() {
        Token::A =&gt; {
            consume();
            // Call the A' procedure again
            parse_A_prime();
        }
        _ =&gt; {}
    }
}</code></pre>
<p>The loop is broken by the \(\epsilon\) in the \(A'\) rule, which stops the recursion:
$$ A' \rightarrow aA' \ | \ \epsilon $$</p>
<p>Keep in mind that this is a simple example. Left factoring can be more complex for larger grammars. You may unknowingly introduce left recursion when factoring out common prefixes, so be careful.</p>
<h1 id="resources-10"><a class="header" href="#resources-10">Resources</a></h1>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Recursive_descent_parser">Recursive Descent Parsing</a> on Wikipedia</li>
<li><a href="https://maeyler.github.io/Automata-2018/cfg/Bilal_RecursiveDescentParser.html">RDP Visualizer</a> - A visualizer for recursive descent parsers</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="llk-parsing"><a class="header" href="#llk-parsing">LL(k) Parsing</a></h1>
<blockquote>
<p><strong>❗Not required</strong></p>
<p>This section is not required for the book, but is included for completeness.</p>
</blockquote>
<p>Whilst <a href="./recursive_descent_parsing.html">Recursive Descent Parsing (RDP)</a> is predominantly LL(1), it's worth noting that LL(k) parsing is a more general form of top-down parsing. The "LL" stands for "Left-to-right, Leftmost derivation", meaning that the parser reads the input from left to right and constructs a leftmost derivation of the input. The "k" represents the number of tokens of lookahead the parser uses to make parsing decisions.</p>
<p>This will not be implemented in this book, so you may skip it. I'd still recommend reading it to understand the differences between LL(1) and LL(k) parsing.</p>
<p>It is important to understanding <a href="./context_free_grammars.html">CFGs</a> and <a href="./recursive_descent_parsing.html">RDP</a> before reading this section.</p>
<h2 id="introduction-to-llk-parsing"><a class="header" href="#introduction-to-llk-parsing">Introduction to LL(k) Parsing</a></h2>
<p>LL(k) parsers are a superset of LL(k) parsers, where k is 1: whilst LL(1) only use a single look-ahead, LL(k) parsers use k tokens of lookahead to make parsing decisions. This could increase flexibility, but also complexity and memory usage.</p>
<p>Evaluation is similar to the one specified in <a href="./recursive_descent_parsing.html#evaluation">RDP</a>.</p>
<h2 id="how-it-works-1"><a class="header" href="#how-it-works-1">How it works</a></h2>
<p><strong>Parsing tables</strong> are constructed based on the LL(k) grammar, which includes parsing action and goto tables. These tables are used to determine the next parsing action based on the current input token and lookahead. Simply put, parsing functions use these tables to make parsing decisions. They contain entries that map parsing states, input tokens, and lookahead symbols to to parsing actions or transitions.</p>
<p><strong>Parsing actions</strong> are the operations that the parser performs when it encounters a specific input token and lookahead symbol. These actions include shifting the input token onto the parsing stack, reducing the stack based on a grammar rule, or accepting the input as a valid parse.</p>
<p><strong>Goto transitions</strong> are the transitions that the parser makes when it encounters a specific input token and lookahead symbol. These transitions are used to move the parser from one state to another based on the current state and input token.</p>
<h3 id="example-4"><a class="header" href="#example-4">Example</a></h3>
<p>Given the following simple grammar:
$$
\begin{align*}
S &amp;\rightarrow A \\
A &amp;\rightarrow b
\end{align*}
$$</p>
<p>Let's construct the parsing table step by step.</p>
<h4 id="step-1-compute-first-and-follow-sets"><a class="header" href="#step-1-compute-first-and-follow-sets">Step 1: Compute FIRST and FOLLOW Sets</a></h4>
<p>"FIRST" and "FOLLOW" sets are used to construct the parsing table. Let's break down that concept.</p>
<p><strong>FIRST Set</strong>:</p>
<ul>
<li>The FIRST set of a non-terminal symbol is the set of terminals that begin the strings derivable from that non-terminal.
<ul>
<li>If the non-terminal can derive a terminal, that terminal is in the FIRST set.</li>
<li>If the non-terminal can derive a string that starts with another non-terminal, the FIRST set of that non-terminal is also in the FIRST set of the original non-terminal.</li>
<li>Repeat until no new terminals are added to the FIRST set.</li>
</ul>
</li>
</ul>
<p><strong>FOLLOW Set</strong>:</p>
<ul>
<li>The FOLLOW set of a non-terminal symbol is the set of terminals that can appear immediately to the right of the non-terminal in some sentential form.
<ul>
<li>The end-of-input marker \($\) is in the FOLLOW set of the start symbol.</li>
<li>If there is a production \(A \rightarrow \alpha B \beta\), then everything in the FIRST set of \(\beta\) except for \(\epsilon\) is in the FOLLOW set of \(B\).</li>
<li>If there is a production \(A \rightarrow \alpha B\) or \(A \rightarrow \alpha B \beta\) where \(\beta\) is nullable, then everything in the FOLLOW set of \(A\) is in the FOLLOW set of \(B\).</li>
</ul>
</li>
</ul>
<p>Let's derive the FIRST and FOLLOW sets for the given grammar.</p>
<p><strong>FIRST Set</strong>:</p>
<ul>
<li>\(FIRST(S) = {b}\), since \(S \rightarrow A\) and \(A \rightarrow b\).</li>
<li>\(FIRST(A) = {b}\), since \(A \rightarrow b\).</li>
</ul>
<p><strong>FOLLOW Set</strong>:</p>
<ul>
<li>\(FOLLOW(S) = {$}\), since \(S\) is the start symbol.</li>
<li>\(FOLLOW(A) = {$}\), since \(S \rightarrow A\).</li>
</ul>
<h4 id="step-2-construct-the-parsing-table"><a class="header" href="#step-2-construct-the-parsing-table">Step 2: Construct the Parsing Table</a></h4>
<p>We'll create a parsing table with rows corresponding to non-terminal symbols, and columns corresponding to terminal symbols (input \($\)).</p>
<p>These table entries will help indicate whether to perform a production, or an error action.</p>
<div class="table-wrapper"><table><thead><tr><th>Non-Terminal</th><th>Terminal (b)</th><th>$</th></tr></thead><tbody>
<tr><td>S</td><td>S -&gt; A</td><td></td></tr>
<tr><td>A</td><td>A -&gt; b</td><td></td></tr>
</tbody></table>
</div>
<p>The parsing table is constructed based on the FIRST and FOLLOW sets, and the grammar rules. The table is used to determine the next parsing action based on the current input token and lookahead.</p>
<h4 id="step-3-populate-the-parsing-table"><a class="header" href="#step-3-populate-the-parsing-table">Step 3: Populate the Parsing Table</a></h4>
<p>Let's populate the parsing table based on the FIRST and FOLLOW sets.</p>
<p>For each production \(A \rightarrow \alpha\) in the grammar:</p>
<ul>
<li>If \(\epsilon \in FIRST(\alpha)\), then for each terminal \(a\) in \(FOLLOW(A)\), add \(A \rightarrow \alpha\) to the table entry \(A, a\).
<ul>
<li>Otherwise, add \(A \rightarrow \alpha\) to the table entry \(A, a\) for each terminal \(a\) in \(FIRST(\alpha)\).</li>
</ul>
</li>
<li>If \(\epsilon \in FIRST(\alpha)\) and \($ \in FOLLOW(A) \), add \(A \rightarrow \alpha\) to the table entry \(A, $\).</li>
</ul>
<p>The parsing table is populated as follows:</p>
<div class="table-wrapper"><table><thead><tr><th>Non-Terminal</th><th>Terminal (b)</th><th>$</th></tr></thead><tbody>
<tr><td>S</td><td>S -&gt; A</td><td></td></tr>
<tr><td>A</td><td>A -&gt; b</td><td></td></tr>
</tbody></table>
</div>
<p>Unfortunately no difference, as the grammar is simple, and I don't want to restart that process. The process is still clear though.</p>
<h2 id="conclusion"><a class="header" href="#conclusion">Conclusion</a></h2>
<p>LL(k) is a more general form of top-down parsing, and is much more flexible -- albeit more complex -- than LL(1) parsing. We will not be using it in this book.</p>
<h1 id="resources-11"><a class="header" href="#resources-11">Resources</a></h1>
<ul>
<li><a href="https://en.wikipedia.org/wiki/LL_parser">LL(k) Parsing</a> on Wikipedia</li>
<li><a href="https://www.cs.uaf.edu/~cs331/notes/FirstFollow.pdf">LL(k) Parsing</a> on University of Alaska Fairbanks</li>
<li><a href="https://github.com/GabrielMajeri/LL-K-Parser">LL(k) Parser</a>: A Python implementation of an LL(k) parser</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lr-parsing"><a class="header" href="#lr-parsing">LR Parsing</a></h1>
<blockquote>
<p><strong>❗Not required</strong></p>
<p>This section is not required for the book, but is included for completeness.</p>
</blockquote>
<div class="warning">
    <p><strong style='font-size: 2.5rem;'>This chapter is incomplete</strong><br/></p>
</div>
<p>LR Parsing is a bottom-up parsing technique that is more powerful than LL Parsing. It is used to parse large grammars and is more efficient than LL Parsing. The "L" stands for left-to-right scanning of the input, and the "R" stands for constructing a rightmost derivation in reverse.</p>
<p>This will not be implemented in this book, so you may skip it. Compared to <a href="./ll_k_parsing.html">LL(k) parsers</a>, LR parsers are more powerful and can handle a larger class of grammars. They are also more efficient and can parse larger grammars in \(O(n)\) time.</p>
<p>It is important to understand <a href="./recursive_descent_parsing.html">Recursive Descent Parsing (RDP)</a> and <a href="./ll_k_parsing.html">LL(k) Parsing</a> before reading this section.</p>
<h2 id="introduction-to-lr-parsing"><a class="header" href="#introduction-to-lr-parsing">Introduction to LR Parsing</a></h2>
<p>LR Parsing is pretty much starting with the basic ingredients to bake a cake (flour, eggs, etc.) and then adding them in the right order to make a cake. It uses a different approach: <strong>shift-reduce parsing</strong>.</p>
<h3 id="how-it-works-2"><a class="header" href="#how-it-works-2">How it works</a></h3>
<p>LR Parsing uses a <strong>shift-reduce</strong> approach to parsing. It reads the input from left to right and constructs a rightmost derivation of the input. The parser uses a stack to keep track of the input and the grammar rules that have been applied. The parser then uses a set of parsing actions to shift input tokens onto the stack, reduce the stack based on a grammar rule, or accept the input as a valid parse.</p>
<p>Much like LL(k) Parsing, LR Parsing uses a <strong>parsing table</strong> to determine the next parsing action based on the current input token and lookahead. The parsing table contains entries that map parsing states, input tokens, and lookahead symbols to parsing actions or transitions.</p>
<p>As the parser reads the tokens, it shifts them onto a stack while simultaneously reducing them into larger structures. This process involves comparing the tokens to the rules of the programming language's grammar.</p>
<h1 id="resources-12"><a class="header" href="#resources-12">Resources</a></h1>
<ul>
<li><a href="https://en.wikipedia.org/wiki/LR_parser">LR Parsing</a> on Wikipedia</li>
<li><a href="https://www.geeksforgeeks.org/lr-parser/">LR Parsing GfG</a>: GeeksforGeeks article on LR Parsing</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="implementing-an-ast"><a class="header" href="#implementing-an-ast">Implementing an AST</a></h1>
<p>Now that we have:</p>
<ul>
<li>A Lexer</li>
<li>A way to represent tokens</li>
<li>A way to represent errors</li>
</ul>
<p>We now need to implement an Abstract Syntax Tree from the grammar we developed, and derive a parser from it. We'll add some more methods later on during parsing, since the best explanatory example is a working example.</p>
<h2 id="designing-the-ast"><a class="header" href="#designing-the-ast">Designing the AST</a></h2>
<p>Recall the <a href="./context_free_grammars.html#lets-design-a-grammar">grammar we designed</a> for our language. We will use this grammar to design our AST.</p>
<details>
<summary><strong>View the grammar</strong></summary>
<pre><code class="language-ebnf">program           ::= item*

item              ::= function_decl
                    # | struct_decl
                    # | enum_decl
                    # | impl_block

function_decl     ::= "fn" IDENTIFIER "(" {parameter_list} ")" "-&gt;" type block

parameter_list    ::= parameter ("," parameter)*

parameter         ::= IDENTIFIER ":" type

type              ::= primitive_type
                    # | user_defined_type
                    # | tuple_type
                    # | reference_type
                    # | array_type
                    # | function_type

primitive_type    ::= "int" | "float" | "bool"

block             ::= "{" statement* "}"

statement         ::= expression
                    | variable_decl ";"
                    | flow_statement ";"


expression        ::= primary_expression
                    | unary_expression
                    | binary_expression

primary_expression ::= literal
                    | IDENTIFIER
                    | "(" expression ")"
                    | function_call

literal           ::= INT | FLOAT | BOOLEAN

unary_expression  ::= "-" expression | "!" expression #| "&amp;" expression | "*" expression

binary_expression ::= expression OPERATOR expression

function_call     ::= IDENTIFIER "(" arguments ")"
arguments         ::= expression ("," expression)* | ε

variable_decl     ::= "let" IDENTIFIER ":" type "=" expression

flow_statement    ::= "if" expression block {"else" block}
                    | "return" expression?
</code></pre>
</details>
<p>We can simply go one-by-one and design the AST for each non-terminal in the grammar.</p>
<h3 id="file-structure-1"><a class="header" href="#file-structure-1">File Structure</a></h3>
<p>We currently have a <code>src</code> directory with the following structure:</p>
<pre><code>compiler/
|-- Cargo.toml
|-- grammar.bnf
+-- src
  +-- ast           // An empty AST folder
  |-- errors.rs
  |-- lexer.rs
  |-- main.rs
  |-- parser.rs
  |-- token.rs
+-- tests
</code></pre>
<p>It would be significantly beneficial to have a separate file for each non-terminal in the grammar. We can also group them into little categories, which we'll do here:</p>
<p>In <code>src/ast</code>, we'll have:</p>
<ol>
<li><code>core.rs</code> for the core items in the language (<code>Program</code>, etc.)</li>
<li><code>expr.rs</code> for the expressions</li>
<li><code>flow.rs</code> for the flow statements</li>
<li><code>functions.rs</code> for the different function non-terminals</li>
<li><code>nary_expr.rs</code> representing any -ary expressions (unary, binary, etc.)</li>
<li><code>ops.rs</code> for the operators</li>
<li><code>prim_expr.rs</code> for the primary expressions</li>
<li><code>types.rs</code> for all datatypes, including primitive</li>
</ol>
<p>We'll also have a <code>mod.rs</code> file in the <code>ast</code> folder to re-export everything, which will look like this simply:</p>
<pre><code class="language-rust ignore">//! Declares the AST for the language.
//! 
//! Abstract Syntax Trees (ASTs) represent the structure of the language,
//! generated by the parser. Raw parser ASTs should be optimized further.
//! 
//! We can use this to generate the IR (Intermediate Representation) for the
//! language, which is then used to generate the final machine code.

// Load all the modules
mod core;
mod ops;
mod types;
mod prim_expr;
mod nary_expr;
mod expr;
mod functions;
mod flow;
mod utils;

// Re-export everything
pub use self::core::*;
pub use self::ops::*;
pub use self::types::*;
pub use self::prim_expr::*;
pub use self::nary_expr::*;
pub use self::expr::*;
pub use self::functions::*;
pub use self::flow::*;
pub use self::utils::*;</code></pre>
<p>This is a little bit tedious, but let's get started.</p>
<p>It's important to note that since parts of the AST may refer to sister-modules, we'll add <code>use crate::ast::*;</code> to the top of each file.</p>
<h3 id="core-items"><a class="header" href="#core-items">Core Items</a></h3>
<p>This will consist of the <code>Program</code> and <code>Item</code> non-terminals. Additionally, we'll also wrap the entire program into an <code>AST</code> struct, which will also contain the <code>file_id</code>:</p>
<pre><code class="language-rust ignore">use crate::ast::*;

#[derive(Debug)]
pub struct AST {
    pub program: Program,
    pub file_id: usize,
}

#[derive(Debug)]
pub struct Program {
    pub items: Vec&lt;Item&gt;,
}

#[derive(Debug)]
pub enum Item {
    FunctionDecl(FunctionDecl),
}</code></pre>
<h3 id="expressions-1"><a class="header" href="#expressions-1">Expressions</a></h3>
<p>Referring to our grammar, expressions are either:</p>
<ul>
<li>Primary expressions</li>
<li>Unary expressions</li>
<li>Binary expressions</li>
</ul>
<p>Blocks are also expressions, so we'll consider them too.</p>
<pre><code class="language-rust ignore">use crate::ast::*;

#[derive(Debug)]
pub enum Expression {
    Primary(PrimaryExpression),
    Unary(UnaryExpression),
    Binary(BinaryExpression),
}

#[derive(Debug)]
pub struct Block {
    pub statements: Vec&lt;Statement&gt;,
}</code></pre>
<h3 id="flow-statements"><a class="header" href="#flow-statements">Flow Statements</a></h3>
<p>Flow statements consist of <code>if</code>, <code>return</code>, etc. but we will include regular statements within this too.</p>
<p>Referring to our grammar, statements are either:</p>
<ul>
<li>Expressions</li>
<li>Variable declarations</li>
<li>Flow statements</li>
<li>Return statements</li>
</ul>
<p>We will not include the nodes for expressions here.</p>
<pre><code class="language-rust ignore">use crate::ast::*;

#[derive(Debug)]
pub enum Statement {
    Expression(Box&lt;Expression&gt;),
    VariableDecl(VariableDecl),
    Flow(FlowStatement),
    Return(Option&lt;Box&lt;Expression&gt;&gt;),
}

#[derive(Debug)]
pub struct VariableDecl {
    pub ident: Ident,
    pub ty: Type,
    pub expression: Expression,
}

#[derive(Debug)]
pub struct FlowStatement {
    pub condition: Expression,
    pub if_block: Block,
    pub else_block: Option&lt;Block&gt;,
}</code></pre>
<h3 id="functions"><a class="header" href="#functions">Functions</a></h3>
<p>Next, we simply need to determine how a <code>FunctionDecl</code> node will look like. Referring to our grammar, we can abstract away parentheses, commas, etc. and determine that the information a function contains is:</p>
<ol>
<li>An identifier</li>
<li>A list of parameters</li>
<li>A return type</li>
<li>A block (the function body)</li>
</ol>
<p>We should create nodes for parameters too, as they are a part of the function declaration.</p>
<pre><code class="language-rust ignore">use crate::ast::*;

#[derive(Debug)]
pub struct FunctionDecl {
    pub ident: Ident,
    pub parameters: Vec&lt;Parameter&gt;,
    pub ty: Type,
    pub block: Block,
}

#[derive(Debug)]
pub struct Parameter {
    pub ident: Ident,
    pub ty: Type,
}</code></pre>
<h3 id="n-ary-expressions"><a class="header" href="#n-ary-expressions">N-ary Expressions</a></h3>
<p>To support unary and binary operations, we'll add these nodes here.</p>
<p>Unary expressions will simply either be a <code>Negation</code> unary operation, or a <code>Not</code> unary operation.</p>
<pre><code class="language-rust ignore">use crate::ast::*;

#[derive(Debug)]
pub enum UnaryExpression {
    Negation(Box&lt;Expression&gt;),
    Not(Box&lt;Expression&gt;),
}

#[derive(Debug)]
pub struct BinaryExpression {
    pub lhs: Box&lt;Expression&gt;,
    pub op: BinaryOperator,
    pub rhs: Box&lt;Expression&gt;,
}</code></pre>
<h3 id="operators"><a class="header" href="#operators">Operators</a></h3>
<p>This part will be slightly longer, as we need to determine the <strong>precedence</strong> of each operator.</p>
<p>First, let's define the <code>enum</code>:</p>
<pre><code class="language-rust ignore">use crate::ast::*;

#[derive(Debug)]
pub enum BinaryOperator {
    Add,
    Subtract,
    Multiply,
    Divide,
    Modulus,
    Equal,
    NotEqual,
    LessThan,
    GreaterThan,
    LessThanOrEqual,
    GreaterThanOrEqual,
    And,
    Or,
}

pub enum UnaryOperator {
    Negation,
    Not,
}</code></pre>
<p><strong>Operator Precedence</strong> is a critical part of parsing expressions; they define the order in which operators are evaluated. For example, <code>*</code> has a higher precedence than <code>+</code>, so <code>2 + 3 * 4</code> should be parsed as <code>2 + (3 * 4)</code>. You may know this as BIDMAS, BODMAS, or PEMDAS.</p>
<p>First, we know that the highest precedence operator (here, atleast) are the multiplication, division, and modulus operators. Below them are addition and subtraction.</p>
<p>Then there's the relational comparison operators; we would like them to be lower than the arithmetic operations so we can perform evaluations like <code>3 + 4 &lt; 5 + 6</code> as <code>(3 + 4) &lt; (5 + 6)</code>.</p>
<p>Equality comparison operators (<code>==</code>, <code>!=</code>) should have lower precedence than relational operators.</p>
<p>Finally, let's look at the logical operators <code>&amp;&amp;</code> and <code>||</code>. We would like them to have the lowest precedence, <strong>but</strong> there is a difference. Consider <code>true || false &amp;&amp; true</code>. We would like this to be parsed as <code>true || (false &amp;&amp; true)</code>. Thus, <code>&amp;&amp;</code> should have a higher precedence than <code>||</code>.</p>
<p>Let's start from the bottom up and list the operators in terms of order:</p>
<ol>
<li><code>||</code>: <code>Or</code></li>
<li><code>&amp;&amp;</code>: <code>And</code></li>
<li><code>==</code>, <code>!=</code>: Equality Comparison Operators</li>
<li><code>&lt;</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&gt;=</code>: Relational Comparison Operators</li>
<li><code>+</code>, <code>-</code>: Addition and Subtraction</li>
<li><code>*</code>, <code>/</code>, <code>%</code>: Multiplication, Division, and Modulus</li>
</ol>
<p>See the numbers on the list? That would be the precedence value of the operators. We can add this to the <code>BinaryOperator</code> enum:</p>
<pre><code class="language-rust ignore">impl BinaryOperator {
    /// Get the precedence of the operator
    pub fn precedence(&amp;self) -&gt; u8 {
        match self {
            BinaryOperator::Or =&gt; 1,
            BinaryOperator::And =&gt; 2,
            BinaryOperator::Equal | BinaryOperator::NotEqual =&gt; 3,
            BinaryOperator::LessThan
            | BinaryOperator::GreaterThan
            | BinaryOperator::LessThanOrEqual
            | BinaryOperator::GreaterThanOrEqual =&gt; 4,
            BinaryOperator::Add | BinaryOperator::Subtract =&gt; 5,
            BinaryOperator::Multiply | BinaryOperator::Divide | BinaryOperator::Modulus =&gt; 6,
        }
    }
}</code></pre>
<h3 id="primary-expressions"><a class="header" href="#primary-expressions">Primary Expressions</a></h3>
<p>Primary expressions, as referred to in our grammar, are:</p>
<ul>
<li>Literals</li>
<li>Identifiers</li>
<li>Parenthesized expressions</li>
<li>Function calls</li>
</ul>
<p>Keep in mind we do want to minimize the number of AST Nodes we create, so we will skip the creation of paranthesized expressions and function calls. However, we would like to explicitly represent literals and identifiers.</p>
<pre><code class="language-rust ignore">use crate::ast::*;

#[derive(Debug)]
pub enum PrimaryExpression {
    Literal(Literal),
    Ident(Ident),
    Parenthesized(Box&lt;Expression&gt;),
    FunctionCall(Ident, Vec&lt;Expression&gt;),
}

#[derive(Debug)]
pub struct Ident {
    pub ident: String,
}

#[derive(Debug)]
pub enum Literal {
    Int(i32),
    Float(f32),
    Bool(bool),
}</code></pre>
<h3 id="types-1"><a class="header" href="#types-1">Types</a></h3>
<p>Finally, we need to represent the different types in our language. We'll start with the primitive types.</p>
<pre><code class="language-rust ignore">use crate::ast::*;

#[derive(Debug)]
pub enum Type {
    Primitive(PrimitiveType),
}

#[derive(Debug)]
pub enum PrimitiveType {
    Int,
    Float,
    Bool,
}</code></pre>
<h2 id="optional-helpers"><a class="header" href="#optional-helpers">Optional helpers</a></h2>
<p>We can add some helpful methods to display the tree. One option is to implement <a href="https://doc.rust-lang.org/std/fmt/trait.Display.html"><code>Display</code> trait</a>, although I prefer using a custom method for this to keep track of depth or indentation.</p>
<p>Let's create <code>ast/utils.rs</code>:</p>
<pre><code class="language-rust ignore">pub trait PrettyPrint {
    fn pretty_print(&amp;self, indent: usize) -&gt; String;
}</code></pre>
<p>This is a trait we'll implement for each node in the AST, which takes in an <code>indent</code> parameter to determine how much to indent the output.</p>
<p>Unfortunately I will not cover each node, but we'll cover the <code>FunctionDecl</code> node as an example:</p>
<pre><code class="language-rust ignore">impl PrettyPrint for FunctionDecl {
    // format: FuncDecl ident(parameters) -&gt; ty { block }
    fn pretty_print(&amp;self, indent: usize) -&gt; String {
        let mut s = format!(
            "{:indent$}FuncDecl {}({}) -&gt; {} {{\n",
            "",
            self.ident.ident,
            self.parameters
                .iter()
                .map(|p| p.pretty_print(indent))
                .collect::&lt;Vec&lt;String&gt;&gt;()
                .join(", "),
            self.ty.pretty_print(indent),
            indent = indent * 4
        );
        s.push_str(&amp;self.block.pretty_print(indent + 1));
        s.push_str(&amp;format!("{:indent$}}}\n", "", indent = indent * 4));
        s
    }
}</code></pre>
<p>This will allow us to call <code>pretty_print</code> on any node in the AST, and it will return a string representation of the node.</p>
<h1 id="resources-13"><a class="header" href="#resources-13">Resources</a></h1>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Operator-precedence_parser">Operator Precedence</a></li>
<li><a href="https://doc.rust-lang.org/std/boxed/struct.Box.html">Box<T></a></li>
<li><a href="https://doc.rust-lang.org/std/fmt/trait.Display.html"><code>Display</code> trait</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="implementing-a-parser"><a class="header" href="#implementing-a-parser">Implementing a Parser</a></h1>
<p>So far, we have a lexer and a way to represent tokens. Next, we need to implement the parser.</p>
<p>We'll be using <a href="./recursive_descent_parsing.html"><strong>Recursive Descent Parsing</strong> (RDP)</a> to implement the parser.</p>
<h2 id="the-parser-struct"><a class="header" href="#the-parser-struct">The <code>Parser</code> struct</a></h2>
<p>Let's define a struct for our parser. It'll own the stream of <code>Token</code>s, and it should keep track of the position in the stream.</p>
<blockquote>
<p><strong>Note</strong></p>
<p>Instead of owning the <code>Vec&lt;Token&gt;</code>, we could use a reference or better, an iterator. However, for simplicity, we'll use <code>Vec&lt;Token&gt;</code>.</p>
</blockquote>
<h3 id="safe-memory-management"><a class="header" href="#safe-memory-management">Safe memory management</a></h3>
<p>We need to be careful in designing here, since while the stream of tokens may be immutable, the position is <em>not</em>. Let's implement a basic <code>struct</code> and see why it may be a problem:</p>
<pre><code class="language-rust ignore">pub struct Parser {
    tokens: Vec&lt;Token&gt;,
    position: usize,
}

impl Parser {
    // ...

    // Expect a token of a certain type
    fn expect(&amp;mut self, kind: TokenKind) -&gt; Result&lt;&amp;Token&gt; {
        // Get the current token
        let current: &amp;Token = self.current()?;      // Immutable borrow

        // Move to the next token
        self.position += 1;                         // Mutable borrow

        if current.kind == kind {
            Ok(current)
        } else {
            Err("Unexpected token")
        }
    }
}</code></pre>
<p>*<em>Assume the other functions are implemented</em></p>
<p>Here, we have a problem. We're trying to borrow <code>self</code> mutably and immutably at the same time (as labelled in the code). This isn't allowed, since Rust's borrow checker ensures that:</p>
<ol>
<li>There are no mutable references when there are immutable references</li>
<li>There are no references when there are mutable references</li>
</ol>
<p>To solve this problem, we can use <code>RefCell</code> to allow for interior mutability. This will allow us to borrow <code>self</code> mutably and immutably at the same time. It works by checking the borrow rules at runtime, rather than compile time.</p>
<p>Let's redefine <code>Parser</code> using <code>RefCell</code>:</p>
<pre><code class="language-rust ignore">use std::cell::RefCell;

pub struct Parser {
    tokens: Vec&lt;Token&gt;,
    pos: RefCell&lt;usize&gt;,
}</code></pre>
<h3 id="helper-methods-1"><a class="header" href="#helper-methods-1">Helper methods</a></h3>
<p>Before we develop the methods for each non-terminal, we should introduce some helper functions to reduce the boilerplate code we write.</p>
<h4 id="position"><a class="header" href="#position">Position</a></h4>
<p>Whilst we may not use this method when parsing, the helper methods we'll introduce later on will need to get the position we're currently at. Since it's somewhat of a hassle to get the position from the <code>RefCell</code>, we'll define a helper method for it:</p>
<pre><code class="language-rust ignore">impl Parser {
    /// Get position
    fn pos(&amp;self) -&gt; usize {
        *self.pos.borrow()
    }
}</code></pre>
<p>Easy peasy. Now, we can use <code>self.pos()</code> to get the current position.</p>
<p>We may also want to advance, and it's prettier to use functions so:</p>
<pre><code class="language-rust ignore">impl Parser {
    /// Advance the parser by one token
    fn advance(&amp;self) {
        *self.pos.borrow_mut() += 1;
    }
}</code></pre>
<h4 id="token-access"><a class="header" href="#token-access">Token access</a></h4>
<p>Throughout parsing, we'll continually want to get the current token we're parsing. There is a possibility the <code>pos</code> may be <strong>out of bounds</strong>, so we'll opt for returning <code>Option&lt;&amp;Token)</code> -- which, coincidentally, is the return value for <code>Vec::get</code>.</p>
<pre><code class="language-rust ignore">impl Parser {
    /// Get the current token
    fn current(&amp;self) -&gt; Option&lt;&amp;Token&gt; {
        self.tokens.get(self.pos())
    }
}</code></pre>
<p>This is a little messy since we'll need to handle the case where we reached the end of the stream. We can use some helpful features of <code>Option</code> to make this cleaner. This way, we will return <code>Result&lt;&amp;Token&gt;</code> instead, which we can use the <code>?</code> operator to handle.</p>
<pre><code class="language-rust ignore">impl Parser {
    /// Get current token
    /// Raises an error if at EOF
    fn current_or_eof(&amp;self) -&gt; Result&lt;&amp;Token&gt; {
        self.current() // get the current token
            .filter(|token| token.kind != TokenKind::Eof) // remove it if it's EOF
            .ok_or_else(|| anyhow!(self.eof_error())) // raise an error if none
    }
}</code></pre>
<p>There is a lot going on here, so let's break it down:</p>
<ol>
<li><code>self.current()</code> gets the current token</li>
<li><code>filter</code> removes the token if it's EOF</li>
<li><code>ok_or_else</code> raises an error if there are no tokens left</li>
</ol>
<p>We can also create a helper method <code>eof_error()</code> to generate the error message:</p>
<pre><code class="language-rust ignore">impl Parser {
    /// Get the last token and return an EOF error
    fn eof_error(&amp;self) -&gt; LangError {
        let token = self.tokens.last().unwrap();
        LangError::UnexpectedEOF(token.span.clone())
    }
}</code></pre>
<p>Nothing special, all simple.</p>
<h4 id="expecting-a-token"><a class="header" href="#expecting-a-token">Expecting a token</a></h4>
<p>We're often going to need to expect a certain token, simply meaning we need to guarantee that the token we're currently parsing is exactly a certain kind. The process is essentially:</p>
<ol>
<li>Get the current token and advance</li>
<li>Check if it's the expected token</li>
</ol>
<p>Nothing too major.</p>
<pre><code class="language-rust ignore">impl Parser {
    /// Expect a token and advance
    /// Does not work with EOF
    fn expect(&amp;mut self, kind: TokenKind) -&gt; Result&lt;&amp;Token&gt; {
        let current: &amp;Token = self.current_or_eof()?;
        self.advance();

        if current.kind == kind {
            Ok(current)
        } else {
            Err(anyhow!(LangError::ExpectedToken {
                expected: kind,
                found: current.kind.clone(),
                span: current.span.clone(),
            }))
        }
    }
}</code></pre>
<h2 id="non-terminal-methods"><a class="header" href="#non-terminal-methods">Non-terminal methods</a></h2>
<p>Let's start with the start symbol \(S\) of our grammar: <code>program</code>.</p>
<p>Recall the <a href="./context_free_grammars.html">grammar</a> we defined:</p>
<details>
<summary><strong>View the grammar</strong></summary>
<pre><code class="language-ebnf">program           ::= item*

item              ::= function_decl
                    # | struct_decl
                    # | enum_decl
                    # | impl_block

function_decl     ::= "fn" IDENTIFIER "(" {parameter_list} ")" "-&gt;" type block

parameter_list    ::= parameter ("," parameter)*

parameter         ::= IDENTIFIER ":" type

type              ::= primitive_type
                    # | user_defined_type
                    # | tuple_type
                    # | reference_type
                    # | array_type
                    # | function_type

primitive_type    ::= "int" | "float" | "bool"

block             ::= "{" statement* "}"

statement         ::= expression
                    | variable_decl
                    | flow_statement
                    | return_statement


expression        ::= primary_expression
                    | unary_expression
                    | binary_expression

primary_expression ::= literal
                    | IDENTIFIER
                    | "(" expression ")"
                    | function_call

literal           ::= INT | FLOAT | BOOLEAN

unary_expression  ::= "-" expression | "!" expression #| "&amp;" expression | "*" expression

binary_expression ::= expression OPERATOR expression

function_call     ::= IDENTIFIER "(" arguments ")"
arguments         ::= expression ("," expression)* | ε

variable_decl     ::= "let" IDENTIFIER ":" type "=" expression ";"

flow_statement    ::= "if" expression block {"else" block}

return_statement  ::= "return" {expression} ";"
</code></pre>
</details>
<p>The program section:</p>
<pre><code class="language-ebnf">program           ::= item*
</code></pre>
<h3 id="program"><a class="header" href="#program"><code>Program</code></a></h3>
<p>Simply means to parse 0 or more items. We can implement this as a loop that keeps parsing items until we reach the end of the file.</p>
<pre><code class="language-rust ignore">    /// Parse a program
    fn program(&amp;mut self) -&gt; Result&lt;Program&gt; {
        let mut items = Vec::new();
        while self.current().is_some() {
            items.push(self.item()?);
        }

        Ok(Program { items })
    }</code></pre>
<h3 id="item"><a class="header" href="#item"><code>Item</code></a></h3>
<p>The <code>item</code> non-terminal is where we introduce how to implement the pipe <code>|</code> in a parser. Right now, we are only parsing functions, so we'll only implement the function declaration. Regardless, we will implement it for scalability so we can add extra items later.</p>
<pre><code class="language-rust ignore">    /// Parse a single item
    fn item(&amp;mut self) -&gt; Result&lt;Item&gt; {
        let current = self.current_or_eof()?;

        let item = match current.kind {
            TokenKind::Fn =&gt; Item::FunctionDecl(self.function()?),
            _ =&gt; {
                return Err(anyhow!(LangError::ExpectedAnyToken {
                    expected: vec![TokenKind::Fn,],
                    found: current.kind.clone(),
                    span: current.span.clone(),
                }))
            }
        };

        Ok(item)
    }</code></pre>
<h3 id="function-and-parameter"><a class="header" href="#function-and-parameter"><code>Function</code> and <code>Parameter</code></a></h3>
<p>The <code>function</code> non-terminal grammar is as follows:</p>
<pre><code class="language-ebnf">function_decl     ::= "fn" IDENTIFIER "(" {parameter_list} ")" "-&gt;" type block
</code></pre>
<p>There's a few new things here, main one being accumulating information throughout the process. The process is still straightforward, we follow the grammar:</p>
<ol>
<li>Expect <code>fn</code></li>
<li>Expect an <code>ident</code>, and store it</li>
<li>Expect <code>(</code>, and collect parameters
<ol>
<li>Get and store <code>ident</code></li>
<li>Expect <code>:</code></li>
<li>Get and store <code>type</code></li>
<li>Repeat until <code>)</code></li>
</ol>
</li>
<li>Expect <code>)</code></li>
<li>Expect <code>-&gt;</code></li>
<li>Get and store <code>type</code></li>
<li>Parse and store <code>block</code></li>
</ol>
<p>Let's implement it step by step. Let's start with the function signature, not including the body (<code>block</code>).</p>
<pre><code class="language-rust ignore">    fn function(&amp;mut self) -&gt; Result&lt;FunctionDecl&gt; {
        // "fn"
        self.expect(TokenKind::Fn)?;

        // IDENTIFIER
        let ident = self.ident()?;

        // "("
        self.expect(TokenKind::LParen)?;

        // parameter list
        let mut params: Vec&lt;Parameter&gt; = Vec::new();
        while self.current_or_eof()?.kind != TokenKind::RParen {
            let param = self.parameter()?;
            params.push(param);

            if self.current_or_eof()?.kind == TokenKind::Comma {
                self.advance();
            }
        }

        // ")"
        self.expect(TokenKind::RParen)?;

        // "-&gt;"
        self.expect(TokenKind::Arrow)?;

        // type
        let ty = self.type_()?;

        unimplemented!()
    }</code></pre>
<p>Here, we defined a few new functions:</p>
<ul>
<li><code>ident</code>
<ul>
<li>Although tokenization makes this simple, we like to reduce the boilerplate code we write</li>
</ul>
</li>
<li><code>parameter</code></li>
<li><code>type_</code>
<ul>
<li>Same as above</li>
</ul>
</li>
</ul>
<p>Finally, we should parse the block, and return the function. No, we will not implement the block parsing inside the <code>function</code> method. <code>block</code> is a non-terminal which exists in the grammar, so it will have its own method.</p>
<p>The reason we didn't create a method for <code>parameter_list</code> despite it being a non-terminal is for simplicity: the only dependant is <code>function</code>, so we can just implement it inside <code>function</code>. It's still perfectly fine (and probably better) to implement it as a separate method.</p>
<p>Let's add the block parsing and return the function:</p>
<pre><code class="language-rust ignore">    fn function(&amp;mut self) -&gt; Result&lt;FunctionDecl&gt; {
        // ...

        // block
        let block = self.block()?;

        Ok(FunctionDecl {
            ident,
            parameters: params,
            ty,
            block,
        })
    }</code></pre>
<p>The function signature isn't fully complete yet, we need to implement parsing the parameters too. Let's do that now.</p>
<pre><code class="language-rust ignore">    fn parameter(&amp;mut self) -&gt; Result&lt;Parameter&gt; {
        // Expect and store ident
        let ident = self.ident()?;

        // ":"
        self.expect(TokenKind::Colon)?;

        // Get and store type
        let ty = self.type_()?;
        Ok(Parameter { ident, ty })
    }</code></pre>
<h3 id="primitives-types-etc"><a class="header" href="#primitives-types-etc">Primitives, types, etc.</a></h3>
<p>We've reached a point where there are quite a lot of errors on the screen though. Let's cut down on how many errors we see by implementing the <code>type</code>, and <code>ident</code> methods now.</p>
<h4 id="types-2"><a class="header" href="#types-2">Types</a></h4>
<p>Types are simple, we can just match and return a token which are valid types. We can put the primitive types here for now:</p>
<pre><code class="language-rust ignore">    fn type_(&amp;mut self) -&gt; Result&lt;Type&gt; {
        match self.current_or_eof()?.kind {
            TokenKind::Int =&gt; {
                self.advance();
                Ok(Type::Primitive(PrimitiveType::Int))
            }
            TokenKind::Bool =&gt; {
                self.advance();
                Ok(Type::Primitive(PrimitiveType::Bool))
            }
            _ =&gt; Err(anyhow!(LangError::ExpectedAnyToken {
                expected: vec![TokenKind::Int, TokenKind::Bool,],
                found: self.current_or_eof()?.kind.clone(),
                span: self.current_or_eof()?.span.clone(),
            })),
        }
    }</code></pre>
<h4 id="ident"><a class="header" href="#ident"><code>ident</code></a></h4>
<p>Identifier is very very straight-forward. Thanks to our tokenization, we don't need any complex rules since the identifier tokens are already recognised by the lexer. This is simply a function to reduce boilerplate by giving a consistent error message:</p>
<pre><code class="language-rust ignore">    fn ident(&amp;mut self) -&gt; Result&lt;Ident&gt; {
        match self.current_or_eof()?.kind {
            TokenKind::Ident(ref ident) =&gt; {
                let ident = ident.clone();
                self.advance();
                Ok(Ident { ident })
            }
            _ =&gt; Err(anyhow!(LangError::ExpectedToken {
                expected: TokenKind::Ident("".to_string()),
                found: self.current_or_eof()?.kind.clone(),
                span: self.current_or_eof()?.span.clone(),
            })),
        }
    }</code></pre>
<h3 id="blocks"><a class="header" href="#blocks">Blocks</a></h3>
<p>Recall the <code>block</code> non-terminal production rule:</p>
<pre><code class="language-ebnf">block             ::= "{" statement* "}"
</code></pre>
<p>This is pretty simple, we just need to parse 0 or more statements. We can implement this as a loop that keeps parsing statements until we reach the end of the block.</p>
<pre><code class="language-rust ignore">    fn block(&amp;mut self) -&gt; Result&lt;Block&gt; {
        // "{"
        self.expect(TokenKind::LBrace)?;

        let mut statements = Vec::new();
        while self.current_or_eof()?.kind != TokenKind::RBrace {
            statements.push(self.statement()?);
        }

        // "}"
        self.expect(TokenKind::RBrace)?;

        Ok(Block { statements })
    }</code></pre>
<h3 id="statements--variable_decl-flow_statement-return_statement"><a class="header" href="#statements--variable_decl-flow_statement-return_statement">Statements (+ <code>variable_decl</code>, <code>flow_statement</code>, <code>return_statement</code>)</a></h3>
<p>Now we're entering the territory of parsing expressions, something a lot of people struggle with.</p>
<pre><code class="language-ebnf">statement         ::= expression
                    | variable_decl ";"
                    | return_statement ";"
                    | flow_statement ";"
</code></pre>
<p>For this non-terminal, the production rule will be either <code>expression</code>, <code>variable_decl</code>, or <code>flow_statement</code>. The easiest and clearest way to determine which to branch off to is simply the starting token:</p>
<ul>
<li><code>Let</code> implies <code>variable_decl</code></li>
<li><code>If</code> implies <code>flow_statement</code></li>
<li><code>Return</code> implies <code>return_statement</code></li>
<li>Anything else implies <code>expression</code></li>
</ul>
<p>We can implement this as a simple match statement:</p>
<pre><code class="language-rust ignore">    fn statement(&amp;mut self) -&gt; Result&lt;Statement&gt; {
        match self.current_or_eof()?.kind {
            TokenKind::Let =&gt; self.variable_decl(),
            TokenKind::If =&gt; self.flow_statement(),
            TokenKind::Return =&gt; self.return_statement(),
            _ =&gt; Err(anyhow!(LangError::ExpectedAnyToken {
                expected: vec![TokenKind::Let,],
                found: self.current_or_eof()?.kind.clone(),
                span: self.current_or_eof()?.span.clone(),
            })),
        }
    }</code></pre>
<p>Let's explore each in order:</p>
<h4 id="variable_decl"><a class="header" href="#variable_decl"><code>variable_decl</code></a></h4>
<p>The <code>variable_decl</code> non-terminal grammar is as follows:</p>
<pre><code class="language-ebnf">variable_decl     ::= "let" IDENTIFIER ":" type "=" expression ";"
</code></pre>
<p>A relatively trivial process, we just need to follow the grammar:</p>
<pre><code class="language-rust ignore">    fn variable_decl(&amp;mut self) -&gt; Result&lt;Statement&gt; {
        // "let"
        self.expect(TokenKind::Let)?;

        // IDENTIFIER
        let ident = self.ident()?;

        // ":"
        self.expect(TokenKind::Colon)?;

        // type
        let ty = self.type_()?;

        // "="
        self.expect(TokenKind::Equals)?;

        // expression
        let expression = self.expression()?;

        let var_decl = Statement::VariableDecl(VariableDecl {
            ident,
            ty,
            expression,
        });

        // ";"
        self.expect(TokenKind::Semicolon)?;

        Ok(var_decl)
    }</code></pre>
<h4 id="flow_statement"><a class="header" href="#flow_statement"><code>flow_statement</code></a></h4>
<p>The <code>flow_statement</code> non-terminal grammar is as follows:</p>
<pre><code class="language-ebnf">flow_statement    ::= "if" expression block {"else" block}
</code></pre>
<p>This is a simple process:</p>
<ol>
<li>Expect <code>if</code></li>
<li>Parse and store <code>expression</code></li>
<li>Parse and store <code>block</code></li>
<li>Branch depending on token, and store:
<ul>
<li><code>else</code>: parse and store <code>block</code> -&gt; <code>Some(block)</code></li>
<li>Anything else: return <code>None</code></li>
</ul>
</li>
</ol>
<pre><code class="language-rust ignore">    fn flow_statement(&amp;mut self) -&gt; Result&lt;Statement&gt; {
        // "if"
        self.expect(TokenKind::If)?;

        // condition
        let condition = self.expression()?;

        // block
        let if_block = self.block()?;

        // may be else, may be not
        let else_block = if self.current_or_eof()?.kind == TokenKind::Else {
            self.advance();
            Some(self.block()?)
        } else {
            None
        };

        let stmt = Statement::Flow(FlowStatement {
            condition,
            if_block,
            else_block,
        });

        Ok(stmt)
    }</code></pre>
<h4 id="return_statement"><a class="header" href="#return_statement"><code>return_statement</code></a></h4>
<p>The <code>return_statement</code> non-terminal grammar is as follows:</p>
<pre><code class="language-ebnf">return_statement  ::= "return" {expression} ";"
</code></pre>
<p>We can convert this to code quite easily:</p>
<pre><code class="language-rust ignore">    fn return_statement(&amp;mut self) -&gt; Result&lt;Statement&gt; {
        // "return"
        self.expect(TokenKind::Return)?;

        // expression or not
        let expression = if self.current_or_eof()?.kind != TokenKind::Semicolon {
            Some(Box::new(self.expression()?))
        } else {
            None
        };

        // ";"
        self.expect(TokenKind::Semicolon)?;

        let stmt = Statement::Return(expression);

        Ok(stmt)
    }</code></pre>
<h3 id="expressions-2"><a class="header" href="#expressions-2">Expressions</a></h3>
<p>Prior to this, we followed straight-forward rules. Here, we'll have to implement operator precedence. This is a bit more complex, but we'll start with the simple stuff.</p>
<blockquote>
<p><strong>Note</strong></p>
<p>Associativity will not be covered, since we don't have any right-associative operators (like exponentiation).</p>
</blockquote>
<h4 id="expression"><a class="header" href="#expression"><code>expression</code></a></h4>
<p>Here's the grammar for <code>expression</code>:</p>
<pre><code class="language-ebnf">expression        ::= primary_expression
                    | unary_expression
                    | binary_expression
</code></pre>
<p>This states that it can be one of stated three expression types. First, we'll make a simplistic approach where we calculate the left-hand side (lhs), and then the right-hand side (rhs). We'll then combine them into a <code>BinaryExpression</code> if we find an operator.</p>
<pre><code class="language-rust ignore">    fn expression(&amp;mut self) -&gt; Result&lt;Expression&gt; {
        let lhs = self.primary_expression()?;

        // Check if there's an operator
        if let Some(op) = self.current()?.binary_operator() {
            self.advance();
            let rhs = self.expression()?;
            Ok(Expression::Binary(BinaryExpression {
                lhs: Box::new(lhs),
                op: op.clone(),
                rhs: Box::new(rhs),
            }))
        } else {
            Ok(lhs)
        }
    }</code></pre>
<p>The problem with this approach is that it doesn't handle operator precedence (i.e. the following binary expression will take precedence over the previous one).</p>
<p>Instead of determining the left and right-hand side, we'll maintain a mutable <code>expr</code> variable and keep updating it as we parse the expression.</p>
<p>To do this, we would terminate the loop of updating <code>expr</code> when the operator being parsed is of a lower precedence than the previous expression. We need a way to determine the operator of a given expression first, if any. Let's implement a helper method in <code>src/ast/expr.rs</code>:</p>
<pre><code class="language-rust ignore">impl Expression {
    /// Get the binary operator of the expression, if any
    pub fn binary_operator(&amp;self) -&gt; Option&lt;&amp;BinaryOperator&gt; {
        match self {
            Expression::Binary(binary_expr) =&gt; Some(&amp;binary_expr.op),
            _ =&gt; None,
        }
    }
}</code></pre>
<p>With this, we can start implementing the new algorithm. Here's the process:</p>
<ol>
<li>Parse &amp; store <code>expr</code></li>
<li>While there is an operator, continue this loop:
<ol>
<li>If the operator has a lower precedence than the previous operator, break the loop</li>
<li>Parse the <code>rhs = primary()</code> and store it</li>
<li>Update <code>expr</code> to a new <code>BinaryExpression</code> with <code>lhs</code>, <code>rhs</code>, and the current <code>op</code></li>
<li>Repeat the loop with the top condition</li>
</ol>
</li>
</ol>
<p>The new algorithm will allow us to retain information about the left-hand side expression and whether or not it should take precedence over the new right-hand side.</p>
<pre><code class="language-rust ignore">    fn expression(&amp;mut self) -&gt; Result&lt;Expression&gt; {
        let mut expr = self.primary()?;

        while let Some(op) = self.current_or_eof()?.kind.as_binary_operator() {
            let op_precedence = op.precedence();

            // Check if the operator precedence is higher than the previous operator
            if let Some(prev_op) = expr.binary_operator() {
                if op_precedence &lt;= prev_op.precedence() {
                    break; // Break the loop if the current operator has lower precedence
                }
            }

            // Consume the operator token
            self.advance();

            // Parse the right-hand side expression
            let rhs = self.primary()?;

            // Parse binary expression
            expr = Expression::Binary(BinaryExpression {
                lhs: Box::new(expr),
                op: op,
                rhs: Box::new(rhs),
            });
        }

        Ok(expr)
    }</code></pre>
<h4 id="primary_expression"><a class="header" href="#primary_expression"><code>primary_expression</code></a></h4>
<p>The <code>primary_expression</code> non-terminal grammar is as follows:</p>
<pre><code class="language-ebnf">primary_expression ::= literal
                    | IDENTIFIER
                    | "(" expression ")"
                    | function_call
</code></pre>
<p>This is a straightforward <code>match</code> statement, and so we'll implement it as such:</p>
<pre><code class="language-rust ignore">    fn primary(&amp;mut self) -&gt; Result&lt;Expression&gt; {
        match self.current_or_eof()?.kind {
            TokenKind::IntLiteral(_) | TokenKind::BoolLiteral(_) =&gt; {
                self.literal().map(Expression::Primary)
            }
            TokenKind::Ident(_) =&gt; {
                let ident = self.ident()?;
                Ok(Expression::Primary(PrimaryExpression::Ident(ident)))
            }
            TokenKind::LParen =&gt; {
                self.advance();
                let expr = self.expression()?;
                self.expect(TokenKind::RParen)?;
                Ok(Expression::Primary(PrimaryExpression::Parenthesized(
                    Box::new(expr),
                )))
            }
            _ =&gt; Err(anyhow!(LangError::ExpectedAnyToken {
                expected: vec![
                    TokenKind::IntLiteral(0),
                    TokenKind::BoolLiteral(false),
                    TokenKind::Ident("".to_string()),
                    TokenKind::LParen,
                ],
                found: self.current_or_eof()?.kind.clone(),
                span: self.current_or_eof()?.span.clone(),
            })),
        }
    }</code></pre>
<h4 id="unary-expressions"><a class="header" href="#unary-expressions">Unary expressions</a></h4>
<p>For simplicity sake, we'll implement the unary operators within the <code>expression</code> function.</p>
<p>To do this, we'll add a loop to handle unary operators. We'll parse the right-hand side expression, and then handle the unary operator. We'll then update <code>expr</code> to a new <code>UnaryExpression</code> with the current <code>op</code> and <code>rhs</code>.</p>
<p>Let's adjust the loop:</p>
<pre><code class="language-rust ignore">        while let Some(op) = self.current_or_eof()?.kind.as_binary_operator() {
            // ...

            // Parse the right-hand side expression
            let rhs = self.primary()?;

            // Handle unary operators
            while let Some(unary_op) = self.current_or_eof()?.kind.as_unary_operator() {
                // Consume the operator token
                self.advance();

                // Parse the right-hand side expression
                let rhs = self.primary()?;

                let unary_expression = match unary_op {
                    UnaryOperator::Negate =&gt; UnaryExpression::Negation(Box::new(rhs)),
                    UnaryOperator::Not =&gt; UnaryExpression::Not(Box::new(rhs)),
                };

                expr = Expression::Unary(unary_expression);
            }

            // Parse binary expression
            expr = Expression::Binary(BinaryExpression {
                lhs: Box::new(expr),
                op: op,
                rhs: Box::new(rhs),
            });
        }</code></pre>
<h4 id="literal"><a class="header" href="#literal"><code>literal</code></a></h4>
<p>The <code>literal</code> non-terminal grammar is as follows:</p>
<pre><code class="language-ebnf">literal           ::= INT | FLOAT | BOOLEAN
</code></pre>
<p>This is a simple process, we just need to match and return a token which are valid literals:</p>
<pre><code class="language-rust ignore">    fn literal(&amp;mut self) -&gt; Result&lt;PrimaryExpression&gt; {
        match self.current_or_eof()?.kind {
            TokenKind::IntLiteral(value) =&gt; {
                self.advance();
                Ok(PrimaryExpression::Literal(Literal::Int(value)))
            }
            TokenKind::BoolLiteral(value) =&gt; {
                self.advance();
                Ok(PrimaryExpression::Literal(Literal::Bool(value)))
            }
            _ =&gt; Err(anyhow!(LangError::ExpectedAnyToken {
                expected: vec![TokenKind::IntLiteral(0), TokenKind::BoolLiteral(false),],
                found: self.current_or_eof()?.kind.clone(),
                span: self.current_or_eof()?.span.clone(),
            })),
        }
    }</code></pre>
<h4 id="function-calls"><a class="header" href="#function-calls">Function calls</a></h4>
<p>Again, we will directly implement parsing function calls inside <code>primary</code> (primary_expressions). This is because function calls are a part of the primary expression, and so it makes sense to implement it here.</p>
<p>In the <code>TokenKind::Ident(_)</code> match, we'll check if the next token is an <code>LParen</code>. If it is, we'll parse the arguments and return a <code>FunctionCall</code> expression.</p>
<pre><code class="language-rust ignore">            TokenKind::Ident(_) =&gt; {
                let ident = self.ident()?;

                // Function call
                if self.current_or_eof()?.kind == TokenKind::LParen {
                    self.advance();
                    let args = self.expression_list()?;
                    self.expect(TokenKind::RParen)?;
                    Ok(Expression::Primary(PrimaryExpression::FunctionCall(
                        FunctionCall { ident, args },
                    )))
                } else {
                    Ok(Expression::Primary(PrimaryExpression::Ident(ident)))
                }
            }</code></pre>
<p>Let's implement the <code>expression_list</code> method now. This is a simple process, we just need to parse 0 or more expressions. We can implement this as a loop that keeps parsing expressions until we reach the end of the list.</p>
<pre><code class="language-rust ignore">    fn expression_list(&amp;mut self) -&gt; Result&lt;Vec&lt;Expression&gt;&gt; {
        let mut args = Vec::new();
        while self.current_or_eof()?.kind != TokenKind::RParen {
            args.push(self.expression()?);
            if self.current_or_eof()?.kind == TokenKind::Comma {
                self.advance();
            }
        }
        Ok(args)
    }</code></pre>
<h2 id="testing"><a class="header" href="#testing">Testing</a></h2>
<p>That's quite a lot of code. In fact, currently my <code>parser.rs</code> file is at 482 lines! It's a good idea to test the parser to ensure it's working as expected.</p>
<p>This section would a lot cleaner if you completed the section on <a href="./implementing_an_ast.html#optional-helpers">AST Optional Helper Methods</a>. You can still debug-print them if you haven't, but it's a lot more convenient to use the helper methods.</p>
<p>Let's run the test on this file:</p>
<pre><code class="language-rust ignore">fn main() -&gt; int {
    let a: int = 5 + 2;
    let b: int = 6;
    let c: int = a + b;
    return c;
}</code></pre>
<p>This is a test in the <code>main::add_compile</code>, so we can try <code>cargo test add_compile -- -nocapture</code> to test &amp; view the output.</p>
<p>We get the following output:</p>
<pre><code>Program
    FuncDecl main() -&gt; int {
        Block
            VariableDecl a:int =
                BinaryExpression
                op=Add
                    Int 5
                    Int 2
            VariableDecl b:int =
                Int 6
            VariableDecl c:int =
                BinaryExpression
                op=Add
                    Ident a
                    Ident b
            Return
                Ident c
    }
</code></pre>
<p>It seems to be working pretty well! One of the typical errors is forgetting a semicolon, so let's see if it catches that too.</p>
<pre><code class="language-rust ignore">fn main() -&gt; int {
    let a: int = 5 + fn 2;
    let b: int = 6;
    let c: int = a + b;
    return c;
}</code></pre>
<p>With the same call, we get:</p>
<pre><code class="language-plaintext">Error:
   ╭─[tests/add.pyl:2:22]
   │
 2 │     let a: int = 5 + fn 2;
   │                      ─┬
   │                       ╰── Expected any of the tokens: `IntegerLiteral`, `BooleanLiteral`, `Ident`, `(`, found: `fn`
───╯
Error: Failed to parse file
</code></pre>
<h1 id="resources-14"><a class="header" href="#resources-14">Resources</a></h1>
<ul>
<li><a href="https://doc.rust-lang.org/std/cell/struct.RefCell.html">RefCell documentation</a></li>
<li><a href="https://doc.rust-lang.org/book/ch15-05-interior-mutability.html">RefCell in book</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="semantic-analysis"><a class="header" href="#semantic-analysis">Semantic Analysis</a></h1>
<p>Semantic Analysis is a stage in the compilation process where the compiler checks the program for semantic errors; they're errors that occur when the program is syntactically correct but the meaning of the program is incorrect. For example, if you try to add a number to a string, the parser will not catch this error during parsing because the syntax is correct. However, the program will not run correctly because you cannot add a number to a string.</p>
<p>This code will be contained in the module <code>src/semantic_analysis</code>, so be sure to create that directory and add a file <code>mod.rs</code> to it.</p>
<blockquote>
<p><strong>Note</strong></p>
<p>Before implementing, it's advisable to adjust your error module to accept <strong>warnings</strong> too, since we will be analyzing the code for both errors and warnings.</p>
<p>We will not cover it here.</p>
</blockquote>
<h1 id="resources-15"><a class="header" href="#resources-15">Resources</a></h1>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Symbol_table">Symbol Table - Wikipedia</a></li>
<li><a href="https://www.geeksforgeeks.org/symbol-table-compiler/">Compiler Design: Symbol Table - GeeksforGeeks</a></li>
<li><a href="https://home.adelphi.edu/~siegfried/cs372/372l3.pdf">Symbol Table Lecture Slides</a> at Adelphi University</li>
<li><a href="https://www.tutorialspoint.com/compiler_design/compiler_design_symbol_table.htm">Symbol Table - Tutorialspoint</a></li>
<li><a href="https://en.wikipedia.org/wiki/Persistent_array">Scope Stacks - Persistant Array</a></li>
<li><a href="https://en.wikipedia.org/wiki/Type_checking">Type Checking</a></li>
<li><a href="https://raw.githubusercontent.com/mgrabmueller/AlgorithmW/master/pdf/AlgorithmW.pdf">Algorithm W</a> by Martin Grabmüller</li>
<li><a href="https://pfudke.wordpress.com/2014/11/20/hindley-milner-type-inference-a-practical-example-2/">Hindley-Milner System</a></li>
<li><a href="https://okmij.org/ftp/ML/generalization.html">OCaml's type checker</a></li>
<li><a href="https://doc.rust-lang.org/std/boxed/struct.Box.html">Box</a></li>
<li><a href="https://web.mit.edu/rust-lang_v1.25/arch/amd64_ubuntu1404/share/doc/rust/html/book/first-edition/the-stack-and-the-heap.html">Memory Stack and Heap</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="symbol-tables"><a class="header" href="#symbol-tables">Symbol Tables</a></h1>
<h2 id="introduction-3"><a class="header" href="#introduction-3">Introduction</a></h2>
<p>When performing semantic analysis, you may need to check the validity (and usage) of identifiers like variables and functions. For example, there may be a function call to a function which hasn't been declared yet.</p>
<p>To solve this, we use <strong>Symbol Tables</strong>. These tables store information about identifiers in the program, such as their type, scope, and location. They help the compiler resolve identifiers to their declarations and detect errors like undeclared variables or functions.</p>
<h2 id="components-of-a-symbol-table"><a class="header" href="#components-of-a-symbol-table">Components of a Symbol Table</a></h2>
<p>A symbol table typically consists of:</p>
<ul>
<li><strong>Entries</strong>: Data structures representing individual symbols (e.g., identifier, type)</li>
<li><strong>Scope</strong>: A region of the program where a symbol is visible and accessible
<ul>
<li>It is often hierarchical, with symbol tables containing a reference to the parent scope</li>
</ul>
</li>
<li><strong>Operations</strong>: Common operations include insertion, lookup, and deletion of symbols</li>
</ul>
<p>Let's look at an example function declaration in Python:</p>
<pre><code class="language-python ignore">x: int = 10
y: int = 20
z: int = x + y
</code></pre>
<p>Let's go construct a symbol table step by step.</p>
<div class="table-wrapper"><table><thead><tr><th>Line</th><th>Symbol Table</th></tr></thead><tbody>
<tr><td><code>x: int = 10</code></td><td>Add <code>x: int</code>: <br/> <table> <thead> <tr> <th>Symbol</th> <th>Type</th> </tr> </thead> <tbody> <tr> <td>x</td> <td>int</td> </tr> </table> <br/></td></tr>
<tr><td><code>y: int = 20</code></td><td>Add <code>y: int</code>: <br/> <table> <thead> <tr> <th>Symbol</th> <th>Type</th> </tr> </thead> <tbody> <tr> <td>x</td> <td>int</td> </tr> <tr> <td>y</td> <td>int</td> </tr> </table> <br/></td></tr>
</tbody></table>
</div>
<p><code>x</code> and <code>y</code> (of type <code>int</code>) were declared here, so we insert them directly into the symbol table.</p>
<p>The next line <code>z: int = x + y</code> involves an expression with <code>x</code> and <code>y</code>. We need to check if <code>x</code> and <code>y</code> are declared before. If they are, we can add <code>z</code> to the symbol table.</p>
<div class="table-wrapper"><table><thead><tr><th>Line</th><th>Symbol Table</th></tr></thead><tbody>
<tr><td><code>z: int = x + y</code></td><td>1. Ensure if <code>x</code> and <code>y</code> are declared. <br/> 2. Add <code>z: int</code>: <br/> <table> <thead> <tr> <th>Symbol</th> <th>Type</th> </tr> </thead> <tbody> <tr> <td>x</td> <td>int</td> </tr> <tr> <td>y</td> <td>int</td> </tr> <tr> <td>z</td> <td>int</td> </tr> </table> <br/></td></tr>
</tbody></table>
</div>
<p>And since they were declared, we can add <code>z</code> to the symbol table! Let's add a new line of code:</p>
<pre><code class="language-python ignore">x: int = 10
y: int = 20
z: int = x + y
a: int = b + c  # New line
</code></pre>
<div class="table-wrapper"><table><thead><tr><th>Line</th><th>Symbol Table</th></tr></thead><tbody>
<tr><td><code>a: int = b + c</code></td><td>1. <code>b</code> and <code>c</code> are undeclared, raise an error <br/> 2. Add <code>a: int</code>: <br/> <table> <thead> <tr> <th>Symbol</th> <th>Type</th> </tr> </thead> <tbody> <tr> <td>x</td> <td>int</td> </tr> <tr> <td>y</td> <td>int</td> </tr> <tr> <td>z</td> <td>int</td> </tr> <tr> <td>a</td> <td>int</td> </tr> </table> <br/></td></tr>
</tbody></table>
</div>
<p>Here, <code>b</code> and <code>c</code> in <code>a</code>'s declaration are undeclared, so we raise an error.</p>
<p>You hopefully noticed that we still added <code>a</code> to the symbol table. We did this because the error was raised, and it's guaranteed the program will not continue to the next stage of the compilation process. However, since we will continue to look through the program for errors, it's useful to keep track of all the symbols we've seen so far. This is known as <strong>error recovery</strong>.</p>
<h2 id="scopes"><a class="header" href="#scopes">Scopes</a></h2>
<p>Hash Tables are a popular choice for implementing symbol tables, due to their \(O(1)\) average-case lookup time. Without going into much detail, in a hash table-based symbol table, identifiers are mapped to unique indices using a hash function. Because of this, we can quickly find the symbol's entry in the table.</p>
<p>Let's look at a simplistic implementation in C++. We'll assume that we have the <code>SymbolEntry</code> class defined elsewhere.</p>
<pre><code class="language-cpp">#include &lt;unordered_map&gt;
#include &lt;string&gt;

class SymbolTable {
private:
    std::unordered_map&lt;std::string, SymbolEntry&gt; table;

public:
    void insert(const std::string&amp; name, const SymbolEntry&amp; entry) {
        table[name] = entry;
    }

    SymbolEntry lookup(const std::string&amp; name) {
        return table[name];
    }

    void remove(const std::string&amp; name) {
        table.erase(name);
    }
};
</code></pre>
<p>This symbol table implementation allows us to determine which variables are declared within the current scope. One problem, however, is that typically within programs you're able to access variables in parent scopes, which this implementation doesn't account for. To handle this, we can use different methods.</p>
<p>We'll cover scope stacks and nested symbol tables, but there are methods. Here are some of the most common ones:</p>
<ul>
<li>Dynamic Scoping</li>
<li>Scope Chains</li>
</ul>
<h3 id="scope-stack"><a class="header" href="#scope-stack">Scope Stack</a></h3>
<p>A scope stack is a stack of symbol tables owned by the compiler. It contains all the symbol tables for the scopes \(S\) that the compiler has encountered so far. When the compiler enters a new scope, it pushes a new symbol table onto the stack. When it exits the scope, it pops the symbol table off the stack.</p>
<p>Let's see how scope stacks work when analyzing the following Rust code:</p>
<pre><pre class="playground"><code class="language-rust">fn main() {
    let x = 10;
    {
        let y = 20;
        println!("{}", x + y);
    }
}</code></pre></pre>
<p>Here's how the scope stack would look like:</p>
<div class="table-wrapper"><table><thead><tr><th>Code</th><th>Scope Stack</th><th></th></tr></thead><tbody>
<tr><td><code>fn main()</code></td><td>\([S_{\text{global}}]\)</td><td>The global scope is pushed onto the stack.</td></tr>
<tr><td><code>{</code></td><td>\([S_{\text{global}}, S_{\text{main}}]\)</td><td>The <code>main</code> scope is pushed onto the stack.</td></tr>
<tr><td><code>let x = 10;</code></td><td>\([S_{\text{global}}, S_{\text{main}}]\)</td><td><code>x</code> is added to the <code>main</code> scope.</td></tr>
<tr><td><code>{</code></td><td>\([S_{\text{global}}, S_{\text{main}}, S_{\text{block}}]\)</td><td>A new block scope is pushed onto the stack.</td></tr>
<tr><td><code>let y = 20;</code></td><td>\([S_{\text{global}}, S_{\text{main}}, S_{\text{block}}]\)</td><td><code>y</code> is added to the block scope.</td></tr>
<tr><td><code>println!("{}", x + y);</code></td><td>\([S_{\text{global}}, S_{\text{main}}, S_{\text{block}}]\)</td><td><code>x</code> and <code>y</code> are looked up in the block scope.</td></tr>
<tr><td><code>}</code></td><td>\([S_{\text{global}}, S_{\text{main}}]\)</td><td>The block scope is popped off the stack.</td></tr>
<tr><td><code>}</code></td><td>\([S_{\text{global}}]\)</td><td>The <code>main</code> scope is popped off the stack.</td></tr>
</tbody></table>
</div>
<p>The line of code <code>println!("{}", x + y);</code> involves <code>x</code> and <code>y</code>. Let's evaluate:</p>
<ol>
<li>We look up <code>x</code> in the current scope \(S_{\text{block}}\)
<ol>
<li><code>x</code> is not found in \(S_{\text{block}}\). Let's look in the parent scope \(S_{\text{main}}\).</li>
<li><code>x</code> is found in \(S_{\text{main}}\). We can proceed.</li>
</ol>
</li>
<li>We look up <code>y</code> in the current scope \(S_{\text{block}}\)
<ol>
<li><code>y</code> is found in \(S_{\text{block}}\). We can proceed.</li>
</ol>
</li>
</ol>
<p>Both <code>x</code> and <code>y</code> are found, so we can evaluate the expression.</p>
<h3 id="nested-symbol-tables-tree-structure"><a class="header" href="#nested-symbol-tables-tree-structure">Nested Symbol Tables (tree structure)</a></h3>
<p>Another approach is to use nested symbol tables, where each symbol table corresponds to a specific scope. When the compiler enters a new scope, it creates a new symbol table and links it to the parent scope's symbol table. This way, the compiler can traverse the symbol tables hierarchically to resolve identifiers.</p>
<p>Let's see how a nested symbol table may operate in the same Rust code:</p>
<pre><pre class="playground"><code class="language-rust">fn main() {
    let x = 10;
    {
        let y = 20;
        println!("{}", x + y);
    }
}</code></pre></pre>
<div class="table-wrapper"><table><thead><tr><th>Code</th><th>Symbol Table Tree</th><th></th></tr></thead><tbody>
<tr><td><code>fn main()</code></td><td>\(S_{\text{global}}\)</td><td>Create global scope</td></tr>
<tr><td><code>{</code></td><td>\(S_{\text{global}} \\ \quad S_{\text{main}} \)</td><td>Create main scope</td></tr>
<tr><td><code>let x = 10;</code></td><td>\(S_{\text{global}} \\ \quad S_{\text{main}} \)</td><td>Add <code>x</code> to main scope</td></tr>
<tr><td><code>{</code></td><td>\(S_{\text{global}} \\ \quad S_{\text{main}} \\ \quad \quad S_{\text{block}} \)</td><td>Create block scope</td></tr>
<tr><td><code>let y = 20;</code></td><td>\(S_{\text{global}} \\ \quad S_{\text{main}} \\ \quad \quad S_{\text{block}} \)</td><td>Add <code>y</code> to block scope</td></tr>
<tr><td><code>println!("{}", x + y);</code></td><td>\(S_{\text{global}} \\ \quad S_{\text{main}} \\ \quad \quad S_{\text{block}} \)</td><td>Look up <code>x</code> and <code>y</code> in block scope</td></tr>
<tr><td><code>}</code></td><td>\(S_{\text{global}} \\ \quad S_{\text{main}} \)</td><td>Remove block scope</td></tr>
<tr><td><code>}</code></td><td>\(S_{\text{global}}\)</td><td>Remove main scope</td></tr>
</tbody></table>
</div>
<p>Let's follow the steps to resolve <code>x</code> and <code>y</code>:</p>
<ol>
<li>Look up <code>x</code> in the block scope
<ol>
<li><code>x</code> is not found in the block scope. Look up in the parent scope (main scope).</li>
<li><code>x</code> is found in the main scope. Proceed.</li>
</ol>
</li>
<li>Look up <code>y</code> in the block scope
<ol>
<li><code>y</code> is found in the block scope. Proceed.</li>
</ol>
</li>
</ol>
<p>It is very similar to the scope stack approach, but the symbol tables are organized in a tree structure.</p>
<h3 id="scope-shadowing"><a class="header" href="#scope-shadowing">Scope Shadowing</a></h3>
<p>Shadowing is a common practice in programming languages where a variable in an inner scope has the same name as a variable in an outer scope. The variable in the inner scope "shadows" the variable in the outer scope, meaning the inner variable takes precedence. In Rust, it looks like this:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let x: i32 = 10;
{
    let x: i32 = 20;
    println!("{}", x); // Prints 20
}
println!("{}", x); // Prints 10
<span class="boring">}</span></code></pre></pre>
<p>The inner <code>x</code> shadows the outer <code>x</code> within the block scope. This is simple to implement with symbol tables by adding a new entry for the shadowed variable in the inner scope.</p>
<p>Variable shadowing is a way to <em>re-use</em> a variable, overwriting the previous value, not to be confused with scope shadowing, where a variable in an inner scope hides a variable in an outer scope.</p>
<h2 id="symbol-attributes-and-metadata"><a class="header" href="#symbol-attributes-and-metadata">Symbol Attributes and Metadata</a></h2>
<p>Within a symbol entry, you can store additional information about the symbol. This varies depending on what the symbol is, but common attributes include:</p>
<ul>
<li><strong>Type</strong>: The data type of the symbol (e.g., <code>int</code>, <code>float</code>, <code>string</code>)</li>
<li><strong>Scope</strong>: The scope in which the symbol is declared
<ul>
<li>May not be needed if using scope stacks or nested symbol tables</li>
</ul>
</li>
<li><strong>Span</strong>: The location of the symbol in the source code (e.g., line number, column number)</li>
<li><strong>Modifiers</strong>: Additional information like <code>const</code>, <code>static</code>, <code>public</code>, <code>private</code>
<ul>
<li>This can help enforce language-specific rules and optimizations</li>
</ul>
</li>
</ul>
<p>Using these attributes will help you perform more advanced semantic analysis, such as type checking, scope resolution, and error reporting.</p>
<h1 id="resources-16"><a class="header" href="#resources-16">Resources</a></h1>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Symbol_table">Symbol Table - Wikipedia</a></li>
<li><a href="https://www.geeksforgeeks.org/symbol-table-compiler/">Compiler Design: Symbol Table - GeeksforGeeks</a></li>
<li><a href="https://home.adelphi.edu/~siegfried/cs372/372l3.pdf">Symbol Table Lecture Slides</a> at Adelphi University</li>
<li><a href="https://www.tutorialspoint.com/compiler_design/compiler_design_symbol_table.htm">Symbol Table - Tutorialspoint</a></li>
<li><a href="https://en.wikipedia.org/wiki/Persistent_array">Scope Stacks - Persistant Array</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="type-checking"><a class="header" href="#type-checking">Type Checking</a></h1>
<p>Type Checking is a vital part of the compilation process; it ensures that all types are matching and that the program is semantically correct.</p>
<p>Some languages, like Python, are dynamically typed, meaning that the type of a variable is determined at runtime. This allows for more flexibility but can lead to runtime errors if the types don't match. On the other hand, statically typed languages, like C++, require the type of a variable to be known at compile time. This allows for type checking to be done at compile time, catching errors before the program is run.</p>
<p>We'll be using statically typed languages as our example, but the concepts can be applied to dynamically typed languages as well.</p>
<h2 id="type-systems"><a class="header" href="#type-systems">Type Systems</a></h2>
<p>There are different types of type systems, each with its own set of rules. It's important you know which type system your language uses, as it will affect how you implement type checking.</p>
<h3 id="strong-vs-weak-typing"><a class="header" href="#strong-vs-weak-typing">Strong vs Weak typing</a></h3>
<p>A language is strongly typed if it enforces strict type checking, meaning that the type of a variable is checked before it's used. Weakly typed languages, on the other hand, allow for more flexibility, often implicitly converting types.</p>
<p>Here's an example of some code which would be valid in a weakly typed language but not in a strongly typed language:</p>
<pre><code class="language-c">int x = 10;
char y = 'a';
int z = x + y; // Error in a strongly typed language
</code></pre>
<p>Here we attempt to apply the addition operation between two operands of different types. We cannot add a <code>char</code> to an <code>int</code> in a strongly typed language, but weakly typed languages (such as Java) apply this conversion implicitly.</p>
<h3 id="explicit-vs-implicit-typing"><a class="header" href="#explicit-vs-implicit-typing">Explicit vs Implicit typing</a></h3>
<p>Languages can also be classified based on how they handle type declarations. In languages with explicit typing, the type of a variable must be explicitly declared. In contrast, languages with implicit typing infer the type of a variable based on its value.</p>
<p>Not all languages strictly fall under these two categories; some may have a mix of both. Rust, for example, is a statically typed language with type inference.</p>
<h3 id="type-compatibility-and-coercion"><a class="header" href="#type-compatibility-and-coercion">Type Compatibility and Coercion</a></h3>
<p>Type compatibility refers to whether two types can be used interchangeably. For example, in C, an <code>int</code> can be implicitly converted to a <code>float</code>, but not the other way around. This is known as type coercion.</p>
<div class="warning">
<strong>Warning</strong>: 
Type coercion can lead to unexpected behavior and bugs, so it's essential to understand how your language handles type compatibility.
</div>
<h2 id="type-checking-algorithms"><a class="header" href="#type-checking-algorithms">Type Checking Algorithms</a></h2>
<h3 id="hindley-milner-algorithm"><a class="header" href="#hindley-milner-algorithm">Hindley-Milner Algorithm</a></h3>
<p>The Hindley-Milner algorithm is a type inference algorithm used in functional programming languages, such as Haskell. It's based on the lambda calculus and is used to infer the most general type of an expression. Our language is not functional, so we won't be covering this algorithm in detail.</p>
<p>This algorithm is mainly able to infer the types of expressions without the need for explicit type annotations (often with no explicit type annotations at all!).</p>
<h1 id="resources-17"><a class="header" href="#resources-17">Resources</a></h1>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Type_checking">Type Checking</a></li>
<li><a href="https://raw.githubusercontent.com/mgrabmueller/AlgorithmW/master/pdf/AlgorithmW.pdf">Algorithm W</a> by Martin Grabmüller</li>
<li><a href="https://pfudke.wordpress.com/2014/11/20/hindley-milner-type-inference-a-practical-example-2/">Hindley-Milner System</a></li>
<li><a href="https://okmij.org/ftp/ML/generalization.html">OCaml's type checker</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="implementing-a-symbol-table"><a class="header" href="#implementing-a-symbol-table">Implementing a Symbol Table</a></h1>
<p>Here, we'll implement a Symbol Table for our language.</p>
<h2 id="symbol-entries"><a class="header" href="#symbol-entries">Symbol Entries</a></h2>
<p>First, let's determine what exactly the entries will be in our symbol table. Since we'll implement type-checking and such, we'll need to store more than just the name of the symbol. Here's a list of what we'll store in each entry:</p>
<ul>
<li><code>name</code>: The name of the symbol.
<ul>
<li>This is stored in the symbol table anyways</li>
</ul>
</li>
<li><code>span</code>: The span of the symbol in the source code.</li>
<li><code>type</code>: The type of the symbol.</li>
</ul>
<p>However, we do not want to allow people to be able to call variables as if they were a variable. Instead, we'll introduce <strong>two</strong> hashtables, as well as two symbol entry types: one for variables, one for functions.</p>
<p>A Variable Symbol would contain the type of the variable, as well as the span (as usual). Let's quickly define <code>VarSymbol</code>:</p>
<p>In <code>src/semantic_analysis/symbol_table.rs</code>:</p>
<pre><code class="language-rust ignore">use crate::ast::*;
use crate::token::Span;

/// Represents a variable symbol
#[derive(Debug)]
pub struct VarSymbol {
    /// The type of the variable
    pub ty: Type,
    /// Full span
    pub span: Span,
}</code></pre>
<p>Function symbols are a little more complex. We'll need to store the return type and span, as well as the parameters. Let's define <code>FuncSymbol</code>:</p>
<pre><code class="language-rust ignore">use crate::ast::*;
use crate::token::Span;

/// Represents a function symbol
#[derive(Debug)]
pub struct FuncSymbol {
    /// The return type of the function
    pub return_ty: Type,
    /// The parameters of the function
    pub params: Vec&lt;(String, Type)&gt;,
    /// Full span
    pub span: Span,
}</code></pre>
<p>There is a little problem; the <code>span</code> field would represent the entire function declaration (including the block!). We would rather have different spans for each part of the function. For now, let's add 2 more for the identifier, and the signature (ident, params, types):</p>
<pre><code class="language-rust ignore">use crate::ast::*;
use crate::token::Span;

/// Represents a function symbol
#[derive(Debug)]
pub struct FuncSymbol {
    /// Parameter types
    pub params: Vec&lt;Type&gt;,
    /// Return type
    pub ret_ty: Type,
    /// Full
    pub span: Span,
    /// Span of the identifier
    pub ident_span: Span,
    /// Signature span
    pub sig_span: Span,
}</code></pre>
<h2 id="symbol-table"><a class="header" href="#symbol-table">Symbol Table</a></h2>
<p>Now that we have the symbol entries, we can store the symbols in a symbol table. We'll use two hashmaps: one for variables, one for functions. We'll also use <strong>nested scopes</strong>, so we'll keep a reference (on the heap) to the parent scope, if any.</p>
<p>Let's define the <code>SymbolTable</code> struct:</p>
<pre><code class="language-rust ignore">use crate::ast::*;
use crate::token::Span;
use std::collections::HashMap;

/// Represents a symbol table
#[derive(Debug)]
pub struct SymbolTable&lt;'a&gt; {
    /// Table for variables
    pub variables: HashMap&lt;Ident, VarSymbol&gt;,
    /// Table for functions
    pub functions: HashMap&lt;Ident, FuncSymbol&gt;,
    /// Parent
    pub parent: Option&lt;Box&lt;&amp;'a SymbolTable&lt;'a&gt;&gt;&gt;,
}</code></pre>
<p>Great! Let's implement some methods for the <code>struct</code> now. We'll start with methods to add and get symbols.</p>
<h3 id="adding-and-retrieving-symbols"><a class="header" href="#adding-and-retrieving-symbols">Adding and Retrieving Symbols</a></h3>
<p>Before we do this, let's prime on the process of adding a variable symbol to the table:</p>
<ol>
<li>Retrieve a reference to the variable declaration AST node</li>
<li>Check if the variable is already declared in the current scope
<ul>
<li>If it is, return an error and terminate</li>
</ul>
</li>
<li>Add the variable to the table
<ol>
<li>Create a variable symbol from the variable declaration</li>
<li>Insert the symbol into the table</li>
</ol>
</li>
</ol>
<p>For variable retrival, the process is less straightforward:</p>
<ol>
<li>Retrieve the identifier of the variable</li>
<li>Search for the variable in the current scope
<ul>
<li>If it is found, return the symbol and terminate</li>
</ul>
</li>
<li>If the variable is not found, search in the parent scope
<ul>
<li>If it is found, return the symbol and terminate</li>
</ul>
</li>
<li>If the variable is not found in the current or parent scope, return <strong>nothing</strong></li>
</ol>
<p>We want to avoid returning an error if the variable isn't found; there may be contexts where it's not required to have a variable declared.</p>
<pre><code class="language-rust ignore">use anyhow::{anyhow, Result};
use crate::errors::SemanticError;

impl&lt;'a&gt; SymbolTable&lt;'a&gt; {
    /// Inserts a variable symbol into the table
    pub fn add_var(&amp;mut self, var: &amp;VariableDecl) -&gt; Result&lt;()&gt; {
        if let Some(existing) = self.variables.get(&amp;var.ident) {
            return Err(anyhow!(SemanticError::VariableAlreadyDeclared(
                var.ident.clone(),
                var.span.clone(),
                existing.span.clone()
            )));
        } else {
            self.variables.insert(
                var.ident.clone(),
                VarSymbol {
                    ty: var.ty.clone(),
                    span: var.span.clone(),
                },
            );
        }

        Ok(())
    }

    /// Looks up a variable symbol in the table
    pub fn get_var(&amp;self, name: &amp;Ident) -&gt; Option&lt;&amp;VarSymbol&gt; {
        match self.variables.get(name) {
            Some(v) =&gt; Some(v),
            None =&gt; match &amp;self.parent {
                Some(p) =&gt; p.get_var(name),
                None =&gt; None,
            },
        }
    }
}</code></pre>
<blockquote>
<p>Make sure you make implementations for <code>.span()</code> for your AST nodes!</p>
</blockquote>
<h3 id="adding-and-retrieving-functions"><a class="header" href="#adding-and-retrieving-functions">Adding and Retrieving Functions</a></h3>
<p>Functions follow an identical to variables, although you may want to handle overloading functions here. I don't like that concept, so we won't be implementing that here.</p>
<pre><code class="language-rust ignore">impl&lt;'a&gt; SymbolTable&lt;'a&gt; {
    /// Inserts a function symbol into the table
    pub fn add_fn(&amp;mut self, func: &amp;FunctionDecl) -&gt; Result&lt;()&gt; {
        let params = func.parameters.iter().map(|p| p.ty.clone()).collect();
        let ret_ty = func.ty.clone();

        if let Some(existing) = self.functions.get(&amp;func.ident) {
            warn!("Function already declared: {}", func.ident.ident);
            return Err(anyhow!(SemanticError::FunctionAlreadyDeclared(
                func.ident.clone(),
                func.span.clone(),
                existing.span.clone()
            )));
        } else {
            debug!("Adding function: {}", func.ident.ident);
            self.functions.insert(
                func.ident.clone(),
                FuncSymbol {
                    params,
                    ret_ty,
                    span: func.span.clone(),
                    ident_span: func.ident.span.clone(),
                    sig_span: Span::combine(&amp;func.ident.span, &amp;func.ty.span()),
                },
            );
        }

        Ok(())
    }

    /// Looks up a function symbol in the table
    pub fn get_fn(&amp;self, name: &amp;Ident) -&gt; Option&lt;&amp;FuncSymbol&gt; {
        match self.functions.get(name) {
            Some(f) =&gt; Some(f),
            None =&gt; match &amp;self.parent {
                Some(p) =&gt; p.get_fn(name),
                None =&gt; None,
            },
        }
    }
}</code></pre>
<h2 id="conclusion-1"><a class="header" href="#conclusion-1">Conclusion</a></h2>
<p>Now we have a symbol table to represent each scope, and the symbols within the scopes. We can now use this symbol table to implement type-checking and other semantic analysis.</p>
<h1 id="resources-18"><a class="header" href="#resources-18">Resources</a></h1>
<ul>
<li><a href="https://doc.rust-lang.org/std/boxed/struct.Box.html">Box</a></li>
<li><a href="https://web.mit.edu/rust-lang_v1.25/arch/amd64_ubuntu1404/share/doc/rust/html/book/first-edition/the-stack-and-the-heap.html">Memory Stack and Heap</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="implementing-a-semantic-analyzer"><a class="header" href="#implementing-a-semantic-analyzer">Implementing a Semantic Analyzer</a></h1>
<p>Now that we have a Symbol Table struct, we'll be able to implement a semantic analyzer. This process is pretty difficult and can be challenging to debug, but try not to get discouraged! It's a very rewarding process.</p>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>Our parser was quite limited in retrospect; it would terminate and fail to perform error recovery. This is intended, as it's much more simple to implement a parser that fails on the first error. However, this is not the case for a semantic analyzer. We want to collect as many errors as possible and report them all at once.</p>
<p>Let's implement some sort of analysis algorithm which will aggregate all errors found in the program. We'll start by creating a new module <code>analysis</code> in <code>src/semantic_analysis/</code> if you haven't already.</p>
<h2 id="analysis-trait"><a class="header" href="#analysis-trait"><code>Analysis</code> trait</a></h2>
<p>Again, instead of diving in, let's outline exactly how we want the process to look like. We could have functions (e.g. <code>analyze_fn</code>, <code>analyze_block</code>, etc.) that take in the Symbol Table and the AST node to analyze. This way, we can easily traverse the AST and check for errors.</p>
<p>In order to make the code cleaner, however, it would be more idiomatic to introduce a new <strong>trait</strong> that would allow us to call <code>analyze</code> on any AST node. This way, we can easily call <code>analyze</code> on any node and have the node analyze itself.</p>
<p>Let's declare the trait in <code>src/semantic_analysis/traits.rs</code>:</p>
<pre><code class="language-rust ignore">use super::symbols::SymbolTable;

use anyhow::Error;

/// Trait for Semantic Analysis
///
/// When implementing, you are expected to analyse
/// every part of the node, aggregating and returning
/// *all* errors found.
pub trait Analysis {
    /// Analyze the node
    fn analyze(&amp;self, table: &amp;mut SymbolTable) -&gt; Vec&lt;Error&gt;;
}</code></pre>
<p>We take in the current symbol table to represent the current scope. We'll also return a vector of <code>Error</code>s, which will contain all errors found in the node.</p>
<h2 id="the-main-analysis-function"><a class="header" href="#the-main-analysis-function">The main analysis function</a></h2>
<p>Before we implement <code>Analysis</code> on each AST node, let's make an interface for us to perform analysis on an entire program.</p>
<p>Within this function, we'll first create a new <code>SymbolTable</code> to represent the global scope. We can then call <code>analyze</code> on the AST itself, passing in the global scope.</p>
<p>In <code>src/semantic_analysis/analysis.rs</code>:</p>
<pre><code class="language-rust ignore">use super::symbols::SymbolTable;
use super::traits::Analysis;
use crate::ast::*;
use crate::errors::SemanticError;
use crate::token::Span;
use anyhow::{anyhow, Error};

pub fn analyse&lt;'a&gt;(ast: &amp;'a AST) -&gt; Vec&lt;Error&gt; {
    let program = &amp;ast.program;

    let mut global_table = SymbolTable::new();
    let mut errors = Vec::new();

    // let AST analyse itself
    errors.extend(ast.analyze(&amp;mut global_table));

    errors
}</code></pre>
<p>There is a fundamental flaw in this design; it does not analyze the functions at all, so the symbol table initializes as empty. We can implement this either in the AST node, or we can do that here.</p>
<p>Since it is a little different than how our typical <code>impl Analaysis for ...</code> would look like, let's implement it here.</p>
<p>We'll recognise all functions by iterating through all <code>Item</code>s in the program, registering them within the symbol table.</p>
<p>The presence of the <code>main</code> function is also required, so we'll flag an error if it is not found. We also would like this to return <code>int</code>, which would represent the exit code of the program, akin to C++.</p>
<p>Let's implement this:</p>
<pre><code class="language-rust ignore">    // ...
    let mut errors = Vec::new();

    // recognise all functions
    let mut main_node: Option&lt;&amp;FunctionDecl&gt; = None;

    for item in &amp;program.items {
        match item {
            Item::FunctionDecl(f) =&gt; {
                if f.ident.ident == "main" {
                    main_node = Some(f);
                }

                if let Err(e) = global_table.add_fn(f) {
                    errors.push(e);
                }
            }
        }
    }

    if main_node.is_none() {
        errors.push(anyhow!(SemanticError::MissingMainFunction(Span::default())));
    } else {
        // ensure return type is int
        let ret_ty = main_node.unwrap().ty.clone();
        match ret_ty {
            Type::Primitive(ty) =&gt; {
                if ty.kind != PrimitiveKind::Int {
                    errors.push(anyhow!(SemanticError::MainMustReturnInt(ty.span.clone())));
                }
            },
        }
    }

    // let AST analyse itself
    errors.extend(ast.analyze(&amp;mut global_table));

    // ...</code></pre>
<h2 id="implementing-our-analysis-trait"><a class="header" href="#implementing-our-analysis-trait">Implementing our <code>Analysis</code> trait</a></h2>
<p>Here comes the difficult and long part, we picked this hell for ourselves.</p>
<h3 id="implementing-analysis-for-ast-and-item"><a class="header" href="#implementing-analysis-for-ast-and-item">Implementing <code>Analysis</code> for <code>AST</code> and <code>Item</code></a></h3>
<p>This is thankfully pretty simple, we just need to go through each item in <code>AST</code> and call <code>analyze</code> on them. For each item, we need to match what kind of item it is and call <code>analyze</code> on it.</p>
<pre><code class="language-rust ignore">impl Analysis for AST {
    fn analyze(&amp;self, table: &amp;mut SymbolTable) -&gt; Vec&lt;Error&gt; {
        let mut errors = Vec::new();

        for item in &amp;self.program.items {
            errors.extend(item.analyze(table));
        }

        errors
    }
}

impl Analysis for Item {
    fn analyze(&amp;self, table: &amp;mut SymbolTable) -&gt; Vec&lt;Error&gt; {
        match self {
            Item::FunctionDecl(f) =&gt; f.analyze(table),
        }
    }
}</code></pre>
<p>As tempting as it may be to implement <code>Analysis</code> for <code>FunctionDecl</code> next, let's continue going in order.</p>
<h2 id="implementing-analysis-for-block"><a class="header" href="#implementing-analysis-for-block">Implementing <code>Analysis</code> for <code>Block</code></a></h2>
<p>When analysing a <code>Block</code>, it will introduce a new <strong>scope</strong>. To make things simple, when we analyze a function it will introduce a scope for the parameters, and the block will introduce its own scope.</p>
<p>Within a block, we'd simply analyze each statement for errors.</p>
<pre><code class="language-rust ignore">impl Analysis for Block {
    fn analyze(&amp;self, table: &amp;mut SymbolTable) -&gt; Vec&lt;Error&gt; {
        let mut errors = Vec::new();
        let mut new_table = SymbolTable::child(table);

        for statement in &amp;self.statements {
            debug!("Analyzing statement: {statement:?}");
            errors.extend(statement.analyze(&amp;mut new_table));
        }

        errors
    }
}</code></pre>
<p>This analyzes each statement within the block. There's also two other problems we would like to solve:
12. If the block is a function body, we need a way to
1. See if the return for the block is guaranteed or not, taking into account all possible paths
2. Get all the types for these return expressions
2. A block may have <strong>dead code</strong> (unreachable code), and <strong>unused variables</strong></p>
<p>We'll ignore determining whether conditions are guaranteed or not.</p>
<blockquote>
<p><strong>Performance Warning</strong></p>
<p>This is a very naive implementation, and will be very slow. Try to introduce caching and memoization to speed up the process.</p>
</blockquote>
<p>We'll solve the first problem by introducing a method; <code>get_return_stmts</code>. This will check if the block is guaranteed to return, and get all the return types.</p>
<p>The process will follow:</p>
<ol>
<li>Iterate through all the statements. If the statement is:
<ul>
<li><strong>Return</strong>
<ol>
<li>Mark as guaranteed return, and early return</li>
<li>Add expression type</li>
</ol>
</li>
<li><strong>Variable Declaration</strong>
<ol>
<li>Add variable to the symbol table (may be referenced later)</li>
</ol>
</li>
<li><strong>Flow statement</strong> (<code>if</code>)
<ol>
<li>Get the return statement expressions, and whether the block is independently guaranteed to return</li>
<li>Add expression types</li>
<li><code>guaranteed_return *= if_block_guaranteed_return</code> (if block is guaranteed to return, then the entire block is guaranteed to return)</li>
<li>If there is an <code>else</code> block:
<ol>
<li>Do the same process as above</li>
<li>If both blocks are guaranteed, then the entire block is guaranteed. Set the early return flag to true.</li>
</ol>
</li>
</ol>
</li>
</ul>
</li>
<li>Return the statement types, and whether the block is guaranteed to return</li>
</ol>
<p>Let's implement this helper method:</p>
<pre><code class="language-rust ignore">impl Block {
    pub fn get_return_stmts(&amp;self, table: &amp;mut SymbolTable) -&gt; (Vec&lt;Type&gt;, bool) {
        let mut return_stmts_types = Vec::new();
        let mut guaranteed_return = true;
        let mut tmp_my_table = SymbolTable::child(table);
        let mut early_return = false;

        for statement in &amp;self.statements {
            match statement {
                Statement::Return(stmt) =&gt; {
                    if let Some(expr) = stmt {
                        if let Ok(ty) = expr.get_type(&amp;tmp_my_table) {
                            return_stmts_types.push(ty.clone());
                        }
                    }
                    guaranteed_return = true;
                    early_return = true;
                    break;
                }
                Statement::VariableDecl(v) =&gt; {
                    // recover error, add your own debugging stuff
                    if let Err(e) = tmp_my_table.add_var(v) {}
                }
                Statement::Flow(flow) =&gt; {
                    let (returns, if_guaranteed) = flow.if_block.get_return_stmts(&amp;mut tmp_my_table);

                    return_stmts_types.extend(returns);
                    guaranteed_return &amp;= if_guaranteed;

                    if let Some(else_block) = &amp;flow.else_block {
                        let (returns, else_guaranteed) = else_block.get_return_stmts(&amp;mut tmp_my_table);

                        return_stmts_types.extend(returns);
                        guaranteed_return &amp;= else_guaranteed;

                        // in the case where both blocks have a return statement
                        // we can guarantee a return
                        if if_guaranteed &amp;&amp; else_guaranteed {
                            early_return = true;
                            break;
                        }
                    }
                }
                _ =&gt; {}
            }
        }

        (return_stmts_types, guaranteed_return &amp;&amp; early_return)
    }
}</code></pre>
<p>Next, we need to introduce a method to check for dead code and unused variables: <code>check_dead_unreachable</code>.</p>
<p>Here, we'll want to check for dead and unreachable code, aggregating the errors and warnings we'll find. We would also like to check for which variables are declared.</p>
<p>The process is as follows:</p>
<ol>
<li>Set the <code>early_return</code> flag to <code>false</code>. We need this to check for unreachable code.</li>
<li>Iterate through all the statements. If the statement is:
<ul>
<li><strong>Return</strong>
<ol>
<li>If <code>early_return</code> is true, then skip the statement. It will never execute.</li>
<li>Check whether it's an early return</li>
<li>Set used variables in expression as used</li>
</ol>
</li>
<li><strong>Variable Declaration</strong>
<ol>
<li>If <code>early_return</code> is true, then skip the statement. It will never execute.</li>
<li>Add the variable to the symbol table. Ignore if error.</li>
<li>Set used variables in expression as used</li>
</ol>
</li>
<li><strong>Flow statement</strong> (<code>if</code>)
<ol>
<li>If <code>early_return</code> is true, then skip the statement. It will never execute.</li>
<li>Get whether the block is guaranteed to return</li>
<li>Set used variables in block as used</li>
<li>If there's an <code>else</code> block:
<ol>
<li>Get whether the block is guaranteed to return</li>
<li>Set used variables in block as used</li>
<li>If both blocks are guaranteed to return, then set <code>early_return</code> to true</li>
</ol>
</li>
</ol>
</li>
</ul>
</li>
<li>If there's an early return:
<ol>
<li>Raise a warning for the section of dead code following the return statement</li>
</ol>
</li>
<li>For each undeclared variable, raise an error</li>
<li>Return all errors/warnings, and the variables declared</li>
</ol>
<p>Let's implement this helper method:</p>
<pre><code class="language-rust ignore">impl Block {
    fn check_dead_unreachable(&amp;self, table: &amp;SymbolTable) -&gt; (Vec&lt;Error&gt;, HashMap&lt;Ident, bool&gt;) {
        let mut errors = Vec::new();
        let mut tmp_my_table = SymbolTable::child(table);

        // map of variables declared in this scope, and whether they are used
        let mut declared_vars: HashMap&lt;Ident, bool&gt; = HashMap::new();

        let mut early_return = false;
        for (cur_idx, statement) in self.statements.iter().enumerate() {
            match statement {
                Statement::Return(expr) =&gt; {
                    if early_return {
                        continue;
                    }

                    early_return = cur_idx + 1 != self.statements.len();
                    let idents_used = expr.as_ref().map(|e| e.idents_used()).unwrap_or_default();

                    for ident in &amp;idents_used {
                        if let Some(declared) = declared_vars.get_mut(&amp;ident) {
                            *declared = true;
                        }
                    }
                }
                Statement::VariableDecl(v) =&gt; {
                    if early_return {
                        continue;
                    }

                    if let Err(e) = tmp_my_table.add_var(v) {}
                    declared_vars.insert(v.ident.clone(), false);

                    // expression may use the variable
                    for ident in &amp;v.expression.idents_used() {
                        if let Some(declared) = declared_vars.get_mut(&amp;ident) {
                            *declared = true;
                        }
                    }
                }
                Statement::Flow(flow) =&gt; {
                    if early_return {
                        continue;
                    }

                    let (_, if_guaranteed) = flow.if_block.get_return_stmts(&amp;mut tmp_my_table);

                    // block may use a variable in this scope
                    for (ident, used) in &amp;flow.if_block.check_dead_unreachable(&amp;tmp_my_table).1 {
                        if let Some(declared) = declared_vars.get_mut(&amp;ident) {
                            *declared |= *used;
                        }
                    }

                    if let Some(else_block) = &amp;flow.else_block {
                        let (_, else_guaranteed) = else_block.get_return_stmts(&amp;mut tmp_my_table);
                        for (ident, used) in &amp;else_block.check_dead_unreachable(&amp;tmp_my_table).1 {
                            if let Some(declared) = declared_vars.get_mut(&amp;ident) {
                                *declared |= *used;
                            }
                        }

                        // in the case where both blocks have a return statement
                        // we can guarantee a return
                        if if_guaranteed &amp;&amp; else_guaranteed {
                            early_return = true;
                            break;
                        }
                    }
                }
                _ =&gt; {}
            }
        }

        if early_return {
            // get span of unreachable code
            let span = Span::combine(
                &amp;self
                    .statements
                    .first()
                    .map(|s| s.span())
                    .unwrap_or(self.span.clone()),
                &amp;self
                    .statements
                    .last()
                    .map(|s| s.span())
                    .unwrap_or(self.span.clone()),
            );

            errors.push(anyhow!(Warning::UnreachableCode(span)));
        };

        for (ident, declared) in &amp;declared_vars {
            if !declared &amp;&amp; !ident.ident.starts_with("_") {
                errors.push(anyhow!(Warning::UnusedVariable(
                    ident.clone(),
                    ident.span.clone()
                )));
            }
        }

        (errors, declared_vars)
    }
}</code></pre>
<p>Be sure to add this to the <code>Analysis</code> implementation:</p>
<pre><code class="language-rust ignore">impl Analysis for Block {
    fn analyze(&amp;self, table: &amp;mut SymbolTable) -&gt; Vec&lt;Error&gt; {
        debug!("Analyzing block: {self:?}, table: {table:?}");
        let mut errors = Vec::new();
        let mut new_table = SymbolTable::child(table);

        for statement in &amp;self.statements {
            debug!("Analyzing statement: {statement:?}");
            errors.extend(statement.analyze(&amp;mut new_table));
        }

        errors.extend(self.check_dead_unreachable(&amp;table).0);

        debug!("Block analysis complete, errors: {errors:?}");

        errors
    }
}</code></pre>
<p>We do <strong>not</strong> check for guaranteed return here, since we don't have enough context of the block (it may be a flow block, not a function body). The function body will handle this.</p>
<h2 id="implementing-analysis-for-expression"><a class="header" href="#implementing-analysis-for-expression">Implementing <code>Analysis</code> for <code>Expression</code></a></h2>
<p>Since this is an <code>enum</code>, we can call <code>analyze</code> on each variant.</p>
<pre><code class="language-rust ignore">impl Analysis for Expression {
    fn analyze(&amp;self, table: &amp;mut SymbolTable) -&gt; Vec&lt;Error&gt; {
        match self {
            Expression::Primary(p) =&gt; p.analyze(table),
            Expression::Unary(u) =&gt; u.analyze(table),
            Expression::Binary(b) =&gt; b.analyze(table),
        }
    }
}</code></pre>
<p>However, we have some methods we may need to use.</p>
<p>We called <code>get_type</code> to evaluate what the expression may be. We also called <code>idents_used</code> to get all the identifiers used in the expression. We'll need to implement these methods.</p>
<pre><code class="language-rust ignore">impl Expression {
    pub fn get_type(&amp;self, table: &amp;SymbolTable) -&gt; Result&lt;Type&gt; {
        match self {
            Expression::Primary(p) =&gt; p.get_type(table),
            Expression::Unary(u) =&gt; u.get_type(table),
            Expression::Binary(b) =&gt; b.get_type(table),
        }
    }

    pub fn idents_used(&amp;self) -&gt; Vec&lt;Ident&gt; {
        match self {
            Expression::Primary(p) =&gt; p.idents_used(),
            Expression::Unary(u) =&gt; u.idents_used(),
            Expression::Binary(b) =&gt; b.idents_used(),
        }
    }
}</code></pre>
<h2 id="implementing-analysis-for-statement"><a class="header" href="#implementing-analysis-for-statement">Implementing <code>Analysis</code> for <code>Statement</code></a></h2>
<p>This is similar to <code>Expression</code>, we'll call <code>analyze</code> on each variant. The return statements variant contains a reference to an expression, so we'll need to call <code>analyze</code> on that.</p>
<pre><code class="language-rust ignore">impl Analysis for Statement {
    fn analyze(&amp;self, table: &amp;mut SymbolTable) -&gt; Vec&lt;Error&gt; {
        match self {
            Statement::Expression(e) =&gt; e.analyze(table),
            Statement::VariableDecl(v) =&gt; v.analyze(table),
            Statement::Flow(f) =&gt; f.analyze(table),
            Statement::Return(e) =&gt; e.as_ref().map_or_else(|| vec![], |e| e.analyze(table)),
        }
    }
}</code></pre>
<h2 id="implementing-analysis-for-variabledecl"><a class="header" href="#implementing-analysis-for-variabledecl">Implementing <code>Analysis</code> for <code>VariableDecl</code></a></h2>
<p>When analyzing a variable declaration, we'd get given the current scope. We would like to declare the current variable to the symbol table. We'd also like to check whether the expression is the same as the variable type.</p>
<p>Let's implement:</p>
<pre><code class="language-rust ignore">impl Analysis for VariableDecl {
    fn analyze(&amp;self, table: &amp;mut SymbolTable) -&gt; Vec&lt;Error&gt; {
        let mut errors = Vec::new();

        // add variable to symbol table
        match table.add_var(&amp;self) {
            Ok(_) =&gt; (),
            Err(e) =&gt; errors.push(e),
        };

        // check if expression type matches variable type
        match self.expression.get_type(table) {
            Ok(ty) =&gt; {
                if ty != self.ty {
                    errors.push(anyhow!(SemanticError::TypesDoNotMatch {
                        expected_type: self.ty.clone(),
                        expected_span: self.ty.span(),
                        found_type: ty,
                        found_span: self.expression.span(),
                    }));
                }
            }
            Err(e) =&gt; {
                errors.push(e);
                return errors;
            }
        };

        errors
    }
}</code></pre>
<h2 id="implementing-analysis-for-flowstatement"><a class="header" href="#implementing-analysis-for-flowstatement">Implementing <code>Analysis</code> for <code>FlowStatement</code></a></h2>
<p>To analyze the flow statement, we'd need to check a few things. The structure is a little complex, so let's look at a typical flow statement and identify what we need to determine:</p>
<pre><code class="language-rust ignore">if a == 1 {
    return 1;
} else {
    return 2;
}</code></pre>
<ol>
<li>We need to check the condition expression. This should be a boolean.</li>
<li>We need to check the if block. Pretty simple.</li>
<li>If there's an else block, we need to check that too.</li>
</ol>
<p>Let's implement this:</p>
<pre><code class="language-rust ignore">
impl Analysis for FlowStatement {
    fn analyze(&amp;self, table: &amp;mut SymbolTable) -&gt; Vec&lt;Error&gt; {
        let mut errors = Vec::new();

        // check if condition is a boolean
        match self.condition.get_type(table) {
            Ok(ty) =&gt; {
                let bool_type = Type::Primitive(PrimitiveType {
                    kind: PrimitiveKind::Bool,
                    span: Span::default(),
                });

                if ty != bool_type {
                    errors.push(anyhow!(SemanticError::NonBooleanCondition {
                        found_type: ty,
                        found_span: self.condition.span(),
                    }));
                }
            }
            Err(e) =&gt; {
                errors.push(e);
                return errors;
            }
        };

        errors.extend(self.if_block.analyze(table));

        if let Some(else_block) = &amp;self.else_block {
            errors.extend(else_block.analyze(table));
        }

        errors
    }
}</code></pre>
<h2 id="implementing-analysis-for-functiondecl"><a class="header" href="#implementing-analysis-for-functiondecl">Implementing <code>Analysis</code> for <code>FunctionDecl</code></a></h2>
<p>When analyzing a function declaration, we'd need to do a few things:</p>
<ol>
<li>Make sure all return statements match the function type</li>
<li>Make sure the function is guaranteed to return</li>
<li>Analyze each parameter</li>
<li>Analyze the block</li>
</ol>
<p>Let's implement the basic analysis function:</p>
<pre><code class="language-rust ignore">impl Analysis for FunctionDecl {
    fn analyze(&amp;self, table: &amp;mut SymbolTable) -&gt; Vec&lt;Error&gt; {
        let mut errors = Vec::new();

        errors.extend(self.analyze_return_stmt(table));

        let mut new_table = SymbolTable::child(table);
        let param_errors = self.analyze_parameters(&amp;mut new_table);
        errors.extend(param_errors);

        // analyze the block
        errors.extend(self.block.analyze(&amp;mut new_table));

        errors
    }
}</code></pre>
<p>We used a few functions we haven't made yet. Let's implement them.</p>
<p>The <code>analyze_return_stmt</code> function will simply analyze the first two criteria mentioned:</p>
<ol>
<li>Make sure all return statements match the function type</li>
<li>Make sure the function is guaranteed to return</li>
</ol>
<p>This would look at the block, and get the return statements. If there are no return statements, or the return statements do not match the function type, we'll raise an error.</p>
<p>Thankfully, we defined the <code>get_return_stmts</code> function in <code>Block</code> earlier; we can use that to our advantage.</p>
<pre><code class="language-rust ignore">impl FunctionDecl {
    fn analyze_return_stmt(&amp;self, table: &amp;mut SymbolTable) -&gt; Vec&lt;Error&gt; {
        let mut errors = Vec::new();

        // multiple return statements
        let (return_values, guaranteed_return) = self.block.get_return_stmts(table);

        if !guaranteed_return {
            if return_values.len() == 0 {
                errors.push(anyhow!(SemanticError::MissingReturnStatement(
                    self.span.clone()
                )));
            } else {
                errors.push(anyhow!(SemanticError::ReturnNotGuaranteed(
                    self.span.clone()
                )));
            }
        } else {
            for ty in return_values {
                if ty != self.ty {
                    errors.push(anyhow!(SemanticError::IncompatibleReturnType {
                        expected_type: self.ty.clone(),
                        expected_span: self.ty.span(),
                        found_type: ty.clone(),
                        found_span: ty.span(),
                    }));
                }
            }
        }

        errors
    }
}</code></pre>
<p>Next, we need to implement the <code>analyze_parameters</code> function.</p>
<p>We'll receive the current parameter block, and we'll add each parameter to the symbol table. There's no need to check for duplicate parameters, as the symbol table will handle that.</p>
<pre><code class="language-rust ignore">impl FunctionDecl {
    fn analyze_parameters(&amp;self, new_table: &amp;mut SymbolTable) -&gt; Vec&lt;Error&gt; {
        let mut errors = Vec::new();

        for param in &amp;self.parameters {
            if let Err(e) = new_table.add_param(param) {
                errors.push(e);
            }
        }
        errors
    }
}</code></pre>
<p>Very simple!</p>
<h2 id="implementing-analysis-for-unaryexpression"><a class="header" href="#implementing-analysis-for-unaryexpression">Implementing <code>Analysis</code> for <code>UnaryExpression</code></a></h2>
<p>Let's do this before Binary Expressions, since they involve a little extra work.</p>
<p>When analyzing a unary expression, we'd need to check a few things:</p>
<ol>
<li>The operator must be valid for the type (i.e. we can't call <code>!</code> on an integer)</li>
<li>The expression must be valid</li>
</ol>
<p>Let's quickly write the implementation for not (<code>!</code>). We would only like the expression to be a boolean.</p>
<pre><code class="language-rust ignore">impl Analysis for UnaryExpression {
    fn analyze(&amp;self, table: &amp;mut SymbolTable) -&gt; Vec&lt;Error&gt; {
        match &amp;self.kind {
            UnaryExpressionKind::Negation(_) =&gt; self.analyze_negation(table),
            UnaryExpressionKind::Not(_) =&gt; self.analyze_not(table),
        }
    }
}

impl UnaryExpression {
    fn analyze_not(&amp;self, table: &amp;SymbolTable) -&gt; Vec&lt;Error&gt; {
        let mut errors = Vec::new();

        let expr: &amp;Box&lt;Expression&gt; = match &amp;self.kind {
            UnaryExpressionKind::Not(e) =&gt; e,
            _ =&gt; unreachable!(),
        };

        // check if unsupported type
        let expr_type = match expr.get_type(table) {
            Ok(t) =&gt; t,
            Err(e) =&gt; {
                errors.push(e);
                return errors;
            }
        };

        match expr_type {
            Type::Primitive(ref prim_ty) =&gt; match prim_ty.kind {
                PrimitiveKind::Bool =&gt; (),
                _ =&gt; errors.push(anyhow!(SemanticError::UnsupportedUnaryOperation {
                    operator: "Not".to_string(),
                    operand_type: expr_type.clone(),
                    span: self.span.clone(),
                })),
            },
        }

        errors
    }
}</code></pre>
<p>Let's do the same for negation (<code>-</code>). We'd only want the expression to be an integer or float strictly. This follows the same code structure as the <code>!</code> operator.</p>
<pre><code class="language-rust ignore">impl UnaryExpression {
    fn analyze_negation(&amp;self, table: &amp;SymbolTable) -&gt; Vec&lt;Error&gt; {
        // ...

        match expr_type {
            Type::Primitive(ref prim_ty) =&gt; match prim_ty.kind {
                PrimitiveKind::Int | PrimitiveKind::Float =&gt; (),
                _ =&gt; errors.push(anyhow!(SemanticError::UnsupportedUnaryOperation {
                    operator: "Negation".to_string(),
                    operand_type: expr_type.clone(),
                    span: self.span.clone(),
                })),
            },
        }

        errors
    }
}</code></pre>
<p>Recall that we declared <code>get_type</code> and <code>idents_used</code> for the expression, which would propagate onto this struct. We'll pretty much redirect them back to the expression, which would call the same function on the inner expression.</p>
<pre><code class="language-rust ignore">impl UnaryExpression {
    pub fn get_type(&amp;self, table: &amp;SymbolTable) -&gt; Result&lt;Type&gt; {
        let expr = match &amp;self.kind {
            UnaryExpressionKind::Negation(e) =&gt; e,
            UnaryExpressionKind::Not(e) =&gt; e,
        };

        expr.get_type(table)
    }

    pub fn idents_used(&amp;self) -&gt; Vec&lt;Ident&gt; {
        match &amp;self.kind {
            UnaryExpressionKind::Negation(e) =&gt; e.idents_used(),
            UnaryExpressionKind::Not(e) =&gt; e.idents_used(),
        }
    }
}</code></pre>
<h2 id="implementing-analysis-for-binaryexpression"><a class="header" href="#implementing-analysis-for-binaryexpression">Implementing <code>Analysis</code> for <code>BinaryExpression</code></a></h2>
<p>A binary expression involves two operands. We'd need to check whether both the left hand size <code>lhs</code> and right hand size <code>lhs</code> are the same type (we do not want implicit type coercion here). We'd also like to make sure they're valid types to be added together.</p>
<p>Nothing, too extravagant, let's implement:</p>
<pre><code class="language-rust ignore">impl Analysis for BinaryExpression {
    fn analyze(&amp;self, table: &amp;mut SymbolTable) -&gt; Vec&lt;Error&gt; {
        // validate both sides are same type
        let mut errors = Vec::new();

        let lhs_type = match self.lhs.get_type(table) {
            Ok(t) =&gt; t,
            Err(e) =&gt; {
                errors.push(e);
                return errors;
            }
        };

        let rhs_type = match self.rhs.get_type(table) {
            Ok(t) =&gt; t,
            Err(e) =&gt; {
                errors.push(e);
                return errors;
            }
        };

        if lhs_type != rhs_type {
            errors.push(anyhow!(SemanticError::TypesDoNotMatch {
                expected_type: lhs_type.clone(),
                expected_span: self.lhs.span(),
                found_type: rhs_type.clone(),
                found_span: self.rhs.span(),
            }));
        };

        // check if either side is not a valid type
        if !self.is_valid_type(&amp;lhs_type) {
            errors.push(anyhow!(SemanticError::UnsupportedBinaryOperation {
                operator: self.op.kind.to_string(),
                operand_type: lhs_type,
                span: self.span.clone(),
            }));
        }

        if !self.is_valid_type(&amp;rhs_type) {
            errors.push(anyhow!(SemanticError::UnsupportedBinaryOperation {
                operator: self.op.kind.to_string(),
                operand_type: rhs_type,
                span: self.span.clone(),
            }));
        }

        errors
    }
}

impl BinaryExpression {
    fn is_valid_type(&amp;self, ty: &amp;Type) -&gt; bool {
        match ty {
            Type::Primitive(ref prim_ty) =&gt; match prim_ty.kind {
                PrimitiveKind::Int | PrimitiveKind::Float | PrimitiveKind::Bool =&gt; true,
            },
        }
    }
}</code></pre>
<p>Similar to unary expressions, we'll need to implement <code>get_type</code> and <code>idents_used</code> for binary expressions. We'll redirect these functions to the inner expressions.</p>
<pre><code class="language-rust ignore">impl BinaryExpression {
    pub fn get_type(&amp;self, table: &amp;SymbolTable) -&gt; Result&lt;Type&gt; {
        let lhs_type = self.lhs.get_type(table)?;
        let rhs_type = self.rhs.get_type(table)?;

        if lhs_type != rhs_type {
            return Err(anyhow!(SemanticError::TypesDoNotMatch {
                expected_type: lhs_type,
                expected_span: self.lhs.span(),
                found_type: rhs_type,
                found_span: self.rhs.span(),
            }));
        }

        // if comparison, return bool
        match self.op.kind {
            BinaryOperatorKind::Equal
            | BinaryOperatorKind::NotEqual
            | BinaryOperatorKind::LessThan
            | BinaryOperatorKind::LessThanOrEqual
            | BinaryOperatorKind::GreaterThan
            | BinaryOperatorKind::GreaterThanOrEqual =&gt; Ok(Type::Primitive(PrimitiveType {
                kind: PrimitiveKind::Bool,
                span: self.span.clone(),
            })),
            _ =&gt; Ok(lhs_type),
        }
    }

    pub fn idents_used(&amp;self) -&gt; Vec&lt;Ident&gt; {
        let mut idents = self.lhs.idents_used();
        idents.extend(self.rhs.idents_used());
        idents
    }
}</code></pre>
<h2 id="implementing-analysis-for-primaryexpression"><a class="header" href="#implementing-analysis-for-primaryexpression">Implementing <code>Analysis</code> for <code>PrimaryExpression</code></a></h2>
<p>Since this is an enum, we simply call <code>analyze</code> on each variant. However, the function call variant is a little more complex. We'll define and create <code>analyze_fn_call</code> for this.</p>
<pre><code class="language-rust ignore">impl Analysis for PrimaryExpression {
    fn analyze(&amp;self, _table: &amp;mut SymbolTable) -&gt; Vec&lt;Error&gt; {
        match self {
            PrimaryExpression::Literal(_) =&gt; vec![],
            PrimaryExpression::Ident(_) =&gt; vec![],
            PrimaryExpression::Parenthesized(p) =&gt; p.analyze(_table),
            PrimaryExpression::FunctionCall(_, _) =&gt; self.analyze_fn_call(_table),
        }
    }
}</code></pre>
<p>When analyzing a function call, we need to ensure that:</p>
<ol>
<li>The function is declared</li>
<li>The number of arguments passed match</li>
<li>The types of the arguments match the function parameters</li>
</ol>
<p>This is pretty much the process of the entire function, so we can just implement. We need to unwrap the primary expression, look for the function in the symbol table, and check the arguments.</p>
<pre><code class="language-rust ignore">impl PrimaryExpression {
    fn analyze_fn_call(&amp;self, table: &amp;mut SymbolTable) -&gt; Vec&lt;Error&gt; {
        let mut errors = Vec::new();

        let (ident, args) = match self {
            PrimaryExpression::FunctionCall(i, a) =&gt; (i, a),
            _ =&gt; unreachable!("analyze_fn_call called on non-FunctionCall variant"),
        };

        // can we find the function in the symbol table?
        if let Some(func) = table.get_fn(&amp;ident) {
            // check if the number of arguments match
            if func.params.len() != args.len() {
                // get the span of arguments supplied
                let call_span = Span::combine(
                    &amp;args.first().map(|a| a.span()).unwrap_or(ident.span.clone()),
                    &amp;args.last().map(|a| a.span()).unwrap_or(ident.span.clone()),
                );

                errors.push(anyhow!(SemanticError::ArgumentCountMismatch {
                    expected: func.params.len(),
                    found: args.len(),
                    call_span,
                    decl_span: func.sig_span.clone(),
                }));
            } else {
                // check if the types of the arguments match
                for (param_ty, arg) in func.params.iter().zip(args) {
                    let arg_ty: Type = match arg.get_type(table) {
                        Ok(ty) =&gt; ty,
                        Err(e) =&gt; {
                            errors.push(e);
                            continue;
                        }
                    };

                    if *param_ty != arg_ty {
                        errors.push(anyhow!(SemanticError::TypesDoNotMatch {
                            expected_type: param_ty.clone(),
                            expected_span: param_ty.span(),
                            found_type: arg_ty,
                            found_span: arg.span(),
                        }));
                    }
                }
            }
        } else {
            errors.push(anyhow!(SemanticError::FunctionNotDeclared(
                ident.clone(),
                ident.span.clone()
            )));
        }

        errors
    }
}</code></pre>
<p>Since <code>Expression</code> propagated its <code>get_type</code> and <code>idents_used</code> functions through to us, we need to implement this here. Since this is a primary expression, we'll actually handle it this time.</p>
<p>It depends on what kind of primary expression it is, however:</p>
<ul>
<li><strong>Literal</strong>: Call the <code>get_type</code> function on the literal</li>
<li><strong>Ident</strong>: Get the type of the identifier from the symbol table</li>
<li><strong>Parenthesized</strong>: Call the <code>get_type</code> function on the expression</li>
<li><strong>FunctionCall</strong>: Return the function return type, if it exists</li>
</ul>
<pre><code class="language-rust ignore">impl PrimaryExpression {
    pub fn get_type(&amp;self, table: &amp;SymbolTable) -&gt; Result&lt;Type&gt; {
        match self {
            PrimaryExpression::Literal(l) =&gt; Ok(l.get_type()),
            PrimaryExpression::Ident(i) =&gt; {
                if let Some(var) = table.get_var(&amp;i) {
                    Ok(var.ty.clone())
                } else {
                    Err(anyhow!(SemanticError::VariableNotDeclared(
                        i.clone(),
                        i.span.clone()
                    )))
                }
            }
            PrimaryExpression::Parenthesized(p) =&gt; p.get_type(table),
            PrimaryExpression::FunctionCall(i, _) =&gt; {
                if let Some(func) = table.get_fn(&amp;i) {
                    Ok(func.ret_ty.clone())
                } else {
                    Err(anyhow!(SemanticError::FunctionNotDeclared(
                        i.clone(),
                        i.span.clone()
                    )))
                }
            }
        }
    }
}</code></pre>
<p>For idents used it, too, depends on the variant:</p>
<ul>
<li><strong>Literal</strong>: No identifiers used</li>
<li><strong>Ident</strong>: Return the identifier</li>
<li><strong>Parenthesized</strong>: Call the <code>idents_used</code> function on the expression</li>
<li><strong>FunctionCall</strong>: Return the identifiers used in the arguments</li>
</ul>
<pre><code class="language-rust ignore">impl PrimaryExpression {
    pub fn idents_used(&amp;self) -&gt; Vec&lt;Ident&gt; {
        match self {
            PrimaryExpression::Literal(_) =&gt; vec![],
            PrimaryExpression::Ident(i) =&gt; vec![i.clone()],
            PrimaryExpression::Parenthesized(p) =&gt; p.idents_used(),
            PrimaryExpression::FunctionCall(_, args) =&gt; {
                args.iter().flat_map(|a| a.idents_used()).collect()
            }
        }
    }
}</code></pre>
<h2 id="testing-1"><a class="header" href="#testing-1">Testing</a></h2>
<p>It's important to test your code. Let's look at some code examples and run tests on them. We'll just go through one test.</p>
<pre><code>fn main() -&gt; int {
    return 5;
}

fn foo() -&gt; bool {
    return 5;
}
</code></pre>
<p>This gives us the error:</p>
<pre><code>Error: Incompatible return type
   ╭─[test:5:9]
   │
 4 │ fn foo() -&gt; bool {
   │             ──┬─  
   │               ╰─── expected bool return type
 5 │     return 5;
   │            ┬  
   │            ╰── found int instead of bool
───╯
</code></pre>
<p>This recognised that not only is <code>foo</code> returning an integer rather than <code>bool</code>, it knows that the <code>main</code> function exists and returns an integer. Nice.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="intermediate-representation"><a class="header" href="#intermediate-representation">Intermediate Representation</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="code-generation"><a class="header" href="#code-generation">Code Generation</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="postlude"><a class="header" href="#postlude">Postlude</a></h1>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <!-- Livereload script (if served using the cli tool) -->
        <script>
            const wsProtocol = location.protocol === 'https:' ? 'wss:' : 'ws:';
            const wsAddress = wsProtocol + "//" + location.host + "/" + "__livereload";
            const socket = new WebSocket(wsAddress);
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </div>
    </body>
</html>
